{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"GigQ: Lightweight Local Job Queue GigQ","text":"<p>Lightweight SQLite Job Queue</p> <p>GigQ is a lightweight job queue system with SQLite as its backend. It's designed for managing and executing small jobs (\"gigs\") locally with atomicity guarantees, particularly suited for processing tasks like data transformations, API calls, or batch operations, without the complexity of distributed job systems.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li> <p>Simple by Design - Define, queue, and process jobs with minimal setup and configuration</p> </li> <li> <p>SQLite Powered - Built on SQLite for reliability and simplicity with no extra dependencies</p> </li> <li> <p>Workflow Support - Create complex job workflows with dependencies and prioritization</p> </li> <li> <p>Lightweight Concurrency - Process jobs in parallel with built-in locking and state management</p> </li> </ul>"},{"location":"#what-is-gigq","title":"What is GigQ?","text":"<p>GigQ is a lightweight job queue system with SQLite as its backend. It's designed for managing and executing small jobs (\"gigs\") locally with atomicity guarantees, particularly suited for processing tasks like data transformations, API calls, or batch operations, without the complexity of distributed job systems.</p> <pre><code>from gigq import Job, JobQueue, Worker\n# Define a job function\ndef process_data(filename, threshold=0.5):\n# Process some data\nprint(f\"Processing {filename} with threshold {threshold}\")\nreturn {\"processed\": True, \"count\": 42}\n# Submit a job\nqueue = JobQueue(\"jobs.db\")\njob_id = queue.submit(Job(\nname=\"process_data_job\",\nfunction=process_data,\nparams={\"filename\": \"data.csv\", \"threshold\": 0.7}\n))\n# Start a worker to process jobs\nworker = Worker(\"jobs.db\")\nworker.start()\n</code></pre>"},{"location":"#why-gigq","title":"Why GigQ?","text":"<p>GigQ fills the gap between complex distributed job queues (like Celery or RQ) and simple task schedulers. It provides a balance of features and simplicity that's perfect for:</p> <ul> <li>Local data processing - Process files, transform data, and generate reports</li> <li>Task automation - Run scheduled tasks with dependencies and error handling</li> <li>API request batching - Queue and process API requests with rate limiting</li> <li>Workflow orchestration - Build simple workflows with dependencies and state management</li> </ul> <p>All without the overhead of setting up Redis, RabbitMQ, or other external services.</p>"},{"location":"#job-lifecycle","title":"Job Lifecycle","text":"<pre><code>stateDiagram-v2\n    [*] --&gt; PENDING: Job Created\n    PENDING --&gt; RUNNING: Worker Claims Job\n    RUNNING --&gt; COMPLETED: Successful Execution\n    RUNNING --&gt; FAILED: Error (max attempts exceeded)\n    RUNNING --&gt; PENDING: Error (retry)\n    RUNNING --&gt; TIMEOUT: Execution Time Exceeded\n    PENDING --&gt; CANCELLED: User Cancellation\n    COMPLETED --&gt; [*]\n    FAILED --&gt; [*]\n    CANCELLED --&gt; [*]\n    TIMEOUT --&gt; [*]</code></pre>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install gigq\n</code></pre>"},{"location":"#quick-start","title":"Quick Start","text":"<p>Check out the Quick Start Guide to begin using GigQ in your projects.</p>"},{"location":"#license","title":"License","text":"<p>GigQ is released under the MIT License. See LICENSE for details.</p>"},{"location":"changelog/","title":"Changelog","text":"<p>This page documents the release history and notable changes for GigQ.</p>"},{"location":"changelog/#010-2025-03-15","title":"0.1.0 (2025-03-15)","text":"<p>Initial release of GigQ.</p>"},{"location":"changelog/#features","title":"Features","text":"<ul> <li>Core job queue functionality with SQLite backend</li> <li>Job definition with parameters, priorities, and dependencies</li> <li>Worker implementation for job processing</li> <li>Workflow support for dependent jobs</li> <li>Command-line interface for job and worker management</li> <li>Automatic retry and timeout handling</li> <li>GitHub Archive processing example</li> </ul>"},{"location":"changelog/#api","title":"API","text":"<ul> <li><code>Job</code> class for defining jobs</li> <li><code>JobQueue</code> class for managing jobs</li> <li><code>Worker</code> class for processing jobs</li> <li><code>Workflow</code> class for creating job workflows</li> <li><code>JobStatus</code> enum for job state</li> </ul>"},{"location":"changelog/#cli-commands","title":"CLI Commands","text":"<ul> <li><code>submit</code> - Submit a job</li> <li><code>status</code> - Check job status</li> <li><code>list</code> - List jobs</li> <li><code>cancel</code> - Cancel a job</li> <li><code>requeue</code> - Requeue a failed job</li> <li><code>clear</code> - Clear completed jobs</li> <li><code>worker</code> - Start a worker</li> </ul>"},{"location":"changelog/#future-development","title":"Future Development","text":"<p>While not yet released, these features are planned for future versions:</p>"},{"location":"changelog/#020-planned","title":"0.2.0 (Planned)","text":"<ul> <li>Enhanced monitoring and metrics</li> <li>Additional backend options (Redis, PostgreSQL)</li> <li>Job scheduling capabilities</li> <li>Better error handling and recovery</li> <li>Improved CLI with interactive mode</li> <li>Web-based admin interface</li> </ul>"},{"location":"changelog/#030-planned","title":"0.3.0 (Planned)","text":"<ul> <li>Distributed job processing</li> <li>Custom job types</li> <li>Workflow templates</li> <li>Advanced job prioritization</li> <li>Performance optimizations</li> <li>Plugin system</li> </ul>"},{"location":"changelog/#contributing","title":"Contributing","text":"<p>GigQ is open for contributions. If you'd like to contribute, please:</p> <ol> <li>Check the issue tracker for open issues</li> <li>Submit a pull request with your changes</li> <li>Ensure tests pass and code meets the project's style guidelines</li> </ol> <p>When contributing, please update the changelog with your changes under an \"Unreleased\" section.</p>"},{"location":"faq/","title":"Frequently Asked Questions","text":"<p>This page answers common questions about using GigQ. If you don't find your question answered here, please check the detailed documentation or create an issue on GitHub.</p>"},{"location":"faq/#general-questions","title":"General Questions","text":""},{"location":"faq/#what-is-gigq","title":"What is GigQ?","text":"<p>GigQ is a lightweight job queue system with SQLite as its backend. It's designed for managing and executing small jobs (\"gigs\") locally with atomicity guarantees, particularly suited for processing tasks that don't require a distributed system.</p>"},{"location":"faq/#when-should-i-use-gigq","title":"When should I use GigQ?","text":"<p>GigQ is ideal for:</p> <ul> <li>Processing batches of data locally</li> <li>Running background tasks in a single application</li> <li>Creating simple workflows with dependencies</li> <li>Situations where you need job persistence but don't want to set up Redis, RabbitMQ, or other message brokers</li> </ul>"},{"location":"faq/#how-does-gigq-compare-to-celery-rq-or-other-task-queues","title":"How does GigQ compare to Celery, RQ, or other task queues?","text":"<p>GigQ is more lightweight and simpler to set up than distributed task queues like Celery or RQ:</p> Feature GigQ Celery/RQ Setup complexity Low (SQLite only) Higher (requires Redis, RabbitMQ, etc.) Scalability Moderate (single machine) High (distributed) Dependencies None beyond Python stdlib Multiple dependencies Job persistence Built-in (SQLite) Requires configuration Workflows Simple built-in support Requires additional code/plugins Monitoring Basic CLI tools Advanced dashboards available <p>GigQ focuses on simplicity and ease of use for local job processing, while Celery and RQ are designed for distributed, high-throughput scenarios.</p>"},{"location":"faq/#installation-and-setup","title":"Installation and Setup","text":""},{"location":"faq/#what-are-gigqs-dependencies","title":"What are GigQ's dependencies?","text":"<p>GigQ has minimal dependencies:</p> <ul> <li>Python 3.7 or newer</li> <li>SQLite 3.8.3 or newer (included with Python)</li> </ul> <p>For Python &lt; 3.8, it also requires:</p> <ul> <li>importlib-metadata &gt;= 1.0</li> </ul>"},{"location":"faq/#how-do-i-install-gigq","title":"How do I install GigQ?","text":"<pre><code>pip install gigq\n</code></pre>"},{"location":"faq/#can-i-use-gigq-with-a-virtual-environment","title":"Can I use GigQ with a virtual environment?","text":"<p>Yes, and it's recommended:</p> <pre><code>python -m venv gigq-env\nsource gigq-env/bin/activate  # On Windows: gigq-env\\Scripts\\activate\npip install gigq\n</code></pre>"},{"location":"faq/#jobs-and-job-queue","title":"Jobs and Job Queue","text":""},{"location":"faq/#how-do-i-submit-a-job","title":"How do I submit a job?","text":"<pre><code>from gigq import Job, JobQueue\ndef process_data(data):\n# Process the data...\nreturn {\"result\": \"processed\"}\nqueue = JobQueue(\"jobs.db\")\njob = Job(name=\"process\", function=process_data, params={\"data\": \"example\"})\njob_id = queue.submit(job)\n</code></pre>"},{"location":"faq/#how-do-i-check-a-jobs-status","title":"How do I check a job's status?","text":"<pre><code>status = queue.get_status(job_id)\nprint(f\"Job status: {status['status']}\")\n</code></pre>"},{"location":"faq/#can-i-cancel-a-job","title":"Can I cancel a job?","text":"<p>Yes, pending jobs can be cancelled:</p> <pre><code>if queue.cancel(job_id):\nprint(\"Job cancelled successfully\")\nelse:\nprint(\"Job could not be cancelled (may be running or completed)\")\n</code></pre>"},{"location":"faq/#how-do-i-handle-job-failures","title":"How do I handle job failures?","text":"<p>GigQ automatically retries failed jobs based on the <code>max_attempts</code> setting. You can also manually requeue failed jobs:</p> <pre><code>if queue.requeue_job(job_id):\nprint(\"Job requeued successfully\")\n</code></pre>"},{"location":"faq/#can-job-functions-accept-and-return-complex-data","title":"Can job functions accept and return complex data?","text":"<p>Yes, job parameters and results are stored as JSON, so any JSON-serializable data can be passed to jobs and returned from them. This includes:</p> <ul> <li>Dictionaries</li> <li>Lists</li> <li>Strings</li> <li>Numbers</li> <li>Booleans</li> <li>None</li> <li>Nested combinations of the above</li> </ul> <p>Non-serializable data (like file handles, database connections, etc.) cannot be passed directly and should be initialized within the job function.</p>"},{"location":"faq/#workers","title":"Workers","text":""},{"location":"faq/#how-do-i-start-a-worker","title":"How do I start a worker?","text":"<pre><code>from gigq import Worker\nworker = Worker(\"jobs.db\")\nworker.start()  # This blocks until the worker is stopped\n</code></pre> <p>Or via the CLI:</p> <pre><code>gigq --db jobs.db worker\n</code></pre>"},{"location":"faq/#how-many-workers-can-i-run-simultaneously","title":"How many workers can I run simultaneously?","text":"<p>You can run multiple workers simultaneously, limited primarily by your system resources and SQLite's concurrency capabilities. For most use cases, 2-8 workers is a reasonable range.</p>"},{"location":"faq/#how-do-workers-handle-job-failures","title":"How do workers handle job failures?","text":"<p>When a job fails (raises an exception):</p> <ol> <li>The worker logs the error</li> <li>The job's attempt counter is incremented</li> <li>If attempts &lt; max_attempts, the job is requeued</li> <li>If attempts &gt;= max_attempts, the job is marked as failed</li> </ol>"},{"location":"faq/#do-workers-automatically-recover-timed-out-jobs","title":"Do workers automatically recover timed-out jobs?","text":"<p>Yes. When a worker starts, it checks for jobs that have been running longer than their timeout and either requeues them or marks them as timed out.</p>"},{"location":"faq/#can-workers-run-on-different-machines","title":"Can workers run on different machines?","text":"<p>Yes, but they must all have access to the same SQLite database file. This could be on a network share, but be aware that SQLite has limitations when accessed over a network. For distributed scenarios, consider using a more appropriate backend.</p>"},{"location":"faq/#workflows","title":"Workflows","text":""},{"location":"faq/#how-do-i-create-a-workflow-with-dependencies","title":"How do I create a workflow with dependencies?","text":"<pre><code>from gigq import Workflow, Job, JobQueue\nworkflow = Workflow(\"data_pipeline\")\njob1 = Job(name=\"download\", function=download_data)\njob2 = Job(name=\"process\", function=process_data)\njob3 = Job(name=\"analyze\", function=analyze_data)\nworkflow.add_job(job1)\nworkflow.add_job(job2, depends_on=[job1])\nworkflow.add_job(job3, depends_on=[job2])\nqueue = JobQueue(\"workflow.db\")\njob_ids = workflow.submit_all(queue)\n</code></pre>"},{"location":"faq/#what-happens-if-a-job-in-a-workflow-fails","title":"What happens if a job in a workflow fails?","text":"<p>If a job fails after all retry attempts, any dependent jobs won't run. This prevents cascading failures and ensures data integrity.</p>"},{"location":"faq/#can-i-have-conditional-workflows","title":"Can I have conditional workflows?","text":"<p>GigQ doesn't directly support conditional workflows, but you can simulate them by:</p> <ol> <li>Having a job function return data indicating which path to take</li> <li>Creating a \"router\" job that submits additional jobs based on the result of a previous job</li> </ol>"},{"location":"faq/#performance-and-scaling","title":"Performance and Scaling","text":""},{"location":"faq/#is-sqlite-fast-enough-for-production-use","title":"Is SQLite fast enough for production use?","text":"<p>For many use cases, yes. SQLite can handle thousands of jobs per second on modern hardware. However, it's important to consider:</p> <ul> <li>SQLite's concurrency limitations (it uses file-level locking)</li> <li>Network file system performance if the database is shared</li> <li>The nature and size of your jobs</li> </ul>"},{"location":"faq/#how-can-i-optimize-gigq-performance","title":"How can I optimize GigQ performance?","text":"<ol> <li>Use appropriate polling intervals for your workload</li> <li>Run the right number of workers for your hardware</li> <li>Keep the SQLite database on fast local storage</li> <li>Regularly clean up completed jobs</li> <li>Set appropriate job timeouts</li> <li>Use job priorities effectively</li> </ol>"},{"location":"faq/#what-are-the-limitations-of-gigq","title":"What are the limitations of GigQ?","text":"<p>GigQ is primarily designed for local job processing and has some limitations:</p> <ul> <li>Not designed for distributed processing across multiple machines</li> <li>Limited by SQLite's concurrency capabilities</li> <li>No built-in monitoring dashboard</li> <li>Simple priority system may not suit complex scheduling needs</li> </ul>"},{"location":"faq/#command-line-interface","title":"Command Line Interface","text":""},{"location":"faq/#how-do-i-list-jobs-from-the-command-line","title":"How do I list jobs from the command line?","text":"<pre><code>gigq --db jobs.db list\ngigq --db jobs.db list --status pending\n</code></pre>"},{"location":"faq/#how-do-i-run-a-worker-that-processes-just-one-job","title":"How do I run a worker that processes just one job?","text":"<pre><code>gigq --db jobs.db worker --once\n</code></pre>"},{"location":"faq/#how-do-i-submit-a-job-from-the-command-line","title":"How do I submit a job from the command line?","text":"<pre><code>gigq --db jobs.db submit my_module.my_function --name \"My Job\" --param \"filename=data.csv\"\n</code></pre>"},{"location":"faq/#how-do-i-clear-old-jobs-from-the-database","title":"How do I clear old jobs from the database?","text":"<pre><code>gigq --db jobs.db clear --before 7  # Clear jobs completed more than 7 days ago\n</code></pre>"},{"location":"faq/#security-and-data","title":"Security and Data","text":""},{"location":"faq/#is-my-data-secure-with-gigq","title":"Is my data secure with GigQ?","text":"<p>GigQ uses SQLite, which provides basic security through file permissions. However:</p> <ul> <li>Job parameters and results are stored as plain text in the database</li> <li>No encryption is used for data at rest</li> <li>Authentication and authorization must be handled by your application</li> </ul> <p>For sensitive data, consider implementing appropriate security measures in your application.</p>"},{"location":"faq/#how-do-i-backup-my-gigq-data","title":"How do I backup my GigQ data?","text":"<p>Since GigQ uses SQLite, you can simply backup the database file. Make sure to:</p> <ol> <li>Either stop all workers before backup, or</li> <li>Use SQLite's backup API or tools designed for hot backups</li> </ol>"},{"location":"faq/#can-i-use-gigq-with-docker","title":"Can I use GigQ with Docker?","text":"<p>Yes, GigQ works well in Docker containers. Just ensure that:</p> <ul> <li>The SQLite database is stored in a persistent volume</li> <li>Workers can access the database file</li> <li>Signal handling is properly configured</li> </ul>"},{"location":"faq/#troubleshooting","title":"Troubleshooting","text":""},{"location":"faq/#why-is-my-job-stuck-in-the-running-state","title":"Why is my job stuck in the \"running\" state?","text":"<p>This can happen if:</p> <ol> <li>A worker crashed while processing the job</li> <li>The job exceeded its timeout but no worker has checked for timeouts yet</li> </ol> <p>Solutions:</p> <ul> <li>Start a worker, which will check for timed-out jobs</li> <li>Manually update the job status in the database</li> <li>Increase the job's timeout if it's legitimately long-running</li> </ul>"},{"location":"faq/#why-am-i-getting-sqlite-locking-errors","title":"Why am I getting SQLite locking errors?","text":"<p>SQLite uses file-level locking, which can cause contention with many concurrent workers. Try:</p> <ul> <li>Reducing the number of workers</li> <li>Increasing the polling interval</li> <li>Using the <code>timeout</code> parameter for SQLite connections (GigQ uses 30 seconds by default)</li> </ul>"},{"location":"faq/#how-do-i-debug-a-failing-job","title":"How do I debug a failing job?","text":"<ol> <li>Check the job's error message:</li> </ol> <pre><code>status = queue.get_status(job_id)\nprint(f\"Error: {status.get('error')}\")\n</code></pre> <ol> <li>Use logging in your job function:</li> </ol> <pre><code>import logging\nlogging.basicConfig(level=logging.DEBUG)\n</code></pre> <ol> <li>Try running the function directly with the same parameters</li> </ol>"},{"location":"faq/#can-i-use-gigq-with-djangoflaskfastapi","title":"Can I use GigQ with Django/Flask/FastAPI?","text":"<p>Yes, GigQ can be used with any Python web framework. Typical patterns:</p> <ul> <li>Submit jobs from web handlers</li> <li>Run workers as separate processes</li> <li>Use the job queue to track the status of long-running operations</li> </ul>"},{"location":"faq/#advanced-usage","title":"Advanced Usage","text":""},{"location":"faq/#can-i-extend-gigq-with-custom-features","title":"Can I extend GigQ with custom features?","text":"<p>Yes, GigQ is designed to be extensible. You can:</p> <ul> <li>Subclass <code>Job</code>, <code>Worker</code>, or <code>JobQueue</code> to customize behavior</li> <li>Create utility functions for common patterns</li> <li>Integrate with other systems through job functions</li> </ul>"},{"location":"faq/#can-i-use-gigq-with-a-different-database-backend","title":"Can I use GigQ with a different database backend?","text":"<p>GigQ is specifically designed for SQLite, but you could theoretically create a custom <code>JobQueue</code> implementation that uses a different backend. This would require reimplementing much of the queue logic.</p>"},{"location":"faq/#how-do-i-implement-a-periodic-job-scheduler-with-gigq","title":"How do I implement a periodic job scheduler with GigQ?","text":"<p>GigQ doesn't have built-in scheduling, but you can:</p> <ol> <li>Use a system scheduler (cron, Windows Task Scheduler) to submit jobs</li> <li>Create a daemon process that submits jobs on a schedule</li> <li>Use a dedicated scheduling library alongside GigQ</li> </ol> <p>Example with a simple daemon:</p> <pre><code>import time\nimport schedule\nfrom gigq import Job, JobQueue\nqueue = JobQueue(\"scheduled_jobs.db\")\ndef submit_daily_job():\njob = Job(name=\"daily_process\", function=daily_processing)\nqueue.submit(job)\nschedule.every().day.at(\"01:00\").do(submit_daily_job)\nwhile True:\nschedule.run_pending()\ntime.sleep(60)\n</code></pre>"},{"location":"faq/#contributing-to-gigq","title":"Contributing to GigQ","text":""},{"location":"faq/#how-can-i-contribute-to-gigq","title":"How can I contribute to GigQ?","text":"<p>We welcome contributions! You can:</p> <ul> <li>Report bugs and suggest features via GitHub issues</li> <li>Submit pull requests for bug fixes or enhancements</li> <li>Improve documentation</li> <li>Share examples and use cases</li> </ul>"},{"location":"faq/#how-do-i-run-the-gigq-tests","title":"How do I run the GigQ tests?","text":"<pre><code>git clone https://github.com/gigq/gigq.git\ncd gigq\npython -m unittest discover tests\n</code></pre>"},{"location":"faq/#where-can-i-get-help-with-gigq","title":"Where can I get help with GigQ?","text":"<ul> <li>Check the documentation</li> <li>Create an issue on GitHub</li> <li>Ask questions on Stack Overflow with the \"gigq\" tag</li> </ul>"},{"location":"advanced/concurrency/","title":"Concurrency in GigQ","text":"<p>This page explains how GigQ handles concurrent job processing, the SQLite locking mechanisms it relies on, and best practices for running multiple workers.</p>"},{"location":"advanced/concurrency/#overview","title":"Overview","text":"<p>GigQ is designed to support concurrent job processing with multiple workers while ensuring that:</p> <ol> <li>Each job is processed exactly once (no duplicate processing)</li> <li>Job state transitions are atomic (all-or-nothing)</li> <li>Job dependencies are respected</li> <li>System resources are used efficiently</li> </ol> <p>This is achieved through a combination of SQLite's transaction isolation, explicit locks, and careful job claiming design.</p>"},{"location":"advanced/concurrency/#how-job-claiming-works","title":"How Job Claiming Works","text":"<p>The job claiming process is the cornerstone of GigQ's concurrency model:</p> <pre><code>sequenceDiagram\n    participant Worker\n    participant SQLite\n\n    Worker-&gt;&gt;SQLite: BEGIN EXCLUSIVE TRANSACTION\n    Worker-&gt;&gt;SQLite: SELECT job FROM jobs WHERE status = 'pending' ...\n    SQLite--&gt;&gt;Worker: Job data (if available)\n\n    alt Job found\n        Worker-&gt;&gt;SQLite: UPDATE jobs SET status = 'running', worker_id = '...'\n        Worker-&gt;&gt;SQLite: INSERT INTO job_executions ...\n        Worker-&gt;&gt;SQLite: COMMIT\n    else No job found\n        Worker-&gt;&gt;SQLite: ROLLBACK\n    end</code></pre> <p>Key points in this process:</p> <ol> <li>The entire claim operation occurs in an exclusive transaction</li> <li>Workers query for jobs based on status, dependencies, and priority</li> <li>When a job is claimed, its status is updated and an execution record is created</li> <li>The transaction is committed, making the claim visible to other workers</li> <li>If no suitable job is found, the transaction is rolled back</li> </ol> <p>This process ensures that even if multiple workers attempt to claim the same job simultaneously, only one will succeed.</p>"},{"location":"advanced/concurrency/#sqlite-locking-mechanisms","title":"SQLite Locking Mechanisms","text":"<p>GigQ relies on SQLite's locking mechanisms to ensure safe concurrent access:</p>"},{"location":"advanced/concurrency/#transaction-isolation-levels","title":"Transaction Isolation Levels","text":"<p>SQLite supports several transaction modes:</p> <ul> <li>DEFERRED (default): Locks are acquired when needed</li> <li>IMMEDIATE: A reserved lock is acquired immediately</li> <li>EXCLUSIVE: An exclusive lock is acquired immediately</li> </ul> <p>GigQ uses EXCLUSIVE transactions for job claiming to prevent race conditions.</p>"},{"location":"advanced/concurrency/#lock-types","title":"Lock Types","text":"<p>SQLite uses a lock escalation system with five lock states:</p> <ol> <li>UNLOCKED: No locks are held (initial state)</li> <li>SHARED: Multiple readers can access the database</li> <li>RESERVED: Writer has reserved the right to modify the database</li> <li>PENDING: Writer is waiting for readers to finish</li> <li>EXCLUSIVE: Writer has exclusive access to modify the database</li> </ol> <p>When GigQ claims a job, it obtains an EXCLUSIVE lock for the duration of the transaction, ensuring no other workers can modify the database during the critical section.</p>"},{"location":"advanced/concurrency/#connection-timeout","title":"Connection Timeout","text":"<p>GigQ configures SQLite connections with a timeout (default: 30 seconds). If a connection cannot obtain the required lock within this timeout, it will raise an error.</p> <p>This timeout is important for preventing deadlocks and ensuring workers don't wait indefinitely for locks.</p>"},{"location":"advanced/concurrency/#concurrent-worker-operation","title":"Concurrent Worker Operation","text":"<p>When running multiple workers, they follow this general workflow:</p> <pre><code>graph TD\n    A[Worker Starts] --&gt; B[Check for Timeouts]\n    B --&gt; C[Try to Claim a Job]\n    C --&gt; D{Job Claimed?}\n    D -- Yes --&gt; E[Process Job]\n    D -- No --&gt; F[Wait for Polling Interval]\n    E --&gt; G[Update Job Status]\n    G --&gt; B\n    F --&gt; B</code></pre> <p>Multiple workers can execute this cycle simultaneously, with SQLite's locking ensuring that:</p> <ol> <li>Only one worker claims each job</li> <li>Workers don't interfere with each other's database operations</li> <li>Job state is consistent across all workers</li> </ol>"},{"location":"advanced/concurrency/#handling-worker-crashes","title":"Handling Worker Crashes","text":"<p>If a worker crashes while processing a job, the job remains in the \"running\" state. GigQ handles this through a timeout detection mechanism:</p> <ol> <li>When any worker starts (or periodically), it checks for jobs that have been running longer than their timeout</li> <li>If such jobs are found, they are marked as timed out or requeued for retry</li> <li>This ensures that jobs don't remain stuck in the running state indefinitely</li> </ol> <pre><code># Example of timeout detection (simplified from GigQ's implementation)\ndef check_for_timeouts(queue):\nwith sqlite3.connect(queue.db_path) as conn:\ncursor = conn.execute(\n\"\"\"\n            SELECT id, timeout, started_at, attempts, max_attempts\n            FROM jobs\n            WHERE status = 'running'\n            \"\"\"\n)\nnow = datetime.now()\nfor job in cursor.fetchall():\nstarted_at = datetime.fromisoformat(job[\"started_at\"])\ntimeout = job[\"timeout\"] or 300  # Default 5 minutes\nif now - started_at &gt; timedelta(seconds=timeout):\n# Job has timed out\nstatus = \"pending\" if job[\"attempts\"] &lt; job[\"max_attempts\"] else \"timeout\"\nconn.execute(\n\"\"\"\n                    UPDATE jobs\n                    SET status = ?, updated_at = ?, worker_id = NULL\n                    WHERE id = ?\n                    \"\"\",\n(status, now.isoformat(), job[\"id\"])\n)\n</code></pre>"},{"location":"advanced/concurrency/#job-dependencies-and-concurrent-processing","title":"Job Dependencies and Concurrent Processing","text":"<p>GigQ ensures that dependent jobs only run after their dependencies have completed:</p> <ol> <li>When looking for jobs to claim, workers check that a job's dependencies are all completed</li> <li>This check is done within the same exclusive transaction used for claiming the job</li> <li>This ensures that job dependencies are respected even with concurrent workers</li> </ol> <pre><code># Example of dependency checking (simplified from GigQ's implementation)\ndef check_dependencies(conn, dependencies):\nif not dependencies:\nreturn True\nplaceholders = \",\".join([\"?\"] * len(dependencies))\nquery = f\"\"\"\n        SELECT COUNT(*) as count\n        FROM jobs\n        WHERE id IN ({placeholders})\n        AND status != 'completed'\n    \"\"\"\ncursor = conn.execute(query, dependencies)\nresult = cursor.fetchone()\n# If count is 0, all dependencies are completed\nreturn result[\"count\"] == 0\n</code></pre>"},{"location":"advanced/concurrency/#concurrency-limitations","title":"Concurrency Limitations","text":"<p>While GigQ's concurrency model works well for most use cases, there are some limitations to be aware of:</p>"},{"location":"advanced/concurrency/#sqlite-concurrency-limits","title":"SQLite Concurrency Limits","text":"<p>SQLite uses file-level locking, which can become a bottleneck with many concurrent workers. The exact limit depends on:</p> <ul> <li>Hardware (especially disk I/O)</li> <li>Job processing duration</li> <li>Polling frequency</li> <li>SQLite configuration</li> </ul>"},{"location":"advanced/concurrency/#network-filesystem-considerations","title":"Network Filesystem Considerations","text":"<p>If the SQLite database is on a network filesystem (NFS, SMB, etc.), be aware that:</p> <ol> <li>Locking behavior may be less reliable</li> <li>Performance may be significantly slower</li> <li>Some network filesystems don't properly support SQLite's locking protocol</li> </ol> <p>For best results, store the database file on local storage when possible.</p>"},{"location":"advanced/concurrency/#optimizing-concurrency","title":"Optimizing Concurrency","text":"<p>Here are strategies to optimize concurrency with GigQ:</p>"},{"location":"advanced/concurrency/#1-adjust-worker-count","title":"1. Adjust Worker Count","text":"<p>The optimal number of workers depends on:</p> <ul> <li>CPU cores available</li> <li>Nature of your jobs (CPU-bound, I/O-bound, etc.)</li> <li>Other system resources (memory, disk I/O)</li> </ul> <p>For CPU-bound jobs, a good starting point is one worker per CPU core. For I/O-bound jobs, you might use more.</p>"},{"location":"advanced/concurrency/#2-tune-polling-intervals","title":"2. Tune Polling Intervals","text":"<p>The polling interval affects how often workers check for new jobs:</p> <ul> <li>Shorter intervals reduce job latency but increase database load</li> <li>Longer intervals reduce database load but increase job latency</li> </ul> <p>Find a balance that works for your workload.</p> <pre><code># Worker with a custom polling interval\nworker = Worker(\"jobs.db\", polling_interval=2)  # Check every 2 seconds\n</code></pre>"},{"location":"advanced/concurrency/#3-use-job-priorities","title":"3. Use Job Priorities","text":"<p>Prioritize jobs to ensure important work gets processed first:</p> <pre><code># High priority job\nurgent_job = Job(\nname=\"urgent_task\",\nfunction=process_urgent,\npriority=100  # Higher number = higher priority\n)\n# Low priority job\nbackground_job = Job(\nname=\"background_task\",\nfunction=process_background,\npriority=-10  # Lower number = lower priority\n)\n</code></pre>"},{"location":"advanced/concurrency/#4-implement-batching","title":"4. Implement Batching","text":"<p>For many small jobs, consider batching them to reduce overhead:</p> <pre><code>def process_batch(items):\nresults = []\nfor item in items:\nresults.append(process_item(item))\nreturn results\n# Submit a batch job instead of many small jobs\nbatch_job = Job(\nname=\"process_batch\",\nfunction=process_batch,\nparams={\"items\": items_to_process}\n)\n</code></pre>"},{"location":"advanced/concurrency/#5-use-appropriate-timeouts","title":"5. Use Appropriate Timeouts","text":"<p>Set job timeouts based on expected execution time:</p> <pre><code># Short-running job\nquick_job = Job(\nname=\"quick_task\",\nfunction=quick_function,\ntimeout=60  # 1 minute\n)\n# Long-running job\nlong_job = Job(\nname=\"long_task\",\nfunction=long_function,\ntimeout=3600  # 1 hour\n)\n</code></pre>"},{"location":"advanced/concurrency/#advanced-concurrency-patterns","title":"Advanced Concurrency Patterns","text":""},{"location":"advanced/concurrency/#worker-specialization","title":"Worker Specialization","text":"<p>You can create specialized workers that focus on specific types of jobs:</p> <pre><code>class HighPriorityWorker(Worker):\n\"\"\"Worker that only processes high-priority jobs.\"\"\"\ndef _claim_job(self):\nconn = self._get_connection()\ntry:\nconn.execute(\"BEGIN EXCLUSIVE TRANSACTION\")\ncursor = conn.execute(\n\"\"\"\n                SELECT * FROM jobs\n                WHERE status = ? AND priority &gt;= 50\n                ORDER BY priority DESC, created_at ASC\n                LIMIT 1\n                \"\"\",\n(JobStatus.PENDING.value,)\n)\n# Rest of the method follows the original implementation...\n</code></pre>"},{"location":"advanced/concurrency/#job-categories","title":"Job Categories","text":"<p>You can implement job categories by using separate queue databases:</p> <pre><code># Create separate queues for different job types\nurgent_queue = JobQueue(\"urgent_jobs.db\")\nbackground_queue = JobQueue(\"background_jobs.db\")\n# Create workers for each queue\nurgent_worker = Worker(\"urgent_jobs.db\")\nbackground_worker = Worker(\"background_jobs.db\")\n</code></pre>"},{"location":"advanced/concurrency/#distributed-workers","title":"Distributed Workers","text":"<p>While GigQ is primarily designed for local job processing, you can run workers on different machines if they all have access to the same database file. This could be through:</p> <ul> <li>Network file shares (with the limitations noted earlier)</li> <li>Cloud storage solutions that support SQLite (with appropriate caching)</li> <li>Custom synchronization mechanisms</li> </ul>"},{"location":"advanced/concurrency/#monitoring-concurrency","title":"Monitoring Concurrency","text":"<p>To monitor the concurrency of your GigQ setup:</p> <pre><code>def monitor_workers(queue):\n\"\"\"Print information about active workers and job distribution.\"\"\"\nrunning_jobs = queue.list_jobs(status=\"running\")\n# Group by worker\nworkers = {}\nfor job in running_jobs:\nworker_id = job.get(\"worker_id\")\nif worker_id:\nif worker_id not in workers:\nworkers[worker_id] = []\nworkers[worker_id].append(job)\nprint(f\"Active workers: {len(workers)}\")\nfor worker_id, jobs in workers.items():\nprint(f\"  Worker {worker_id}: {len(jobs)} jobs\")\n# Show job status distribution\nstatuses = [\"pending\", \"running\", \"completed\", \"failed\", \"cancelled\", \"timeout\"]\ncounts = {}\nfor status in statuses:\njobs = queue.list_jobs(status=status)\ncounts[status] = len(jobs)\nprint(\"\\nJob status distribution:\")\nfor status, count in counts.items():\nprint(f\"  {status}: {count}\")\n</code></pre>"},{"location":"advanced/concurrency/#best-practices-summary","title":"Best Practices Summary","text":"<ol> <li> <p>Start with a reasonable number of workers based on your system resources and job characteristics.</p> </li> <li> <p>Monitor database contention and adjust worker count and polling intervals if needed.</p> </li> <li> <p>Use appropriate timeouts to ensure jobs don't get stuck.</p> </li> <li> <p>Be cautious with network filesystems - test thoroughly and consider alternatives.</p> </li> <li> <p>Use job priorities to ensure important work is processed first.</p> </li> <li> <p>Consider batching for many small jobs to reduce overhead.</p> </li> <li> <p>Implement appropriate error handling to make jobs resilient to transient failures.</p> </li> <li> <p>Monitor worker health and job processing metrics.</p> </li> </ol>"},{"location":"advanced/concurrency/#next-steps","title":"Next Steps","text":"<p>Now that you understand how GigQ handles concurrency, you might want to explore:</p> <ul> <li>Performance Optimization - Additional strategies for optimizing performance</li> <li>Custom Job Types - Creating specialized job types</li> <li>SQLite Schema - Understanding the underlying database schema</li> </ul>"},{"location":"advanced/custom-job-types/","title":"Custom Job Types","text":"<p>This page explains how to extend GigQ with custom job types to meet specific requirements and use cases.</p>"},{"location":"advanced/custom-job-types/#introduction-to-custom-job-types","title":"Introduction to Custom Job Types","text":"<p>While GigQ's standard <code>Job</code> class handles most scenarios, you may need specialized job behavior for particular use cases. Custom job types allow you to:</p> <ol> <li>Encapsulate domain-specific job behavior</li> <li>Add custom validation and error handling</li> <li>Implement specialized execution logic</li> <li>Provide convenient interfaces for specific job patterns</li> </ol>"},{"location":"advanced/custom-job-types/#creating-a-basic-custom-job-type","title":"Creating a Basic Custom Job Type","text":"<p>To create a custom job type, subclass the <code>Job</code> class:</p> <pre><code>from gigq import Job\nclass DataProcessingJob(Job):\n\"\"\"A specialized job for data processing tasks.\"\"\"\ndef __init__(self, name, input_file, output_file, **kwargs):\n\"\"\"\n        Initialize a data processing job.\n        Args:\n            name: Name of the job\n            input_file: Path to the input file\n            output_file: Path to the output file\n            **kwargs: Additional arguments for the base Job class\n        \"\"\"\n# Define the job function\ndef process_data(input_file, output_file):\n# Data processing logic\nprint(f\"Processing {input_file} to {output_file}\")\n# ... processing code ...\nreturn {\n\"input\": input_file,\n\"output\": output_file,\n\"success\": True\n}\n# Set up job parameters\nparams = {\n\"input_file\": input_file,\n\"output_file\": output_file\n}\n# Initialize the base Job class\nsuper().__init__(\nname=name,\nfunction=process_data,\nparams=params,\n**kwargs\n)\n# Store additional attributes\nself.input_file = input_file\nself.output_file = output_file\n</code></pre> <p>Usage:</p> <pre><code># Create and submit a data processing job\njob = DataProcessingJob(\nname=\"process_csv\",\ninput_file=\"data.csv\",\noutput_file=\"processed.csv\",\npriority=10,\nmax_attempts=3\n)\nqueue = JobQueue(\"jobs.db\")\njob_id = queue.submit(job)\n</code></pre>"},{"location":"advanced/custom-job-types/#adding-custom-validation","title":"Adding Custom Validation","text":"<p>Custom job types can include validation logic:</p> <pre><code>import os\nfrom gigq import Job\nclass FileConversionJob(Job):\n\"\"\"Job for converting files from one format to another.\"\"\"\nSUPPORTED_FORMATS = {\n'csv': ['.xlsx', '.json'],\n'json': ['.csv', '.xml'],\n'xlsx': ['.csv'],\n'xml': ['.json']\n}\ndef __init__(self, name, source_file, target_format, **kwargs):\n\"\"\"\n        Initialize a file conversion job.\n        Args:\n            name: Name of the job\n            source_file: Path to the source file\n            target_format: Target format (without the dot)\n            **kwargs: Additional arguments for the base Job class\n        \"\"\"\n# Validate inputs\nself._validate_inputs(source_file, target_format)\n# Determine output file path\nsource_base, source_ext = os.path.splitext(source_file)\ntarget_file = f\"{source_base}.{target_format}\"\n# Define the job function\ndef convert_file(source_file, target_file):\nprint(f\"Converting {source_file} to {target_file}\")\n# ... conversion code ...\nreturn {\n\"source\": source_file,\n\"target\": target_file\n}\n# Set up job parameters\nparams = {\n\"source_file\": source_file,\n\"target_file\": target_file\n}\n# Initialize the base Job class\nsuper().__init__(\nname=name,\nfunction=convert_file,\nparams=params,\n**kwargs\n)\n# Store additional attributes\nself.source_file = source_file\nself.target_format = target_format\nself.target_file = target_file\ndef _validate_inputs(self, source_file, target_format):\n\"\"\"Validate input parameters.\"\"\"\n# Check that source file exists\nif not os.path.exists(source_file):\nraise ValueError(f\"Source file not found: {source_file}\")\n# Check that source format is supported\nsource_base, source_ext = os.path.splitext(source_file)\nsource_format = source_ext.lstrip('.')\nif source_format not in self.SUPPORTED_FORMATS:\nraise ValueError(f\"Unsupported source format: {source_format}\")\n# Check that target format is supported for this source format\nif target_format not in self.SUPPORTED_FORMATS[source_format]:\nraise ValueError(\nf\"Cannot convert {source_format} to {target_format}. \"\nf\"Supported targets: {self.SUPPORTED_FORMATS[source_format]}\"\n)\n</code></pre> <p>Usage:</p> <pre><code>try:\njob = FileConversionJob(\nname=\"convert_data\",\nsource_file=\"data.csv\",\ntarget_format=\"json\"\n)\nqueue.submit(job)\nexcept ValueError as e:\nprint(f\"Validation error: {e}\")\n</code></pre>"},{"location":"advanced/custom-job-types/#creating-a-job-factory","title":"Creating a Job Factory","text":"<p>For complex job configurations, use a factory pattern:</p> <pre><code>from gigq import Job, Workflow\nclass ETLJobFactory:\n\"\"\"Factory for creating ETL (Extract, Transform, Load) jobs.\"\"\"\ndef __init__(self, source_system, destination_system):\n\"\"\"\n        Initialize the ETL job factory.\n        Args:\n            source_system: Configuration for the source system\n            destination_system: Configuration for the destination system\n        \"\"\"\nself.source_system = source_system\nself.destination_system = destination_system\ndef create_extract_job(self, name, query, **kwargs):\n\"\"\"Create a job to extract data from the source system.\"\"\"\ndef extract_data(source_system, query):\nprint(f\"Extracting data from {source_system['name']} with query: {query}\")\n# ... extraction code ...\nreturn {\"data\": \"extracted_data\", \"source\": source_system['name']}\nreturn Job(\nname=name,\nfunction=extract_data,\nparams={\"source_system\": self.source_system, \"query\": query},\n**kwargs\n)\ndef create_transform_job(self, name, transformation_rules, **kwargs):\n\"\"\"Create a job to transform the extracted data.\"\"\"\ndef transform_data(data, transformation_rules):\nprint(f\"Transforming data with rules: {transformation_rules}\")\n# ... transformation code ...\nreturn {\"data\": \"transformed_data\", \"rules_applied\": transformation_rules}\nreturn Job(\nname=name,\nfunction=transform_data,\nparams={\"data\": \"extracted_data\", \"transformation_rules\": transformation_rules},\n**kwargs\n)\ndef create_load_job(self, name, table_name, **kwargs):\n\"\"\"Create a job to load the transformed data into the destination system.\"\"\"\ndef load_data(destination_system, data, table_name):\nprint(f\"Loading data to {destination_system['name']}.{table_name}\")\n# ... loading code ...\nreturn {\n\"destination\": destination_system['name'],\n\"table\": table_name,\n\"rows_loaded\": 100\n}\nreturn Job(\nname=name,\nfunction=load_data,\nparams={\n\"destination_system\": self.destination_system,\n\"data\": \"transformed_data\",\n\"table_name\": table_name\n},\n**kwargs\n)\ndef create_etl_workflow(self, base_name, query, transformation_rules, table_name):\n\"\"\"Create a complete ETL workflow.\"\"\"\nworkflow = Workflow(f\"etl_{base_name}\")\nextract_job = self.create_extract_job(\nf\"{base_name}_extract\",\nquery=query\n)\ntransform_job = self.create_transform_job(\nf\"{base_name}_transform\",\ntransformation_rules=transformation_rules\n)\nload_job = self.create_load_job(\nf\"{base_name}_load\",\ntable_name=table_name\n)\nworkflow.add_job(extract_job)\nworkflow.add_job(transform_job, depends_on=[extract_job])\nworkflow.add_job(load_job, depends_on=[transform_job])\nreturn workflow\n</code></pre> <p>Usage:</p> <pre><code># Create an ETL factory\netl_factory = ETLJobFactory(\nsource_system={\n\"name\": \"mysql_db\",\n\"connection_string\": \"mysql://user:pass@host/db\"\n},\ndestination_system={\n\"name\": \"data_warehouse\",\n\"connection_string\": \"postgresql://user:pass@host/db\"\n}\n)\n# Create and submit an ETL workflow\nworkflow = etl_factory.create_etl_workflow(\nbase_name=\"daily_sales\",\nquery=\"SELECT * FROM sales WHERE date = CURDATE()\",\ntransformation_rules=[\"format_currency\", \"aggregate_by_region\"],\ntable_name=\"daily_sales_summary\"\n)\nqueue = JobQueue(\"etl_jobs.db\")\njob_ids = workflow.submit_all(queue)\n</code></pre>"},{"location":"advanced/custom-job-types/#advanced-job-types","title":"Advanced Job Types","text":""},{"location":"advanced/custom-job-types/#parameterized-job","title":"Parameterized Job","text":"<p>A job type that enforces specific parameters:</p> <pre><code>class ParameterizedJob(Job):\n\"\"\"A job that enforces specific parameter types.\"\"\"\ndef __init__(self, name, function, params=None, **kwargs):\n\"\"\"\n        Initialize a parameterized job.\n        Args:\n            name: Name of the job\n            function: Function to execute\n            params: Parameters to pass to the function\n            **kwargs: Additional arguments for the base Job class\n        \"\"\"\n# Get the function's signature\nimport inspect\nself.signature = inspect.signature(function)\n# Validate parameters against function signature\nparams = params or {}\nself._validate_params(params)\n# Initialize the base Job class\nsuper().__init__(\nname=name,\nfunction=function,\nparams=params,\n**kwargs\n)\ndef _validate_params(self, params):\n\"\"\"Validate parameters against the function signature.\"\"\"\n# Check required parameters\nfor name, param in self.signature.parameters.items():\nif param.default == inspect.Parameter.empty and name not in params:\nraise ValueError(f\"Missing required parameter: {name}\")\n# Check for unexpected parameters\nfor name in params:\nif name not in self.signature.parameters:\nraise ValueError(f\"Unexpected parameter: {name}\")\n</code></pre>"},{"location":"advanced/custom-job-types/#periodic-job","title":"Periodic Job","text":"<p>A job type designed for recurring execution:</p> <pre><code>class PeriodicJob(Job):\n\"\"\"A job designed to run periodically.\"\"\"\ndef __init__(self, name, function, interval, **kwargs):\n\"\"\"\n        Initialize a periodic job.\n        Args:\n            name: Name of the job\n            function: Function to execute\n            interval: Interval in seconds between runs\n            **kwargs: Additional arguments for the base Job class\n        \"\"\"\nparams = kwargs.pop('params', {})\n# Add metadata about the schedule\nmetadata = kwargs.pop('metadata', {})\nmetadata.update({\n'periodic': True,\n'interval': interval,\n'last_run': None,\n'next_run': None\n})\n# Initialize the base Job class\nsuper().__init__(\nname=name,\nfunction=function,\nparams=params,\nmetadata=metadata,\n**kwargs\n)\nself.interval = interval\ndef reschedule(self, queue):\n\"\"\"\n        Reschedule this job after execution.\n        Args:\n            queue: JobQueue to resubmit to\n        Returns:\n            ID of the new job\n        \"\"\"\nimport time\nnow = time.time()\n# Create a new instance with updated schedule\nnew_job = self.__class__(\nname=self.name,\nfunction=self.function,\ninterval=self.interval,\nparams=self.params,\npriority=self.priority,\nmax_attempts=self.max_attempts,\ntimeout=self.timeout,\ndescription=self.description\n)\n# Update schedule metadata\nnew_job.metadata = self.metadata.copy()\nnew_job.metadata['last_run'] = now\nnew_job.metadata['next_run'] = now + self.interval\n# Submit the new job\nreturn queue.submit(new_job)\n</code></pre>"},{"location":"advanced/custom-job-types/#retry-strategies","title":"Retry Strategies","text":"<p>A job type with custom retry strategies:</p> <pre><code>import random\nimport time\nfrom gigq import Job\nclass RetryStrategyJob(Job):\n\"\"\"A job with customizable retry strategies.\"\"\"\n# Retry strategies\nRETRY_FIXED = \"fixed\"\nRETRY_EXPONENTIAL = \"exponential\"\nRETRY_RANDOM = \"random\"\ndef __init__(self, name, function, retry_strategy=\"fixed\", retry_params=None, **kwargs):\n\"\"\"\n        Initialize a job with custom retry strategy.\n        Args:\n            name: Name of the job\n            function: Function to execute\n            retry_strategy: Strategy for retries (fixed, exponential, random)\n            retry_params: Parameters for the retry strategy\n            **kwargs: Additional arguments for the base Job class\n        \"\"\"\nself.retry_strategy = retry_strategy\nself.retry_params = retry_params or {}\n# Wrap the function to handle retries\nwrapped_function = self._wrap_function_with_retries(function)\n# Initialize the base Job class\nsuper().__init__(\nname=name,\nfunction=wrapped_function,\n**kwargs\n)\ndef _wrap_function_with_retries(self, function):\n\"\"\"Wrap the function with retry logic.\"\"\"\ndef wrapped_function(*args, **kwargs):\nmax_retries = self.retry_params.get(\"max_retries\", 3)\nattempt = 0\nwhile attempt &lt;= max_retries:\ntry:\nreturn function(*args, **kwargs)\nexcept Exception as e:\nattempt += 1\nif attempt &gt; max_retries:\nraise\n# Calculate delay based on strategy\ndelay = self._calculate_retry_delay(attempt)\n# Log the retry\nprint(f\"Retry {attempt}/{max_retries} after {delay:.2f}s: {str(e)}\")\n# Wait before retry\ntime.sleep(delay)\nreturn wrapped_function\ndef _calculate_retry_delay(self, attempt):\n\"\"\"Calculate the delay before the next retry.\"\"\"\nbase_delay = self.retry_params.get(\"base_delay\", 1.0)\nmax_delay = self.retry_params.get(\"max_delay\", 60.0)\nif self.retry_strategy == self.RETRY_FIXED:\ndelay = base_delay\nelif self.retry_strategy == self.RETRY_EXPONENTIAL:\n# Exponential backoff: base_delay * 2^attempt\ndelay = base_delay * (2 ** (attempt - 1))\nelif self.retry_strategy == self.RETRY_RANDOM:\n# Random delay between base_delay and base_delay * 3\ndelay = base_delay + (random.random() * base_delay * 2)\nelse:\ndelay = base_delay\n# Cap at max_delay\nreturn min(delay, max_delay)\n</code></pre> <p>Usage:</p> <pre><code>def unstable_api_call(url):\n\"\"\"Simulate an unstable API that sometimes fails.\"\"\"\nimport random\nif random.random() &lt; 0.7:  # 70% chance of failure\nraise ConnectionError(\"API connection failed\")\nreturn {\"status\": \"success\", \"data\": \"some data\"}\n# Create a job with exponential backoff\njob = RetryStrategyJob(\nname=\"api_call\",\nfunction=unstable_api_call,\nretry_strategy=RetryStrategyJob.RETRY_EXPONENTIAL,\nretry_params={\n\"max_retries\": 5,\n\"base_delay\": 1.0,\n\"max_delay\": 30.0\n},\nparams={\"url\": \"https://api.example.com/data\"}\n)\nqueue.submit(job)\n</code></pre>"},{"location":"advanced/custom-job-types/#job-type-with-custom-worker-handling","title":"Job Type with Custom Worker Handling","text":"<p>You can create job types that require custom worker behavior:</p> <pre><code>class PrioritizedJob(Job):\n\"\"\"A job with advanced priority handling.\"\"\"\ndef __init__(self, name, function, priority_category=\"normal\", **kwargs):\n\"\"\"\n        Initialize a job with category-based priority.\n        Args:\n            name: Name of the job\n            function: Function to execute\n            priority_category: Category for priority (critical, high, normal, low, background)\n            **kwargs: Additional arguments for the base Job class\n        \"\"\"\n# Map categories to numeric priorities\npriority_map = {\n\"critical\": 100,\n\"high\": 50,\n\"normal\": 0,\n\"low\": -50,\n\"background\": -100\n}\n# Get numeric priority\nif priority_category not in priority_map:\nraise ValueError(f\"Invalid priority category: {priority_category}\")\nnumeric_priority = priority_map[priority_category]\n# Initialize the base Job class\nsuper().__init__(\nname=name,\nfunction=function,\npriority=numeric_priority,\n**kwargs\n)\nself.priority_category = priority_category\n</code></pre> <p>Then create a custom worker that knows how to handle these jobs:</p> <pre><code>class PrioritizedWorker(Worker):\n\"\"\"A worker that respects priority categories.\"\"\"\ndef __init__(self, db_path, allowed_categories=None, **kwargs):\n\"\"\"\n        Initialize a prioritized worker.\n        Args:\n            db_path: Path to the SQLite database file\n            allowed_categories: List of priority categories this worker can process\n            **kwargs: Additional arguments for the base Worker class\n        \"\"\"\nsuper().__init__(db_path, **kwargs)\nself.allowed_categories = allowed_categories or [\"critical\", \"high\", \"normal\", \"low\", \"background\"]\ndef _claim_job(self):\n\"\"\"Claim a job from the queue, respecting priority categories.\"\"\"\nconn = self._get_connection()\ntry:\nconn.execute(\"BEGIN EXCLUSIVE TRANSACTION\")\n# Map allowed categories to priority ranges\npriority_ranges = []\nif \"critical\" in self.allowed_categories:\npriority_ranges.append(\"priority &gt;= 100\")\nif \"high\" in self.allowed_categories:\npriority_ranges.append(\"(priority &gt;= 50 AND priority &lt; 100)\")\nif \"normal\" in self.allowed_categories:\npriority_ranges.append(\"(priority &gt;= -50 AND priority &lt; 50)\")\nif \"low\" in self.allowed_categories:\npriority_ranges.append(\"(priority &gt;= -100 AND priority &lt; -50)\")\nif \"background\" in self.allowed_categories:\npriority_ranges.append(\"priority &lt; -100\")\n# Build the query\npriority_clause = \" OR \".join(priority_ranges)\nquery = f\"\"\"\n            SELECT * FROM jobs\n            WHERE status = ? AND ({priority_clause})\n            ORDER BY priority DESC, created_at ASC\n            LIMIT 1\n            \"\"\"\ncursor = conn.execute(query, (JobStatus.PENDING.value,))\n# Continue with normal job claiming process\n# ...\n</code></pre> <p>Usage:</p> <pre><code># Create prioritized jobs\ncritical_job = PrioritizedJob(\nname=\"critical_task\",\nfunction=critical_function,\npriority_category=\"critical\"\n)\nnormal_job = PrioritizedJob(\nname=\"normal_task\",\nfunction=normal_function,\npriority_category=\"normal\"\n)\nbackground_job = PrioritizedJob(\nname=\"background_task\",\nfunction=background_function,\npriority_category=\"background\"\n)\n# Create specialized workers\ncritical_worker = PrioritizedWorker(\n\"jobs.db\",\nallowed_categories=[\"critical\", \"high\"]\n)\nbackground_worker = PrioritizedWorker(\n\"jobs.db\",\nallowed_categories=[\"low\", \"background\"]\n)\n</code></pre>"},{"location":"advanced/custom-job-types/#integrating-with-external-task-systems","title":"Integrating with External Task Systems","text":"<p>You can create job types that integrate with external task systems:</p> <pre><code>class CeleryIntegrationJob(Job):\n\"\"\"A job that delegates to Celery for execution.\"\"\"\ndef __init__(self, name, celery_task, task_args=None, task_kwargs=None, **kwargs):\n\"\"\"\n        Initialize a job that delegates to Celery.\n        Args:\n            name: Name of the job\n            celery_task: Celery task function\n            task_args: Positional arguments for the Celery task\n            task_kwargs: Keyword arguments for the Celery task\n            **kwargs: Additional arguments for the base Job class\n        \"\"\"\nself.celery_task = celery_task\nself.task_args = task_args or []\nself.task_kwargs = task_kwargs or {}\n# Define the job function that delegates to Celery\ndef run_celery_task(task_name, task_args, task_kwargs):\n# Import celery here to avoid making it a dependency for all of GigQ\nimport importlib\nmodule_name, task_name = task_name.rsplit('.', 1)\nmodule = importlib.import_module(module_name)\ntask = getattr(module, task_name)\n# Send the task to Celery and wait for result\nresult = task.delay(*task_args, **task_kwargs)\nreturn {\"celery_task_id\": result.id, \"result\": result.get()}\n# Initialize the base Job class\nsuper().__init__(\nname=name,\nfunction=run_celery_task,\nparams={\n\"task_name\": f\"{celery_task.__module__}.{celery_task.__name__}\",\n\"task_args\": self.task_args,\n\"task_kwargs\": self.task_kwargs\n},\n**kwargs\n)\n</code></pre>"},{"location":"advanced/custom-job-types/#performance-considerations","title":"Performance Considerations","text":"<p>When creating custom job types, keep these performance considerations in mind:</p> <ol> <li>Serialization: All job parameters must be JSON-serializable</li> <li>Function Imports: Functions must be importable from their module</li> <li>Memory Usage: Avoid storing large data in job attributes</li> <li>CPU Usage: Complex validation logic can slow down job submission</li> </ol>"},{"location":"advanced/custom-job-types/#best-practices","title":"Best Practices","text":"<ol> <li>Clear Naming: Give your custom job types clear, descriptive names</li> <li>Modular Design: Break complex jobs into smaller, more manageable ones</li> <li>Good Documentation: Document your custom job types thoroughly</li> <li>Validation: Add input validation to prevent errors during execution</li> <li>Error Handling: Implement appropriate error handling strategies</li> <li>Testing: Write tests for your custom job types</li> </ol>"},{"location":"advanced/custom-job-types/#example-library-common-job-types","title":"Example Library: Common Job Types","text":"<p>Here's a small library of common job types:</p> <pre><code>\"\"\"\nA library of common job types for GigQ.\n\"\"\"\nfrom gigq import Job\nclass HTTPRequestJob(Job):\n\"\"\"Job for making HTTP requests.\"\"\"\ndef __init__(self, name, url, method=\"GET\", headers=None, data=None, **kwargs):\n\"\"\"\n        Initialize an HTTP request job.\n        Args:\n            name: Name of the job\n            url: URL to request\n            method: HTTP method (GET, POST, etc.)\n            headers: HTTP headers\n            data: Request data (for POST, PUT, etc.)\n            **kwargs: Additional arguments for the base Job class\n        \"\"\"\ndef make_request(url, method, headers, data):\nimport requests\nresponse = requests.request(method, url, headers=headers, data=data)\nresponse.raise_for_status()\ntry:\nresult = response.json()\nexcept ValueError:\nresult = response.text\nreturn {\n\"status_code\": response.status_code,\n\"headers\": dict(response.headers),\n\"result\": result\n}\nsuper().__init__(\nname=name,\nfunction=make_request,\nparams={\n\"url\": url,\n\"method\": method,\n\"headers\": headers,\n\"data\": data\n},\n**kwargs\n)\nclass EmailJob(Job):\n\"\"\"Job for sending emails.\"\"\"\ndef __init__(self, name, to, subject, body, from_email=None, **kwargs):\n\"\"\"\n        Initialize an email job.\n        Args:\n            name: Name of the job\n            to: Recipient email address(es)\n            subject: Email subject\n            body: Email body\n            from_email: Sender email address\n            **kwargs: Additional arguments for the base Job class\n        \"\"\"\ndef send_email(to, subject, body, from_email):\nimport smtplib\nfrom email.message import EmailMessage\nmsg = EmailMessage()\nmsg.set_content(body)\nmsg['Subject'] = subject\nmsg['To'] = to\nmsg['From'] = from_email or 'noreply@example.com'\n# Get SMTP settings from environment\nimport os\nsmtp_server = os.environ.get('SMTP_SERVER', 'localhost')\nsmtp_port = int(os.environ.get('SMTP_PORT', 25))\nwith smtplib.SMTP(smtp_server, smtp_port) as server:\nserver.send_message(msg)\nreturn {\"to\": to, \"subject\": subject, \"sent\": True}\nsuper().__init__(\nname=name,\nfunction=send_email,\nparams={\n\"to\": to,\n\"subject\": subject,\n\"body\": body,\n\"from_email\": from_email\n},\n**kwargs\n)\nclass DatabaseQueryJob(Job):\n\"\"\"Job for executing database queries.\"\"\"\ndef __init__(self, name, query, params=None, connection_string=None, **kwargs):\n\"\"\"\n        Initialize a database query job.\n        Args:\n            name: Name of the job\n            query: SQL query to execute\n            params: Query parameters\n            connection_string: Database connection string\n            **kwargs: Additional arguments for the base Job class\n        \"\"\"\ndef execute_query(query, params, connection_string):\nimport os\nimport sqlalchemy\n# Use environment variable if connection string not provided\nconn_str = connection_string or os.environ.get('DATABASE_URL')\nif not conn_str:\nraise ValueError(\"Database connection string not provided\")\nengine = sqlalchemy.create_engine(conn_str)\nwith engine.connect() as connection:\nresult = connection.execute(sqlalchemy.text(query), params or {})\nif result.returns_rows:\n# Get column names\ncolumns = result.keys()\n# Get rows as dictionaries\nrows = [dict(zip(columns, row)) for row in result.fetchall()]\nreturn {\n\"rows\": rows,\n\"row_count\": len(rows),\n\"columns\": columns\n}\nelse:\nreturn {\n\"row_count\": result.rowcount,\n\"rows\": [],\n\"columns\": []\n}\nsuper().__init__(\nname=name,\nfunction=execute_query,\nparams={\n\"query\": query,\n\"params\": params,\n\"connection_string\": connection_string\n},\n**kwargs\n)\n</code></pre>"},{"location":"advanced/custom-job-types/#next-steps","title":"Next Steps","text":"<p>Now that you understand how to create custom job types, you might want to explore:</p> <ul> <li>Workflows - Learn how to create complex workflows with your custom job types</li> <li>Error Handling - Learn about error handling strategies</li> <li>Performance Optimization - Optimize performance for your custom job types</li> </ul>"},{"location":"advanced/performance/","title":"Performance Optimization","text":"<p>This page provides guidance on optimizing GigQ's performance for various workloads and scenarios.</p>"},{"location":"advanced/performance/#understanding-gigqs-performance-characteristics","title":"Understanding GigQ's Performance Characteristics","text":"<p>GigQ is designed to be lightweight while providing reliable job processing. Its performance is influenced by several factors:</p> <ol> <li>SQLite Database Performance: GigQ uses SQLite as its backend, which has specific performance characteristics</li> <li>Worker Configuration: How workers are configured affects throughput and resource usage</li> <li>Job Characteristics: The nature of the jobs being processed impacts overall system performance</li> <li>System Resources: Available CPU, memory, and disk I/O affect how many jobs can be processed concurrently</li> </ol>"},{"location":"advanced/performance/#sqlite-database-optimization","title":"SQLite Database Optimization","text":""},{"location":"advanced/performance/#database-location","title":"Database Location","text":"<p>The location of your SQLite database file significantly impacts performance:</p> Location Performance Reliability Use Case Local SSD Excellent Excellent Production Local HDD Good Good Development Network File System (NFS) Poor to Fair Fair Distributed setups (use cautiously) Memory (<code>:memory:</code> or tmpfs) Excellent Poor (no persistence) Ephemeral workloads <p>For best performance, keep your database file on local SSD storage.</p>"},{"location":"advanced/performance/#connection-pooling","title":"Connection Pooling","text":"<p>SQLite connections have overhead. GigQ creates connections as needed, but you can optimize by:</p> <pre><code># Create a shared connection pool\nimport sqlite3\nimport threading\nclass ConnectionPool:\ndef __init__(self, db_path, max_connections=5):\nself.db_path = db_path\nself.max_connections = max_connections\nself.connections = []\nself.lock = threading.Lock()\ndef get_connection(self):\nwith self.lock:\nif self.connections:\nreturn self.connections.pop()\nelse:\nconn = sqlite3.connect(self.db_path, timeout=30.0)\nconn.row_factory = sqlite3.Row\nreturn conn\ndef release_connection(self, conn):\nwith self.lock:\nif len(self.connections) &lt; self.max_connections:\nself.connections.append(conn)\nelse:\nconn.close()\n# Modify JobQueue to use the connection pool\nclass OptimizedJobQueue(JobQueue):\ndef __init__(self, db_path, initialize=True, connection_pool=None):\nself.db_path = db_path\nself.connection_pool = connection_pool or ConnectionPool(db_path)\nif initialize:\nself._initialize_db()\ndef _get_connection(self):\nreturn self.connection_pool.get_connection()\ndef _release_connection(self, conn):\nself.connection_pool.release_connection(conn)\n# Override methods to properly release connections\ndef submit(self, job):\nconn = self._get_connection()\ntry:\n# Existing implementation\nresult = ...\nreturn result\nfinally:\nself._release_connection(conn)\n</code></pre>"},{"location":"advanced/performance/#index-optimization","title":"Index Optimization","text":"<p>GigQ creates basic indexes, but you might benefit from additional indexes based on your query patterns:</p> <pre><code># Add custom indexes for your specific workload\nconn = sqlite3.connect(\"jobs.db\")\nconn.execute(\"CREATE INDEX IF NOT EXISTS idx_jobs_name ON jobs (name)\")\nconn.execute(\"CREATE INDEX IF NOT EXISTS idx_jobs_priority_status ON jobs (priority, status)\")\nconn.commit()\nconn.close()\n</code></pre>"},{"location":"advanced/performance/#database-maintenance","title":"Database Maintenance","text":"<p>Regular maintenance helps maintain performance:</p> <pre><code>def optimize_database(db_path):\n\"\"\"Perform database maintenance to optimize performance.\"\"\"\nconn = sqlite3.connect(db_path)\n# Analyze to update statistics\nconn.execute(\"ANALYZE\")\n# Vacuum to reclaim space and defragment\nconn.execute(\"VACUUM\")\n# Reindex to optimize indexes\nconn.execute(\"REINDEX\")\nconn.close()\n</code></pre> <p>Run this periodically, especially after clearing many jobs.</p>"},{"location":"advanced/performance/#worker-configuration","title":"Worker Configuration","text":""},{"location":"advanced/performance/#polling-interval","title":"Polling Interval","text":"<p>The worker polling interval affects both responsiveness and database load:</p> <pre><code># More responsive but higher database load\nworker_responsive = Worker(\"jobs.db\", polling_interval=1)\n# Less responsive but lower database load\nworker_efficient = Worker(\"jobs.db\", polling_interval=10)\n</code></pre> <p>Finding the right balance depends on your workload:</p> <ul> <li>For latency-sensitive jobs, use shorter intervals (1-2 seconds)</li> <li>For background processing, use longer intervals (5-30 seconds)</li> </ul>"},{"location":"advanced/performance/#number-of-workers","title":"Number of Workers","text":"<p>The optimal number of workers depends on:</p> <ol> <li>CPU Cores: For CPU-bound jobs, start with one worker per core</li> <li>I/O Operations: For I/O-bound jobs, you can use more workers than cores</li> <li>SQLite Limitations: Too many concurrent workers can cause lock contention</li> </ol> <p>A simple formula to start with:</p> <pre><code>def calculate_workers(job_type=\"mixed\"):\nimport os\ncores = os.cpu_count() or 4\nif job_type == \"cpu_bound\":\nreturn cores\nelif job_type == \"io_bound\":\nreturn cores * 2\nelse:  # mixed\nreturn cores + 2\n</code></pre>"},{"location":"advanced/performance/#specialized-workers","title":"Specialized Workers","text":"<p>Create specialized workers for different job types:</p> <pre><code>class HighPriorityWorker(Worker):\n\"\"\"Worker that only processes high-priority jobs.\"\"\"\ndef _claim_job(self):\nconn = self._get_connection()\ntry:\nconn.execute(\"BEGIN EXCLUSIVE TRANSACTION\")\ncursor = conn.execute(\n\"\"\"\n                SELECT * FROM jobs\n                WHERE status = ? AND priority &gt;= 50\n                ORDER BY priority DESC, created_at ASC\n                LIMIT 1\n                \"\"\",\n(JobStatus.PENDING.value,)\n)\n# Rest of method implementation...\n</code></pre>"},{"location":"advanced/performance/#worker-process-management","title":"Worker Process Management","text":"<p>For long-running workers, consider using a process manager:</p> <pre><code># Example with multiprocessing\nfrom multiprocessing import Process\nimport os\ndef start_workers(db_path, count=4):\n\"\"\"Start multiple worker processes.\"\"\"\nprocesses = []\nfor i in range(count):\nworker_id = f\"worker-{i+1}\"\np = Process(target=run_worker, args=(db_path, worker_id))\np.daemon = True\np.start()\nprocesses.append(p)\nreturn processes\ndef run_worker(db_path, worker_id):\n\"\"\"Run a worker in a separate process.\"\"\"\nworker = Worker(db_path, worker_id=worker_id)\nworker.start()\n# Usage\nprocs = start_workers(\"jobs.db\", count=4)\n# To stop workers\nfor p in procs:\np.terminate()\n</code></pre>"},{"location":"advanced/performance/#job-optimization","title":"Job Optimization","text":""},{"location":"advanced/performance/#job-batching","title":"Job Batching","text":"<p>Batching small jobs into larger ones reduces overhead:</p> <pre><code># Instead of submitting many small jobs\nfor item in items:\njob = Job(name=f\"process_{item}\", function=process_item, params={\"item\": item})\nqueue.submit(job)\n# Batch them into a single job\ndef process_batch(items):\nresults = []\nfor item in items:\nresults.append(process_item(item))\nreturn results\nbatch_job = Job(name=\"process_batch\", function=process_batch, params={\"items\": items})\nqueue.submit(batch_job)\n</code></pre>"},{"location":"advanced/performance/#job-prioritization","title":"Job Prioritization","text":"<p>Use priorities to ensure important jobs run first:</p> <pre><code># Critical job - will execute first\ncritical_job = Job(\nname=\"critical_task\",\nfunction=critical_function,\npriority=100\n)\n# Standard job - will execute after critical jobs\nstandard_job = Job(\nname=\"standard_task\",\nfunction=standard_function,\npriority=0\n)\n# Background job - will execute when no other jobs are available\nbackground_job = Job(\nname=\"background_task\",\nfunction=background_function,\npriority=-100\n)\n</code></pre>"},{"location":"advanced/performance/#appropriate-timeouts","title":"Appropriate Timeouts","text":"<p>Set job timeouts based on expected execution time:</p> <pre><code># Quick job with short timeout\nquick_job = Job(\nname=\"quick_task\",\nfunction=quick_function,\ntimeout=30  # 30 seconds\n)\n# Long-running job with longer timeout\nlong_job = Job(\nname=\"long_task\",\nfunction=long_function,\ntimeout=3600  # 1 hour\n)\n</code></pre> <p>Appropriate timeouts prevent jobs from getting stuck and blocking workers.</p>"},{"location":"advanced/performance/#efficient-job-functions","title":"Efficient Job Functions","text":"<p>Optimize your job functions for performance:</p> <pre><code># Less efficient version\ndef process_data_inefficient(file_path):\n# Read the entire file into memory\nwith open(file_path, 'r') as f:\ndata = f.read()\n# Process line by line\nlines = data.split('\\n')\nresults = []\nfor line in lines:\nresults.append(process_line(line))\nreturn results\n# More efficient version\ndef process_data_efficient(file_path):\nresults = []\n# Process the file in chunks\nwith open(file_path, 'r') as f:\nfor line in f:\nresults.append(process_line(line.strip()))\nreturn results\n</code></pre>"},{"location":"advanced/performance/#database-cleanup","title":"Database Cleanup","text":"<p>Regular database cleanup prevents performance degradation over time:</p> <pre><code># Automated cleanup job\ndef cleanup_job(db_path, days_to_keep=7):\nqueue = JobQueue(db_path)\n# Calculate cutoff date\nimport datetime\ncutoff = (datetime.datetime.now() - datetime.timedelta(days=days_to_keep)).isoformat()\n# Clear completed and cancelled jobs before cutoff\ncleared = queue.clear_completed(before_timestamp=cutoff)\n# Optimize the database\noptimize_database(db_path)\nreturn {\"cleared\": cleared}\n# Schedule this to run periodically\ncleanup_job_obj = Job(\nname=\"database_cleanup\",\nfunction=cleanup_job,\nparams={\"db_path\": \"jobs.db\", \"days_to_keep\": 7},\ndescription=\"Weekly database cleanup\"\n)\n</code></pre>"},{"location":"advanced/performance/#memory-usage-optimization","title":"Memory Usage Optimization","text":""},{"location":"advanced/performance/#memory-efficient-job-processing","title":"Memory-Efficient Job Processing","text":"<p>For memory-intensive jobs, consider streaming approaches:</p> <pre><code># Memory-intensive approach\ndef analyze_large_file_memory_intensive(file_path):\nimport pandas as pd\n# Load entire dataset into memory\ndf = pd.read_csv(file_path)\nresult = df.groupby('category').sum()\nreturn result.to_dict()\n# Memory-efficient approach\ndef analyze_large_file_memory_efficient(file_path):\nimport csv\nfrom collections import defaultdict\n# Process the file in chunks\nresults = defaultdict(int)\nwith open(file_path, 'r') as f:\nreader = csv.DictReader(f)\nfor row in reader:\ncategory = row['category']\nvalue = float(row['value'])\nresults[category] += value\nreturn dict(results)\n</code></pre>"},{"location":"advanced/performance/#limiting-job-result-size","title":"Limiting Job Result Size","text":"<p>Large job results can consume significant memory:</p> <pre><code># Instead of returning large datasets\ndef job_with_large_result():\n# Process data...\nreturn large_dataset  # Could be megabytes of data\n# Save results to a file and return the file path\ndef job_with_efficient_result():\n# Process data...\nresult_path = f\"results/job_{uuid.uuid4()}.json\"\nos.makedirs(os.path.dirname(result_path), exist_ok=True)\nwith open(result_path, 'w') as f:\njson.dump(large_dataset, f)\nreturn {\"result_file\": result_path}\n</code></pre>"},{"location":"advanced/performance/#benchmarking-and-monitoring","title":"Benchmarking and Monitoring","text":""},{"location":"advanced/performance/#job-performance-metrics","title":"Job Performance Metrics","text":"<p>Track job performance to identify bottlenecks:</p> <pre><code>def job_with_metrics(params):\nimport time\n# Measure execution time\nstart_time = time.time()\n# Process step 1\nstep1_start = time.time()\nstep1_result = process_step1(params)\nstep1_time = time.time() - step1_start\n# Process step 2\nstep2_start = time.time()\nstep2_result = process_step2(step1_result)\nstep2_time = time.time() - step2_start\n# Process step 3\nstep3_start = time.time()\nfinal_result = process_step3(step2_result)\nstep3_time = time.time() - step3_start\ntotal_time = time.time() - start_time\n# Add metrics to the result\nmetrics = {\n\"execution_time\": total_time,\n\"steps\": {\n\"step1\": step1_time,\n\"step2\": step2_time,\n\"step3\": step3_time\n}\n}\nreturn {\n\"result\": final_result,\n\"metrics\": metrics\n}\n</code></pre>"},{"location":"advanced/performance/#queue-monitoring","title":"Queue Monitoring","text":"<p>Monitor queue health to detect performance issues:</p> <pre><code>def monitor_queue_performance(db_path):\n\"\"\"Monitor queue performance metrics.\"\"\"\nimport time\nqueue = JobQueue(db_path)\n# Get initial counts\npending_count_start = len(queue.list_jobs(status=\"pending\"))\n# Submit a benchmark job\nbenchmark_job = Job(\nname=\"benchmark\",\nfunction=lambda: time.sleep(0.1),\nparams={}\n)\n# Measure submission time\nsubmit_start = time.time()\njob_id = queue.submit(benchmark_job)\nsubmit_time = time.time() - submit_start\n# Measure queue processing time\nworker = Worker(db_path)\nprocess_start = time.time()\nworker.process_one()\nprocess_time = time.time() - process_start\n# Get status check time\nstatus_start = time.time()\nqueue.get_status(job_id)\nstatus_time = time.time() - status_start\n# Get final counts\npending_count_end = len(queue.list_jobs(status=\"pending\"))\nreturn {\n\"submit_time\": submit_time,\n\"process_time\": process_time,\n\"status_time\": status_time,\n\"pending_delta\": pending_count_end - pending_count_start\n}\n</code></pre>"},{"location":"advanced/performance/#production-deployment-tips","title":"Production Deployment Tips","text":""},{"location":"advanced/performance/#database-journaling-mode","title":"Database Journaling Mode","text":"<p>Adjust SQLite journaling mode for better performance:</p> <pre><code>def optimize_sqlite_settings(db_path):\n\"\"\"Configure SQLite for better performance.\"\"\"\nconn = sqlite3.connect(db_path)\n# Use WAL mode for better concurrency\nconn.execute(\"PRAGMA journal_mode = WAL\")\n# Set cache size (adjust based on available memory)\nconn.execute(\"PRAGMA cache_size = -10000\")  # ~10MB\n# Control how often SQLite syncs to disk\nconn.execute(\"PRAGMA synchronous = NORMAL\")\nconn.close()\n</code></pre>"},{"location":"advanced/performance/#process-supervision","title":"Process Supervision","text":"<p>In production, use proper process supervision:</p> <pre><code># Example systemd service file for GigQ workers\n[Unit]\nDescription=GigQ Worker\nAfter=network.target\n[Service]\nUser=appuser\nWorkingDirectory=/path/to/app\nExecStart=/path/to/python -m gigq.cli --db /path/to/jobs.db worker\nRestart=always\nRestartSec=10\n[Install]\nWantedBy=multi-user.target\n</code></pre>"},{"location":"advanced/performance/#database-backup","title":"Database Backup","text":"<p>Regularly backup your job queue database:</p> <pre><code>def backup_database(db_path, backup_dir):\n\"\"\"Create a backup of the job queue database.\"\"\"\nimport shutil\nimport datetime\n# Create backup directory if it doesn't exist\nos.makedirs(backup_dir, exist_ok=True)\n# Generate a timestamp\ntimestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n# Create backup file path\nbackup_path = os.path.join(backup_dir, f\"jobs_backup_{timestamp}.db\")\n# Copy the database file\nshutil.copy2(db_path, backup_path)\n# Vacuum the backup to optimize size\nconn = sqlite3.connect(backup_path)\nconn.execute(\"VACUUM\")\nconn.close()\nreturn {\"original\": db_path, \"backup\": backup_path}\n</code></pre>"},{"location":"advanced/performance/#load-testing","title":"Load Testing","text":"<p>Before deploying to production, load test your GigQ setup:</p> <pre><code>def load_test_gigq(db_path, num_jobs=1000, num_workers=4, job_type=\"mixed\"):\n\"\"\"Run a load test for GigQ.\"\"\"\nimport time\nimport random\nimport multiprocessing\n# Create a queue\nqueue = JobQueue(db_path)\n# Define test job functions\ndef cpu_intensive_job(iterations=1000000):\nresult = 0\nfor i in range(iterations):\nresult += i * i\nreturn {\"result\": result}\ndef io_intensive_job(sleep_time=0.1):\ntime.sleep(sleep_time)\nreturn {\"slept_for\": sleep_time}\ndef mixed_job(iterations=10000, sleep_time=0.05):\nresult = 0\nfor i in range(iterations):\nresult += i * i\ntime.sleep(sleep_time)\nreturn {\"result\": result, \"slept_for\": sleep_time}\n# Choose job function based on type\nif job_type == \"cpu\":\njob_func = cpu_intensive_job\nelif job_type == \"io\":\njob_func = io_intensive_job\nelse:\njob_func = mixed_job\n# Submit jobs\nprint(f\"Submitting {num_jobs} jobs...\")\nstart_time = time.time()\nfor i in range(num_jobs):\njob = Job(\nname=f\"load_test_{i}\",\nfunction=job_func,\nparams={\"iterations\": random.randint(10000, 1000000)} if job_type != \"io\" else {\"sleep_time\": random.uniform(0.01, 0.2)},\npriority=random.randint(-10, 10)\n)\nqueue.submit(job)\nsubmit_time = time.time() - start_time\nprint(f\"Submitted {num_jobs} jobs in {submit_time:.2f} seconds\")\n# Start workers\nprint(f\"Starting {num_workers} workers...\")\nprocesses = []\nfor i in range(num_workers):\np = multiprocessing.Process(target=run_worker, args=(db_path, f\"worker-{i+1}\"))\np.start()\nprocesses.append(p)\n# Monitor progress\ntotal_jobs = num_jobs\nwhile True:\ntime.sleep(5)\npending = len(queue.list_jobs(status=\"pending\"))\nrunning = len(queue.list_jobs(status=\"running\"))\ncompleted = len(queue.list_jobs(status=\"completed\"))\nfailed = len(queue.list_jobs(status=\"failed\"))\nprogress = (completed + failed) / total_jobs * 100\nprint(f\"Progress: {progress:.1f}% | Pending: {pending} | Running: {running} | Completed: {completed} | Failed: {failed}\")\nif pending == 0 and running == 0:\nbreak\n# Stop workers\nfor p in processes:\np.terminate()\n# Calculate stats\nend_time = time.time()\ntotal_time = end_time - start_time\nprint(f\"\\nLoad Test Results:\")\nprint(f\"Total time: {total_time:.2f} seconds\")\nprint(f\"Average job time: {total_time / total_jobs:.4f} seconds\")\nprint(f\"Jobs per second: {total_jobs / total_time:.2f}\")\nreturn {\n\"total_time\": total_time,\n\"jobs_per_second\": total_jobs / total_time,\n\"average_job_time\": total_time / total_jobs\n}\ndef run_worker(db_path, worker_id):\n\"\"\"Run a worker for load testing.\"\"\"\nworker = Worker(db_path, worker_id=worker_id, polling_interval=0.1)\nworker.start()\n</code></pre>"},{"location":"advanced/performance/#next-steps","title":"Next Steps","text":"<p>Now that you understand how to optimize GigQ's performance, you might want to explore:</p> <ul> <li>Concurrency - Learn more about GigQ's concurrency model</li> <li>SQLite Schema - Understand the database schema for advanced optimization</li> <li>Custom Job Types - Creating specialized job types</li> </ul>"},{"location":"advanced/sqlite-schema/","title":"SQLite Schema","text":"<p>GigQ uses SQLite as its backend storage. This page details the database schema and explains how it's used to manage jobs and their executions.</p>"},{"location":"advanced/sqlite-schema/#overview","title":"Overview","text":"<p>The database consists of two main tables:</p> <ol> <li><code>jobs</code> - Stores job definitions and current state</li> <li><code>job_executions</code> - Tracks individual execution attempts</li> </ol> <p>This simple, efficient schema is designed to balance simplicity with the features needed for reliable job processing.</p>"},{"location":"advanced/sqlite-schema/#tables-structure","title":"Tables Structure","text":""},{"location":"advanced/sqlite-schema/#jobs-table","title":"Jobs Table","text":"<p>The <code>jobs</code> table stores the core information about each job:</p> <pre><code>CREATE TABLE IF NOT EXISTS jobs (\nid TEXT PRIMARY KEY,\nname TEXT NOT NULL,\nfunction_name TEXT NOT NULL,\nfunction_module TEXT NOT NULL,\nparams TEXT,\npriority INTEGER DEFAULT 0,\ndependencies TEXT,\nmax_attempts INTEGER DEFAULT 3,\ntimeout INTEGER DEFAULT 300,\ndescription TEXT,\nstatus TEXT NOT NULL,\ncreated_at TEXT NOT NULL,\nupdated_at TEXT NOT NULL,\nattempts INTEGER DEFAULT 0,\nresult TEXT,\nerror TEXT,\nstarted_at TEXT,\ncompleted_at TEXT,\nworker_id TEXT\n)\n</code></pre>"},{"location":"advanced/sqlite-schema/#field-descriptions","title":"Field Descriptions","text":"Field Type Description <code>id</code> TEXT Unique identifier for the job (UUID) <code>name</code> TEXT Human-readable name for the job <code>function_name</code> TEXT Name of the function to execute <code>function_module</code> TEXT Module containing the function <code>params</code> TEXT JSON-encoded parameters to pass to the function <code>priority</code> INTEGER Execution priority (higher values execute first) <code>dependencies</code> TEXT JSON-encoded list of job IDs that must complete before this job can run <code>max_attempts</code> INTEGER Maximum number of execution attempts <code>timeout</code> INTEGER Maximum execution time in seconds <code>description</code> TEXT Optional description of the job <code>status</code> TEXT Current job status (pending, running, completed, failed, cancelled, timeout) <code>created_at</code> TEXT ISO-format timestamp of job creation <code>updated_at</code> TEXT ISO-format timestamp of last update <code>attempts</code> INTEGER Number of execution attempts <code>result</code> TEXT JSON-encoded result of the job (if completed) <code>error</code> TEXT Error message (if failed) <code>started_at</code> TEXT ISO-format timestamp of when the job started running <code>completed_at</code> TEXT ISO-format timestamp of when the job completed <code>worker_id</code> TEXT ID of the worker processing the job (if running)"},{"location":"advanced/sqlite-schema/#job-executions-table","title":"Job Executions Table","text":"<p>The <code>job_executions</code> table tracks individual execution attempts:</p> <pre><code>CREATE TABLE IF NOT EXISTS job_executions (\nid TEXT PRIMARY KEY,\njob_id TEXT NOT NULL,\nworker_id TEXT NOT NULL,\nstatus TEXT NOT NULL,\nstarted_at TEXT NOT NULL,\ncompleted_at TEXT,\nresult TEXT,\nerror TEXT,\nFOREIGN KEY (job_id) REFERENCES jobs (id)\n)\n</code></pre>"},{"location":"advanced/sqlite-schema/#field-descriptions_1","title":"Field Descriptions","text":"Field Type Description <code>id</code> TEXT Unique identifier for the execution (UUID) <code>job_id</code> TEXT ID of the job being executed <code>worker_id</code> TEXT ID of the worker executing the job <code>status</code> TEXT Status of this execution (running, completed, failed, timeout) <code>started_at</code> TEXT ISO-format timestamp of when execution started <code>completed_at</code> TEXT ISO-format timestamp of when execution completed (if finished) <code>result</code> TEXT JSON-encoded result of the execution (if completed) <code>error</code> TEXT Error message (if failed)"},{"location":"advanced/sqlite-schema/#indexes","title":"Indexes","text":"<p>GigQ creates several indexes to optimize common operations:</p> <pre><code>CREATE INDEX IF NOT EXISTS idx_jobs_status ON jobs (status)\nCREATE INDEX IF NOT EXISTS idx_jobs_priority ON jobs (priority)\n</code></pre> <p>These indexes allow for efficient:</p> <ul> <li>Retrieval of jobs by status (e.g., finding all pending jobs)</li> <li>Ordering of jobs by priority</li> </ul>"},{"location":"advanced/sqlite-schema/#serialization","title":"Serialization","text":"<p>Several fields in the schema store serialized data:</p> Field Format Example <code>params</code> JSON <code>{\"filename\": \"data.csv\", \"threshold\": 0.7}</code> <code>dependencies</code> JSON array <code>[\"job-id-1\", \"job-id-2\"]</code> <code>result</code> JSON <code>{\"processed\": true, \"count\": 42}</code>"},{"location":"advanced/sqlite-schema/#schema-visualization","title":"Schema Visualization","text":"<p>Here's a visual representation of the database schema:</p> <pre><code>erDiagram\n    JOBS {\n        string id PK\n        string name\n        string function_name\n        string function_module\n        string params\n        int priority\n        string dependencies\n        int max_attempts\n        int timeout\n        string description\n        string status\n        string created_at\n        string updated_at\n        int attempts\n        string result\n        string error\n        string started_at\n        string completed_at\n        string worker_id\n    }\n\n    JOB_EXECUTIONS {\n        string id PK\n        string job_id FK\n        string worker_id\n        string status\n        string started_at\n        string completed_at\n        string result\n        string error\n    }\n\n    JOBS ||--o{ JOB_EXECUTIONS : \"has\"</code></pre>"},{"location":"advanced/sqlite-schema/#how-gigq-uses-the-schema","title":"How GigQ Uses the Schema","text":""},{"location":"advanced/sqlite-schema/#job-claiming","title":"Job Claiming","text":"<p>When a worker claims a job:</p> <ol> <li>It starts an exclusive transaction</li> <li>Finds a pending job with no dependencies or with all dependencies completed</li> <li>Updates the job's status to \"running\", increments the attempts count, and sets the worker_id</li> <li>Creates a new record in job_executions</li> <li>Commits the transaction</li> </ol> <p>This ensures that only one worker claims each job.</p>"},{"location":"advanced/sqlite-schema/#state-transitions","title":"State Transitions","text":"<p>The following diagram illustrates how job states are represented in the database:</p> <pre><code>stateDiagram-v2\n    [*] --&gt; PENDING: INSERT INTO jobs\n    PENDING --&gt; RUNNING: UPDATE jobs SET status='running'\n    RUNNING --&gt; COMPLETED: UPDATE jobs SET status='completed'\n    RUNNING --&gt; FAILED: UPDATE jobs SET status='failed'\n    RUNNING --&gt; PENDING: UPDATE jobs SET status='pending' (retry)\n    RUNNING --&gt; TIMEOUT: UPDATE jobs SET status='timeout'\n    PENDING --&gt; CANCELLED: UPDATE jobs SET status='cancelled'</code></pre>"},{"location":"advanced/sqlite-schema/#handling-dependencies","title":"Handling Dependencies","text":"<p>Job dependencies are stored as a JSON array of job IDs in the <code>dependencies</code> field. When a worker looks for jobs to process, it checks:</p> <ol> <li>Are there any pending jobs with no dependencies?</li> <li>If not, are there pending jobs where all dependencies are completed?</li> </ol> <p>This query is optimized using indexes on the <code>jobs</code> table.</p>"},{"location":"advanced/sqlite-schema/#working-with-the-schema-directly","title":"Working with the Schema Directly","text":"<p>While GigQ's API abstracts the database operations, you can interact with the schema directly for advanced use cases:</p> <pre><code>import sqlite3\nimport json\n# Connect to the database\nconn = sqlite3.connect(\"gigq.db\")\nconn.row_factory = sqlite3.Row\n# Get statistics about jobs\ncursor = conn.execute(\"\"\"\n    SELECT status, COUNT(*) as count\n    FROM jobs\n    GROUP BY status\n    ORDER BY count DESC\n\"\"\")\nstats = {row['status']: row['count'] for row in cursor.fetchall()}\nprint(stats)\n# Find stalled jobs (running for too long)\ncursor = conn.execute(\"\"\"\n    SELECT id, name, started_at\n    FROM jobs\n    WHERE status = 'running'\n    AND datetime(started_at) &lt; datetime('now', '-1 hour')\n\"\"\")\nstalled_jobs = [dict(row) for row in cursor.fetchall()]\nprint(stalled_jobs)\nconn.close()\n</code></pre>"},{"location":"advanced/sqlite-schema/#schema-migrations","title":"Schema Migrations","text":"<p>The schema is initialized when a <code>JobQueue</code> is created. The current version of GigQ doesn't include explicit schema migrations, so if you need to modify the schema:</p> <ol> <li>Backup your database</li> <li>Make changes manually or create a migration script</li> <li>Update your GigQ code to work with the modified schema</li> </ol> <p>In future versions, GigQ may include more formal schema migration support.</p>"},{"location":"advanced/sqlite-schema/#performance-considerations","title":"Performance Considerations","text":"<p>SQLite performs well for most local job processing needs, but consider these factors:</p> <ul> <li>Database Size: If you'll be processing millions of jobs, consider adding a periodic cleanup process</li> <li>Concurrency: SQLite's locking model works well for moderate concurrency (a few dozen workers)</li> <li>Disk Speed: Since SQLite is file-based, disk I/O can impact performance</li> </ul>"},{"location":"advanced/sqlite-schema/#next-steps","title":"Next Steps","text":"<p>Now that you understand the database schema, you might want to explore:</p> <ul> <li>Concurrency - Learn how GigQ handles concurrent job processing</li> <li>Performance - Tips for optimizing performance</li> <li>Custom Job Types - How to extend GigQ with custom job types</li> </ul>"},{"location":"api/cli/","title":"CLI API Reference","text":"<p>This page documents the Command Line Interface (CLI) API for GigQ. The CLI is implemented in the <code>cli.py</code> module and is automatically installed as the <code>gigq</code> command when you install the package.</p>"},{"location":"api/cli/#overview","title":"Overview","text":"<p>The GigQ CLI provides a command-line interface for interacting with job queues, submitting jobs, and managing workers. It's designed to be used both interactively and in scripts.</p>"},{"location":"api/cli/#main-entry-point","title":"Main Entry Point","text":"<pre><code>def main():\n\"\"\"Main CLI entrypoint.\"\"\"\nparser = argparse.ArgumentParser(description=\"GigQ: Lightweight SQLite-backed job queue\")\nparser.add_argument(\"--db\", default=\"gigq.db\", help=\"Path to SQLite database file\")\nsubparsers = parser.add_subparsers(dest=\"command\", help=\"Command to run\")\nsubparsers.required = True\n# Add subcommands...\nargs = parser.parse_args()\nreturn args.func(args)\n</code></pre> <p>This is the main entry point for the CLI, which sets up the argument parser and dispatches to the appropriate command function.</p>"},{"location":"api/cli/#commands","title":"Commands","text":"<p>The CLI supports the following commands:</p>"},{"location":"api/cli/#submit","title":"submit","text":"<pre><code>def cmd_submit(args):\n\"\"\"Submit a job to the queue.\"\"\"\n</code></pre> <p>Submits a job to the queue.</p>"},{"location":"api/cli/#arguments","title":"Arguments","text":"<ul> <li><code>function</code>: The function to execute (in module.function format)</li> <li><code>--name</code>: Name for the job (required)</li> <li><code>--param</code>, <code>-p</code>: Parameters as key=value (can be specified multiple times)</li> <li><code>--priority</code>: Job priority (higher runs first, default: 0)</li> <li><code>--max-attempts</code>: Maximum execution attempts (default: 3)</li> <li><code>--timeout</code>: Timeout in seconds (default: 300)</li> <li><code>--description</code>: Job description</li> </ul>"},{"location":"api/cli/#example","title":"Example","text":"<pre><code>gigq --db jobs.db submit my_module.process_data --name \"Process CSV\" \\\n--param \"filename=data.csv\" --param \"threshold=0.7\" \\\n--priority 10 --max-attempts 5 --timeout 600 \\\n--description \"Process the daily data CSV file\"\n</code></pre>"},{"location":"api/cli/#implementation-details","title":"Implementation Details","text":"<p>The <code>submit</code> command:</p> <ol> <li>Imports the specified function from its module</li> <li>Parses parameters from the command line</li> <li>Creates a <code>Job</code> object</li> <li>Submits the job to the queue</li> <li>Prints the job ID</li> </ol>"},{"location":"api/cli/#status","title":"status","text":"<pre><code>def cmd_status(args):\n\"\"\"Check the status of a job.\"\"\"\n</code></pre> <p>Checks the status of a specific job.</p>"},{"location":"api/cli/#arguments_1","title":"Arguments","text":"<ul> <li><code>job_id</code>: Job ID to check</li> <li><code>--show-params</code>: Show job parameters</li> <li><code>--show-result</code>: Show job result</li> <li><code>--show-executions</code>: Show execution history</li> </ul>"},{"location":"api/cli/#example_1","title":"Example","text":"<pre><code>gigq --db jobs.db status 1a2b3c4d-5e6f-7g8h-9i0j-1k2l3m4n5o6p --show-result\n</code></pre>"},{"location":"api/cli/#implementation-details_1","title":"Implementation Details","text":"<p>The <code>status</code> command:</p> <ol> <li>Gets the job status from the queue</li> <li>Prints job details (ID, name, status, timestamps, etc.)</li> <li>Optionally prints parameters, result, and execution history</li> </ol>"},{"location":"api/cli/#list","title":"list","text":"<pre><code>def cmd_list(args):\n\"\"\"List jobs in the queue.\"\"\"\n</code></pre> <p>Lists jobs in the queue.</p>"},{"location":"api/cli/#arguments_2","title":"Arguments","text":"<ul> <li><code>--status</code>: Filter by status</li> <li><code>--limit</code>: Maximum number of jobs to list (default: 100)</li> </ul>"},{"location":"api/cli/#example_2","title":"Example","text":"<pre><code>gigq --db jobs.db list --status pending --limit 20\n</code></pre>"},{"location":"api/cli/#implementation-details_2","title":"Implementation Details","text":"<p>The <code>list</code> command:</p> <ol> <li>Gets a list of jobs from the queue, optionally filtered by status</li> <li>Formats and prints the jobs in a table using <code>tabulate</code></li> </ol>"},{"location":"api/cli/#cancel","title":"cancel","text":"<pre><code>def cmd_cancel(args):\n\"\"\"Cancel a pending job.\"\"\"\n</code></pre> <p>Cancels a pending job.</p>"},{"location":"api/cli/#arguments_3","title":"Arguments","text":"<ul> <li><code>job_id</code>: Job ID to cancel</li> </ul>"},{"location":"api/cli/#example_3","title":"Example","text":"<pre><code>gigq --db jobs.db cancel 1a2b3c4d-5e6f-7g8h-9i0j-1k2l3m4n5o6p\n</code></pre>"},{"location":"api/cli/#implementation-details_3","title":"Implementation Details","text":"<p>The <code>cancel</code> command:</p> <ol> <li>Attempts to cancel the specified job</li> <li>Prints whether the cancellation was successful</li> </ol>"},{"location":"api/cli/#requeue","title":"requeue","text":"<pre><code>def cmd_requeue(args):\n\"\"\"Requeue a failed job.\"\"\"\n</code></pre> <p>Requeues a failed job.</p>"},{"location":"api/cli/#arguments_4","title":"Arguments","text":"<ul> <li><code>job_id</code>: Job ID to requeue</li> </ul>"},{"location":"api/cli/#example_4","title":"Example","text":"<pre><code>gigq --db jobs.db requeue 1a2b3c4d-5e6f-7g8h-9i0j-1k2l3m4n5o6p\n</code></pre>"},{"location":"api/cli/#implementation-details_4","title":"Implementation Details","text":"<p>The <code>requeue</code> command:</p> <ol> <li>Attempts to requeue the specified job</li> <li>Prints whether the requeue was successful</li> </ol>"},{"location":"api/cli/#clear","title":"clear","text":"<pre><code>def cmd_clear(args):\n\"\"\"Clear completed jobs.\"\"\"\n</code></pre> <p>Clears completed jobs from the queue.</p>"},{"location":"api/cli/#arguments_5","title":"Arguments","text":"<ul> <li><code>--before</code>: Clear jobs completed more than N days ago</li> </ul>"},{"location":"api/cli/#example_5","title":"Example","text":"<pre><code>gigq --db jobs.db clear --before 7\n</code></pre>"},{"location":"api/cli/#implementation-details_5","title":"Implementation Details","text":"<p>The <code>clear</code> command:</p> <ol> <li>Calculates the cutoff timestamp (if <code>--before</code> is specified)</li> <li>Clears completed jobs from the queue</li> <li>Prints the number of jobs cleared</li> </ol>"},{"location":"api/cli/#worker","title":"worker","text":"<pre><code>def cmd_worker(args):\n\"\"\"Start a worker process.\"\"\"\n</code></pre> <p>Starts a worker process.</p>"},{"location":"api/cli/#arguments_6","title":"Arguments","text":"<ul> <li><code>--worker-id</code>: Worker ID (generated if not provided)</li> <li><code>--polling-interval</code>: Polling interval in seconds (default: 5)</li> <li><code>--once</code>: Process one job and exit</li> </ul>"},{"location":"api/cli/#example_6","title":"Example","text":"<pre><code>gigq --db jobs.db worker --polling-interval 2\n</code></pre>"},{"location":"api/cli/#implementation-details_6","title":"Implementation Details","text":"<p>The <code>worker</code> command:</p> <ol> <li>Creates a worker with the specified parameters</li> <li>If <code>--once</code> is specified, processes a single job and exits</li> <li>Otherwise, starts the worker and keeps it running until interrupted</li> </ol>"},{"location":"api/cli/#helper-functions","title":"Helper Functions","text":"<p>The CLI module includes several helper functions:</p>"},{"location":"api/cli/#format_time","title":"format_time","text":"<pre><code>def format_time(timestamp):\n\"\"\"Format a timestamp for display.\"\"\"\n</code></pre> <p>Formats an ISO timestamp for display.</p>"},{"location":"api/cli/#parameters","title":"Parameters","text":"<ul> <li><code>timestamp</code>: ISO format timestamp</li> </ul>"},{"location":"api/cli/#returns","title":"Returns","text":"<p>Formatted timestamp string (YYYY-MM-DD HH:MM:SS)</p>"},{"location":"api/cli/#using-the-cli-in-scripts","title":"Using the CLI in Scripts","text":"<p>The GigQ CLI can be used in shell scripts to automate job management. Here's an example:</p> <pre><code>#!/bin/bash\n# Submit a job\nJOB_ID=$(gigq --db jobs.db submit my_module.process_data --name \"Process Data\" \\\n--param \"filename=data.csv\" | grep -oP 'Job submitted: \\K.*')\necho \"Submitted job: $JOB_ID\"\n# Wait for job to complete\nwhile true; do\nSTATUS=$(gigq --db jobs.db status $JOB_ID | grep -oP 'Status: \\K.*')\necho \"Job status: $STATUS\"\nif [[ \"$STATUS\" == \"completed\" ]]; then\necho \"Job completed successfully!\"\nbreak\nelif [[ \"$STATUS\" == \"failed\" || \"$STATUS\" == \"timeout\" || \"$STATUS\" == \"cancelled\" ]]; then\necho \"Job failed!\"\nexit 1\nfi\nsleep 5\ndone\n# Get the result\ngigq --db jobs.db status $JOB_ID --show-result\n</code></pre>"},{"location":"api/cli/#extending-the-cli","title":"Extending the CLI","text":"<p>You can extend the GigQ CLI by adding new commands. Here's an example of adding a custom command:</p> <pre><code># In your own module\nimport argparse\nfrom gigq import JobQueue\ndef cmd_monthly_report(args):\n\"\"\"Generate a monthly report of job statistics.\"\"\"\nqueue = JobQueue(args.db)\n# Your command implementation\n# ...\nreturn 0\ndef add_to_cli():\n\"\"\"Add custom commands to the GigQ CLI.\"\"\"\n# This function would be called from your application's entry point\nfrom gigq.cli import main_parser\n# Add your command to the parser\nreport_parser = main_parser.add_parser(\"monthly-report\", help=\"Generate monthly job statistics\")\nreport_parser.add_argument(\"--month\", help=\"Month (YYYY-MM format)\")\nreport_parser.set_defaults(func=cmd_monthly_report)\n</code></pre>"},{"location":"api/cli/#environment-variables","title":"Environment Variables","text":"<p>The GigQ CLI respects the following environment variables:</p> <ul> <li><code>GIGQ_DB</code>: Default database path</li> <li><code>GIGQ_WORKER_ID</code>: Default worker ID</li> <li><code>GIGQ_POLLING_INTERVAL</code>: Default polling interval</li> </ul> <p>Example:</p> <pre><code>export GIGQ_DB=/path/to/jobs.db\ngigq list  # Will use /path/to/jobs.db\n</code></pre>"},{"location":"api/cli/#exit-codes","title":"Exit Codes","text":"<p>The GigQ CLI returns the following exit codes:</p> <ul> <li><code>0</code>: Success</li> <li><code>1</code>: Error (e.g., job not found, failed to submit, etc.)</li> </ul>"},{"location":"api/cli/#source-code","title":"Source Code","text":"<p>The full implementation of the CLI can be found in the <code>gigq/cli.py</code> file in the GigQ repository.</p>"},{"location":"api/cli/#see-also","title":"See Also","text":"<ul> <li>CLI User Guide - More detailed information on using the CLI</li> <li>Core API Reference - Documentation for the core GigQ classes and functions</li> </ul>"},{"location":"api/core/","title":"Core API Reference","text":"<p>This page documents the core classes and functions in the GigQ library.</p>"},{"location":"api/core/#jobstatus","title":"JobStatus","text":"<pre><code>class JobStatus(Enum):\n\"\"\"Enum representing the possible states of a job.\"\"\"\nPENDING = \"pending\"\nRUNNING = \"running\"\nCOMPLETED = \"completed\"\nFAILED = \"failed\"\nCANCELLED = \"cancelled\"\nTIMEOUT = \"timeout\"\n</code></pre> <p>An enumeration of possible job states:</p> <ul> <li><code>PENDING</code>: The job is waiting to be executed</li> <li><code>RUNNING</code>: The job is currently being executed by a worker</li> <li><code>COMPLETED</code>: The job has successfully completed</li> <li><code>FAILED</code>: The job has failed after exhausting all retry attempts</li> <li><code>CANCELLED</code>: The job was cancelled by the user</li> <li><code>TIMEOUT</code>: The job execution exceeded the timeout</li> </ul>"},{"location":"api/core/#job","title":"Job","text":"<pre><code>class Job:\n\"\"\"\n    Represents a job to be executed by the queue system.\n    \"\"\"\ndef __init__(\nself,\nname: str,\nfunction: Callable,\nparams: Dict[str, Any] = None,\npriority: int = 0,\ndependencies: List[str] = None,\nmax_attempts: int = 3,\ntimeout: int = 300,\ndescription: str = \"\",\n):\n\"\"\"\n        Initialize a new job.\n        Args:\n            name: A name for the job.\n            function: The function to execute.\n            params: Parameters to pass to the function.\n            priority: Job priority (higher numbers executed first).\n            dependencies: List of job IDs that must complete before this job runs.\n            max_attempts: Maximum number of execution attempts.\n            timeout: Maximum runtime in seconds before the job is considered hung.\n            description: Optional description of the job.\n        \"\"\"\n</code></pre> <p>The <code>Job</code> class represents a unit of work to be executed by the queue system.</p>"},{"location":"api/core/#properties","title":"Properties","text":"Property Type Description <code>id</code> str Unique identifier (UUID) for the job <code>name</code> str Human-readable name for the job <code>function</code> callable The function to execute <code>params</code> dict Parameters to pass to the function <code>priority</code> int Execution priority (higher values run first) <code>dependencies</code> list List of job IDs that must complete before this job runs <code>max_attempts</code> int Maximum number of execution attempts <code>timeout</code> int Maximum runtime in seconds before the job is considered hung <code>description</code> str Optional description of the job <code>created_at</code> str ISO format timestamp of when the job was created"},{"location":"api/core/#example","title":"Example","text":"<pre><code>from gigq import Job\ndef process_data(filename, threshold=0.5):\n# Process data...\nreturn {\"processed\": True, \"count\": 42}\n# Create a job\njob = Job(\nname=\"process_data_job\",\nfunction=process_data,\nparams={\"filename\": \"data.csv\", \"threshold\": 0.7},\npriority=10,\nmax_attempts=3,\ntimeout=300,\ndescription=\"Process the daily data CSV file\"\n)\n</code></pre>"},{"location":"api/core/#jobqueue","title":"JobQueue","text":"<pre><code>class JobQueue:\n\"\"\"\n    Manages a queue of jobs using SQLite as a backend.\n    \"\"\"\ndef __init__(self, db_path: str, initialize: bool = True):\n\"\"\"\n        Initialize the job queue.\n        Args:\n            db_path: Path to the SQLite database file.\n            initialize: Whether to initialize the database if it doesn't exist.\n        \"\"\"\n</code></pre> <p>The <code>JobQueue</code> class manages job persistence, state transitions, and retrieval.</p>"},{"location":"api/core/#methods","title":"Methods","text":""},{"location":"api/core/#submit","title":"submit","text":"<pre><code>def submit(self, job: Job) -&gt; str:\n\"\"\"\n    Submit a job to the queue.\n    Args:\n        job: The job to submit.\n    Returns:\n        The ID of the submitted job.\n    \"\"\"\n</code></pre> <p>Submits a job to the queue and returns its ID.</p>"},{"location":"api/core/#cancel","title":"cancel","text":"<pre><code>def cancel(self, job_id: str) -&gt; bool:\n\"\"\"\n    Cancel a pending job.\n    Args:\n        job_id: The ID of the job to cancel.\n    Returns:\n        True if the job was cancelled, False if it couldn't be cancelled.\n    \"\"\"\n</code></pre> <p>Cancels a pending job. Returns <code>True</code> if the job was cancelled, <code>False</code> otherwise (e.g., if the job is already running or completed).</p>"},{"location":"api/core/#get_status","title":"get_status","text":"<pre><code>def get_status(self, job_id: str) -&gt; Dict[str, Any]:\n\"\"\"\n    Get the current status of a job.\n    Args:\n        job_id: The ID of the job to check.\n    Returns:\n        A dictionary containing the job's status and related information.\n    \"\"\"\n</code></pre> <p>Returns a dictionary with the job's current status and details.</p>"},{"location":"api/core/#list_jobs","title":"list_jobs","text":"<pre><code>def list_jobs(\nself,\nstatus: Optional[Union[JobStatus, str]] = None,\nlimit: int = 100\n) -&gt; List[Dict[str, Any]]:\n\"\"\"\n    List jobs in the queue, optionally filtered by status.\n    Args:\n        status: Filter jobs by this status.\n        limit: Maximum number of jobs to return.\n    Returns:\n        A list of job dictionaries.\n    \"\"\"\n</code></pre> <p>Returns a list of jobs, optionally filtered by status.</p>"},{"location":"api/core/#clear_completed","title":"clear_completed","text":"<pre><code>def clear_completed(self, before_timestamp: Optional[str] = None) -&gt; int:\n\"\"\"\n    Clear completed jobs from the queue.\n    Args:\n        before_timestamp: Only clear jobs completed before this timestamp.\n    Returns:\n        Number of jobs cleared.\n    \"\"\"\n</code></pre> <p>Removes completed and cancelled jobs from the queue.</p>"},{"location":"api/core/#requeue_job","title":"requeue_job","text":"<pre><code>def requeue_job(self, job_id: str) -&gt; bool:\n\"\"\"\n    Requeue a failed job, resetting its attempts.\n    Args:\n        job_id: The ID of the job to requeue.\n    Returns:\n        True if the job was requeued, False if not.\n    \"\"\"\n</code></pre> <p>Resets a failed job to pending status for another attempt.</p>"},{"location":"api/core/#example_1","title":"Example","text":"<pre><code>from gigq import JobQueue, Job\n# Create a job queue\nqueue = JobQueue(\"jobs.db\")\n# Submit a job\njob = Job(name=\"example\", function=example_function)\njob_id = queue.submit(job)\n# Check job status\nstatus = queue.get_status(job_id)\nprint(f\"Job status: {status['status']}\")\n# List pending jobs\npending_jobs = queue.list_jobs(status=\"pending\")\n# Cancel a job\nif queue.cancel(job_id):\nprint(f\"Job {job_id} cancelled\")\n</code></pre>"},{"location":"api/core/#worker","title":"Worker","text":"<pre><code>class Worker:\n\"\"\"\n    A worker that processes jobs from the queue.\n    \"\"\"\ndef __init__(\nself,\ndb_path: str,\nworker_id: Optional[str] = None,\npolling_interval: int = 5\n):\n\"\"\"\n        Initialize a worker.\n        Args:\n            db_path: Path to the SQLite database file.\n            worker_id: Unique identifier for this worker (auto-generated if not provided).\n            polling_interval: How often to check for new jobs, in seconds.\n        \"\"\"\n</code></pre> <p>The <code>Worker</code> class processes jobs from the queue.</p>"},{"location":"api/core/#methods_1","title":"Methods","text":""},{"location":"api/core/#start","title":"start","text":"<pre><code>def start(self):\n\"\"\"Start the worker process.\"\"\"\n</code></pre> <p>Starts the worker, which will continuously process jobs until stopped.</p>"},{"location":"api/core/#stop","title":"stop","text":"<pre><code>def stop(self):\n\"\"\"Stop the worker process.\"\"\"\n</code></pre> <p>Stops the worker after completing its current job (if any).</p>"},{"location":"api/core/#process_one","title":"process_one","text":"<pre><code>def process_one(self) -&gt; bool:\n\"\"\"\n    Process a single job from the queue.\n    Returns:\n        True if a job was processed, False if no job was available.\n    \"\"\"\n</code></pre> <p>Processes a single job from the queue and returns.</p>"},{"location":"api/core/#example_2","title":"Example","text":"<pre><code>from gigq import Worker\n# Create a worker\nworker = Worker(\"jobs.db\")\n# Process a single job\nif worker.process_one():\nprint(\"Processed one job\")\nelse:\nprint(\"No jobs available\")\n# Start the worker (blocks until stopped)\nworker.start()\n</code></pre>"},{"location":"api/core/#workflow","title":"Workflow","text":"<pre><code>class Workflow:\n\"\"\"\n    A utility class to help define workflows of dependent jobs.\n    \"\"\"\ndef __init__(self, name: str):\n\"\"\"\n        Initialize a new workflow.\n        Args:\n            name: Name of the workflow.\n        \"\"\"\n</code></pre> <p>The <code>Workflow</code> class helps define and manage workflows with dependent jobs.</p>"},{"location":"api/core/#methods_2","title":"Methods","text":""},{"location":"api/core/#add_job","title":"add_job","text":"<pre><code>def add_job(self, job: Job, depends_on: List[Job] = None) -&gt; Job:\n\"\"\"\n    Add a job to the workflow, with optional dependencies.\n    Args:\n        job: The job to add.\n        depends_on: List of jobs this job depends on.\n    Returns:\n        The job that was added.\n    \"\"\"\n</code></pre> <p>Adds a job to the workflow, optionally specifying dependencies.</p>"},{"location":"api/core/#submit_all","title":"submit_all","text":"<pre><code>def submit_all(self, queue: JobQueue) -&gt; List[str]:\n\"\"\"\n    Submit all jobs in the workflow to a queue.\n    Args:\n        queue: The job queue to submit to.\n    Returns:\n        List of job IDs that were submitted.\n    \"\"\"\n</code></pre> <p>Submits all jobs in the workflow to the queue.</p>"},{"location":"api/core/#example_3","title":"Example","text":"<pre><code>from gigq import Workflow, Job, JobQueue\n# Create a workflow\nworkflow = Workflow(\"data_pipeline\")\n# Define jobs\njob1 = Job(name=\"download\", function=download_data)\njob2 = Job(name=\"process\", function=process_data)\njob3 = Job(name=\"analyze\", function=analyze_data)\n# Add jobs with dependencies\nworkflow.add_job(job1)\nworkflow.add_job(job2, depends_on=[job1])\nworkflow.add_job(job3, depends_on=[job2])\n# Submit all jobs\nqueue = JobQueue(\"workflow.db\")\njob_ids = workflow.submit_all(queue)\nprint(f\"Submitted {len(job_ids)} jobs\")\n</code></pre>"},{"location":"api/core/#complete-usage-example","title":"Complete Usage Example","text":"<p>Here's a complete example showing how to use the core GigQ API:</p> <pre><code>import time\nfrom gigq import Job, JobQueue, Worker, Workflow, JobStatus\n# Define job functions\ndef download_data(url):\nprint(f\"Downloading data from {url}\")\ntime.sleep(1)  # Simulate work\nreturn {\"downloaded\": True, \"url\": url, \"bytes\": 1024}\ndef process_data(downloaded_info):\nprint(f\"Processing data from {downloaded_info['url']}\")\ntime.sleep(2)  # Simulate work\nreturn {\"processed\": True, \"records\": 42}\ndef generate_report(processing_result):\nprint(f\"Generating report for {processing_result['records']} records\")\ntime.sleep(1)  # Simulate work\nreturn {\"report_generated\": True, \"pages\": 5}\n# Create a job queue\nqueue = JobQueue(\"example.db\")\n# Create a workflow\nworkflow = Workflow(\"data_pipeline\")\n# Define jobs\ndownload_job = Job(\nname=\"download_data\",\nfunction=download_data,\nparams={\"url\": \"https://example.com/data.csv\"}\n)\nprocess_job = Job(\nname=\"process_data\",\nfunction=process_data,\nparams={\"downloaded_info\": {\"url\": \"https://example.com/data.csv\", \"bytes\": 1024}}\n)\nreport_job = Job(\nname=\"generate_report\",\nfunction=generate_report,\nparams={\"processing_result\": {\"records\": 42}}\n)\n# Add jobs to workflow with dependencies\nworkflow.add_job(download_job)\nworkflow.add_job(process_job, depends_on=[download_job])\nworkflow.add_job(report_job, depends_on=[process_job])\n# Submit all jobs\njob_ids = workflow.submit_all(queue)\nprint(f\"Submitted {len(job_ids)} jobs\")\n# Create a worker and process jobs\nworker = Worker(\"example.db\")\n# Process jobs one at a time\nfor _ in range(3):\nif worker.process_one():\nprint(\"Processed one job\")\nelse:\nprint(\"No jobs available\")\n# Check status of all jobs\nfor job_id in job_ids:\nstatus = queue.get_status(job_id)\nprint(f\"Job {status['name']}: {status['status']}\")\n</code></pre>"},{"location":"api/core/#extension-points","title":"Extension Points","text":"<p>GigQ is designed to be extensible. Here are some common extension points:</p>"},{"location":"api/core/#custom-job-types","title":"Custom Job Types","text":"<p>You can create subclasses of <code>Job</code> for specific types of jobs:</p> <pre><code>class DataProcessingJob(Job):\n\"\"\"A specialized job for data processing tasks.\"\"\"\ndef __init__(self, name, function, input_file, output_file, **kwargs):\nparams = {\n\"input_file\": input_file,\n\"output_file\": output_file\n}\nsuper().__init__(name, function, params=params, **kwargs)\nself.input_file = input_file\nself.output_file = output_file\n</code></pre>"},{"location":"api/core/#custom-worker-logic","title":"Custom Worker Logic","text":"<p>You can subclass <code>Worker</code> to customize job processing behavior:</p> <pre><code>class PrioritizedWorker(Worker):\n\"\"\"A worker that only processes high-priority jobs.\"\"\"\ndef _claim_job(self):\n\"\"\"Claim a job from the queue, but only high-priority ones.\"\"\"\nconn = self._get_connection()\ntry:\nconn.execute(\"BEGIN EXCLUSIVE TRANSACTION\")\ncursor = conn.execute(\n\"\"\"\n                SELECT * FROM jobs\n                WHERE status = ? AND priority &gt; 50\n                ORDER BY priority DESC, created_at ASC\n                LIMIT 1\n                \"\"\",\n(JobStatus.PENDING.value,)\n)\n# Rest of the method follows the original implementation...\n</code></pre>"},{"location":"api/core/#custom-queue-persistence","title":"Custom Queue Persistence","text":"<p>While GigQ uses SQLite by default, you could extend the <code>JobQueue</code> class to use a different backend:</p> <pre><code>class PostgresJobQueue(JobQueue):\n\"\"\"A job queue that uses PostgreSQL as a backend.\"\"\"\ndef __init__(self, connection_string):\nself.connection_string = connection_string\nself._initialize_db()\ndef _get_connection(self):\n\"\"\"Get a connection to the PostgreSQL database.\"\"\"\nimport psycopg2\nconn = psycopg2.connect(self.connection_string)\nreturn conn\ndef _initialize_db(self):\n\"\"\"Create the necessary database tables if they don't exist.\"\"\"\nconn = self._get_connection()\ncursor = conn.cursor()\n# Create tables with PostgreSQL syntax\ncursor.execute('''\n        CREATE TABLE IF NOT EXISTS jobs (\n            id TEXT PRIMARY KEY,\n            name TEXT NOT NULL,\n            -- Rest of the schema...\n        )\n        ''')\n# Create indices\ncursor.execute('CREATE INDEX IF NOT EXISTS idx_jobs_status ON jobs (status)')\nconn.commit()\nconn.close()\n</code></pre>"},{"location":"api/core/#internal-api-methods","title":"Internal API Methods","text":"<p>The following methods are used internally by GigQ and are not typically called directly by users, but understanding them can be helpful for advanced use cases or extending GigQ.</p>"},{"location":"api/core/#worker-internal-methods","title":"Worker Internal Methods","text":""},{"location":"api/core/#_get_connection","title":"<code>_get_connection()</code>","text":"<p>Gets a connection to the SQLite database with appropriate settings.</p>"},{"location":"api/core/#_import_functionmodule_name-function_name","title":"<code>_import_function(module_name, function_name)</code>","text":"<p>Dynamically imports a function from a module.</p>"},{"location":"api/core/#_claim_job","title":"<code>_claim_job()</code>","text":"<p>Attempts to claim a job from the queue using an exclusive transaction.</p>"},{"location":"api/core/#_complete_jobjob_id-execution_id-status-result-error","title":"<code>_complete_job(job_id, execution_id, status, result, error)</code>","text":"<p>Marks a job as completed or failed.</p>"},{"location":"api/core/#_check_for_timeouts","title":"<code>_check_for_timeouts()</code>","text":"<p>Checks for jobs that have timed out and marks them accordingly.</p>"},{"location":"api/core/#jobqueue-internal-methods","title":"JobQueue Internal Methods","text":""},{"location":"api/core/#_initialize_db","title":"<code>_initialize_db()</code>","text":"<p>Creates the necessary database tables if they don't exist.</p>"},{"location":"api/core/#_get_connection_1","title":"<code>_get_connection()</code>","text":"<p>Gets a connection to the SQLite database with appropriate settings.</p>"},{"location":"api/core/#thread-safety-and-concurrency","title":"Thread Safety and Concurrency","text":"<p>GigQ is designed to work safely with multiple workers. Key points to understand:</p> <ul> <li>Job claiming is done in an exclusive transaction to ensure only one worker claims a job.</li> <li>Job state transitions are atomic.</li> <li>SQLite locking is used to manage concurrent access to the database.</li> </ul> <p>When running multiple workers:</p> <ul> <li>They can be in separate processes or threads.</li> <li>They can be on different machines if they all have access to the same database file (e.g., via a network share).</li> <li>Workers will automatically respect job priorities and dependencies.</li> </ul>"},{"location":"api/core/#error-handling","title":"Error Handling","text":"<p>GigQ includes several mechanisms for handling errors:</p> <ul> <li>Automatic retries based on the <code>max_attempts</code> setting.</li> <li>Timeout detection to recover from hung jobs.</li> <li>Transaction-based state transitions to maintain consistency even when errors occur.</li> <li>Detailed error logging to aid in debugging.</li> </ul> <p>The <code>JobQueue</code> and <code>Worker</code> classes include appropriate error handling to ensure that database operations are safe and consistent.</p>"},{"location":"api/core/#performance-considerations","title":"Performance Considerations","text":"<p>When using GigQ, keep these performance considerations in mind:</p> <ul> <li>Worker polling interval affects both responsiveness and database load.</li> <li>Job timeouts should be set appropriately for the expected runtime.</li> <li>Database file location can impact performance, especially over network shares.</li> <li>Number of workers should be balanced with available system resources.</li> <li>Job priority can be used to ensure critical jobs are processed first.</li> </ul>"},{"location":"api/core/#next-steps","title":"Next Steps","text":"<p>Now that you understand the core API, you might want to explore:</p> <ul> <li>CLI API Reference - Documentation for the command-line interface</li> <li>Job Queue Management - More information on managing job queues</li> <li>Workers - More information on worker configuration and usage</li> <li>Workflows - More information on creating complex workflows</li> </ul>"},{"location":"development/contributing/","title":"Contributing to GigQ","text":"<p>Thank you for your interest in contributing to GigQ! This guide will help you get started with the development process and outline how to contribute to the project.</p>"},{"location":"development/contributing/#code-of-conduct","title":"Code of Conduct","text":"<p>By participating in this project, you agree to adhere to our Code of Conduct. Please treat other contributors with respect and maintain a positive and inclusive environment.</p>"},{"location":"development/contributing/#getting-started","title":"Getting Started","text":""},{"location":"development/contributing/#prerequisites","title":"Prerequisites","text":"<p>To contribute to GigQ, you'll need:</p> <ol> <li>Python 3.7 or newer</li> <li>Git</li> <li>A GitHub account</li> </ol>"},{"location":"development/contributing/#setting-up-your-development-environment","title":"Setting Up Your Development Environment","text":"<ol> <li> <p>Fork the repository on GitHub.</p> </li> <li> <p>Clone your fork to your local machine:</p> </li> </ol> <pre><code>git clone https://github.com/YOUR-USERNAME/gigq.git\ncd gigq\n</code></pre> <ol> <li>Create a virtual environment:</li> </ol> <pre><code>python -m venv env\nsource env/bin/activate  # On Windows: env\\Scripts\\activate\n</code></pre> <ol> <li>Install the package in development mode:</li> </ol> <pre><code>pip install -e .\n</code></pre> <ol> <li>Install development dependencies:    <pre><code>pip install -r requirements-dev.txt\n</code></pre></li> </ol>"},{"location":"development/contributing/#development-workflow","title":"Development Workflow","text":""},{"location":"development/contributing/#creating-a-branch","title":"Creating a Branch","text":"<p>Before making changes, create a branch from the main branch:</p> <pre><code>git checkout -b feature/your-feature-name\n</code></pre> <p>or</p> <pre><code>git checkout -b bugfix/issue-number-description\n</code></pre>"},{"location":"development/contributing/#making-changes","title":"Making Changes","text":"<ol> <li>Make your changes to the code.</li> <li>Add tests for any new functionality or bug fixes.</li> <li>Ensure all tests pass:    <pre><code>python -m unittest discover tests\n</code></pre></li> <li>Run linting to ensure code quality:    <pre><code>flake8 gigq tests\n</code></pre></li> </ol>"},{"location":"development/contributing/#committing-changes","title":"Committing Changes","text":"<ol> <li>Stage your changes:</li> </ol> <pre><code>git add .\n</code></pre> <ol> <li>Commit your changes with a descriptive message:</li> </ol> <pre><code>git commit -m \"Add feature: description of your feature\"\n</code></pre> <ol> <li>Push your changes to your fork:    <pre><code>git push origin feature/your-feature-name\n</code></pre></li> </ol>"},{"location":"development/contributing/#submitting-a-pull-request","title":"Submitting a Pull Request","text":"<ol> <li>Go to the original GigQ repository on GitHub.</li> <li>Click on \"Pull Requests\" and then \"New Pull Request\".</li> <li>Click \"compare across forks\" and select your fork and branch.</li> <li>Add a title and description for your pull request.</li> <li>Submit the pull request.</li> </ol>"},{"location":"development/contributing/#coding-standards","title":"Coding Standards","text":""},{"location":"development/contributing/#python-style-guide","title":"Python Style Guide","text":"<p>GigQ follows the PEP 8 style guide for Python code. Additionally:</p> <ul> <li>Use 4 spaces for indentation (no tabs).</li> <li>Add docstrings for all modules, classes, and functions.</li> <li>Keep line length to a maximum of 88 characters.</li> <li>Use descriptive variable names.</li> </ul>"},{"location":"development/contributing/#docstrings","title":"Docstrings","text":"<p>Use Google-style docstrings:</p> <pre><code>def function_example(param1, param2):\n\"\"\"\n    A brief description of what the function does.\n    Args:\n        param1: Description of param1\n        param2: Description of param2\n    Returns:\n        Description of the return value\n    Raises:\n        ExceptionType: When and why this exception is raised\n    \"\"\"\n</code></pre>"},{"location":"development/contributing/#type-hints","title":"Type Hints","text":"<p>Use type hints for function parameters and return values:</p> <pre><code>def function_with_type_hints(param1: str, param2: int) -&gt; Dict[str, Any]:\n\"\"\"\n    Example function with type hints.\n    \"\"\"\nreturn {\"param1\": param1, \"param2\": param2}\n</code></pre>"},{"location":"development/contributing/#testing","title":"Testing","text":""},{"location":"development/contributing/#writing-tests","title":"Writing Tests","text":"<p>All new features and bug fixes should include tests. GigQ uses Python's built-in <code>unittest</code> framework:</p> <pre><code>import unittest\nfrom gigq import Job, JobQueue\nclass TestJobQueue(unittest.TestCase):\ndef setUp(self):\n# Setup code\nself.queue = JobQueue(\":memory:\")\ndef test_submit_job(self):\njob = Job(name=\"test\", function=lambda: None)\njob_id = self.queue.submit(job)\nself.assertIsNotNone(job_id)\ndef tearDown(self):\n# Cleanup code\npass\n</code></pre>"},{"location":"development/contributing/#running-tests","title":"Running Tests","text":"<p>Run the entire test suite:</p> <pre><code>python -m unittest discover tests\n</code></pre> <p>Run a specific test:</p> <pre><code>python -m unittest tests.test_gigq.TestJobQueue.test_submit_job\n</code></pre>"},{"location":"development/contributing/#documentation","title":"Documentation","text":""},{"location":"development/contributing/#updating-documentation","title":"Updating Documentation","text":"<p>GigQ uses MkDocs with the Material theme for documentation. To update documentation:</p> <ol> <li>Edit the markdown files in the <code>docs/</code> directory.</li> <li>Preview your changes locally:    <pre><code>mkdocs serve\n</code></pre></li> <li>Ensure your documentation changes are included in your pull request.</li> </ol>"},{"location":"development/contributing/#adding-examples","title":"Adding Examples","text":"<p>If you're adding a new feature, consider adding an example to the <code>examples/</code> directory to demonstrate its usage. Make sure to update the documentation to reference your example.</p>"},{"location":"development/contributing/#git-commit-messages","title":"Git Commit Messages","text":"<ul> <li>Use the present tense (\"Add feature\" not \"Added feature\")</li> <li>Use the imperative mood (\"Move cursor to...\" not \"Moves cursor to...\")</li> <li>Limit the first line to 72 characters or less</li> <li>Reference issues and pull requests after the first line</li> </ul>"},{"location":"development/contributing/#releasing","title":"Releasing","text":"<p>The release process is handled by the project maintainers. If you're interested in helping with releases, please contact one of the maintainers.</p>"},{"location":"development/contributing/#getting-help","title":"Getting Help","text":"<p>If you need help with contributing to GigQ, you can:</p> <ul> <li>Open an issue on GitHub</li> <li>Reach out to the project maintainers</li> <li>Check the documentation for guidance</li> </ul>"},{"location":"development/contributing/#thank-you","title":"Thank You!","text":"<p>Thank you for contributing to GigQ! Your efforts help make this project better for everyone.</p>"},{"location":"development/roadmap/","title":"Project Roadmap","text":"<p>This page outlines the planned features and enhancements for future versions of GigQ. While this roadmap represents our current intentions, it may change based on user feedback and evolving priorities.</p>"},{"location":"development/roadmap/#current-version-010","title":"Current Version (0.1.0)","text":"<p>The initial release of GigQ includes:</p> <ul> <li>Core job queue functionality with SQLite backend</li> <li>Job definition with parameters, priorities, and dependencies</li> <li>Worker implementation for job processing</li> <li>Workflow support for dependent jobs</li> <li>Command-line interface for job and worker management</li> <li>Automatic retry and timeout handling</li> <li>GitHub Archive processing example</li> </ul>"},{"location":"development/roadmap/#short-term-goals-020","title":"Short-Term Goals (0.2.0)","text":"<p>These features are planned for the next release:</p>"},{"location":"development/roadmap/#enhanced-monitoring-and-metrics","title":"Enhanced Monitoring and Metrics","text":"<ul> <li> Built-in job execution metrics (runtime, memory usage, etc.)</li> <li> Health check endpoints</li> </ul>"},{"location":"development/roadmap/#improved-error-handling","title":"Improved Error Handling","text":"<ul> <li> Custom error handling strategies</li> <li> Configurable retry policies</li> <li> Job resumability for certain error types</li> </ul>"},{"location":"development/roadmap/#cli-enhancements","title":"CLI Enhancements","text":"<ul> <li> Interactive mode for job submission and monitoring</li> <li> Auto-completion for CLI commands</li> <li> Rich terminal output with colors and formatting</li> <li> Export job results to various formats (CSV, JSON, etc.)</li> </ul>"},{"location":"development/roadmap/#job-scheduling","title":"Job Scheduling","text":"<ul> <li> Built-in support for scheduled/recurring jobs</li> <li> Calendar-based scheduling (e.g., \"first Monday of the month\")</li> </ul>"},{"location":"development/roadmap/#documentation-and-examples","title":"Documentation and Examples","text":"<ul> <li> Additional examples for common use cases</li> </ul>"},{"location":"development/roadmap/#medium-term-goals-030","title":"Medium-Term Goals (0.3.0)","text":"<p>These features are targeted for subsequent releases:</p>"},{"location":"development/roadmap/#alternative-backends","title":"Alternative Backends","text":"<ul> <li> Redis backend support</li> <li> PostgreSQL backend support</li> </ul>"},{"location":"development/roadmap/#advanced-job-management","title":"Advanced Job Management","text":"<ul> <li> Job priorities with preemption</li> <li> Resource-aware job scheduling</li> <li> Job quotas and rate limiting</li> <li> Job groups and batch operations</li> </ul>"},{"location":"development/roadmap/#workflow-enhancements","title":"Workflow Enhancements","text":"<ul> <li> Conditional branches in workflows</li> <li> Workflow templates and reusable components</li> </ul>"},{"location":"development/roadmap/#long-term-vision","title":"Long-Term Vision","text":"<p>Looking further ahead, here are some aspirational goals:</p>"},{"location":"development/roadmap/#plugin-system","title":"Plugin System","text":"<ul> <li> Extensible plugin architecture</li> <li> Custom job types</li> <li> Notification plugins (Slack, Email, etc.)</li> </ul>"},{"location":"development/roadmap/#contributing-to-the-roadmap","title":"Contributing to the Roadmap","text":"<p>We welcome community input on the GigQ roadmap. If you have ideas or would like to contribute to any of these initiatives:</p> <ol> <li>Open an Issue: Share your ideas by opening an issue on our GitHub repository</li> <li>Start a Discussion: Join our discussions about future features</li> <li>Submit a Pull Request: Implement a feature and submit a pull request</li> <li>Provide Feedback: Tell us about your use cases and what features would be most valuable</li> </ol> <p>When suggesting new features, please consider:</p> <ul> <li>How the feature aligns with GigQ's philosophy of simplicity and reliability</li> <li>The target audience for the feature</li> <li>Potential implementation challenges</li> <li>How the feature would be tested</li> </ul>"},{"location":"development/roadmap/#release-planning","title":"Release Planning","text":"<p>We follow semantic versioning:</p> <ul> <li>Major version (x.0.0): Incompatible API changes</li> <li>Minor version (0.x.0): Backwards-compatible new features</li> <li>Patch version (0.0.x): Backwards-compatible bug fixes</li> </ul>"},{"location":"development/roadmap/#experimental-features","title":"Experimental Features","text":"<p>Some features may be introduced as experimental before being officially supported:</p> <ul> <li> Graph-based workflow representation</li> <li> Specialized job types for common tasks</li> </ul> <p>Experimental features will be clearly marked and may change or be removed in future releases.</p>"},{"location":"development/roadmap/#deprecation-policy","title":"Deprecation Policy","text":"<p>As GigQ evolves, some features may need to be deprecated:</p> <ol> <li>Features will be marked as deprecated but will continue to function</li> <li>Deprecation warnings will be issued when using deprecated features</li> <li>Deprecated features will be removed after at least two minor versions</li> <li>Documentation will provide migration guidance</li> </ol>"},{"location":"development/roadmap/#stay-informed","title":"Stay Informed","text":"<p>To stay updated on GigQ's development:</p> <ul> <li>Watch our GitHub repository</li> <li>Follow the release announcements</li> </ul> <p>We're excited about GigQ's future and look forward to your contributions!</p>"},{"location":"development/testing/","title":"Testing GigQ","text":"<p>This guide explains how to test GigQ, both for developers contributing to the project and for users who want to ensure GigQ works correctly in their environment.</p>"},{"location":"development/testing/#testing-philosophy","title":"Testing Philosophy","text":"<p>GigQ follows these testing principles:</p> <ol> <li>Comprehensive Coverage: All core functionality should be tested</li> <li>Isolated Tests: Tests should be independent of each other</li> <li>Fast Execution: The test suite should run quickly</li> <li>Simple Setup: Tests should be easy to run</li> </ol>"},{"location":"development/testing/#test-structure","title":"Test Structure","text":"<p>The tests are organized in the <code>tests/</code> directory, with the main test file being <code>test_gigq.py</code>. Tests are structured using Python's built-in <code>unittest</code> framework.</p> <p>The test suite includes:</p> <ul> <li>Unit Tests: Testing individual functions and methods</li> <li>Integration Tests: Testing interactions between components</li> <li>Functional Tests: Testing end-to-end functionality</li> </ul>"},{"location":"development/testing/#running-tests","title":"Running Tests","text":""},{"location":"development/testing/#basic-test-execution","title":"Basic Test Execution","text":"<p>To run the complete test suite:</p> <pre><code>python -m unittest discover tests\n</code></pre> <p>To run a specific test file:</p> <pre><code>python -m unittest tests.test_gigq\n</code></pre> <p>To run a specific test class:</p> <pre><code>python -m unittest tests.test_gigq.TestJobQueue\n</code></pre> <p>To run a specific test method:</p> <pre><code>python -m unittest tests.test_gigq.TestJobQueue.test_submit_job\n</code></pre>"},{"location":"development/testing/#running-tests-with-coverage","title":"Running Tests with Coverage","text":"<p>To run tests with coverage reporting:</p> <pre><code># Install coverage if you haven't already\npip install coverage\n\n# Run tests with coverage\ncoverage run -m unittest discover tests\n\n# Generate a coverage report\ncoverage report -m\n\n# Generate an HTML coverage report\ncoverage html\n</code></pre> <p>The HTML report will be available in the <code>htmlcov/</code> directory.</p>"},{"location":"development/testing/#writing-tests","title":"Writing Tests","text":""},{"location":"development/testing/#test-file-organization","title":"Test File Organization","text":"<p>Each test file should:</p> <ol> <li>Import the necessary modules</li> <li>Define test classes that inherit from <code>unittest.TestCase</code></li> <li>Include <code>setUp</code> and <code>tearDown</code> methods if needed</li> <li>Define test methods that start with <code>test_</code></li> </ol> <p>Example:</p> <pre><code>\"\"\"\nTests for the GigQ job queue functionality.\n\"\"\"\nimport unittest\nimport tempfile\nimport os\nfrom gigq import JobQueue, Job\nclass TestJobQueue(unittest.TestCase):\n\"\"\"Tests for the JobQueue class.\"\"\"\ndef setUp(self):\n\"\"\"Set up a temporary database for testing.\"\"\"\nself.db_fd, self.db_path = tempfile.mkstemp()\nself.queue = JobQueue(self.db_path)\ndef tearDown(self):\n\"\"\"Clean up the temporary database.\"\"\"\nos.close(self.db_fd)\nos.unlink(self.db_path)\ndef test_submit_job(self):\n\"\"\"Test that a job can be submitted to the queue.\"\"\"\njob = Job(\nname=\"test_job\",\nfunction=lambda: {\"result\": \"success\"}\n)\njob_id = self.queue.submit(job)\nself.assertEqual(job_id, job.id)\n# Check that the job was stored correctly\nstatus = self.queue.get_status(job_id)\nself.assertTrue(status[\"exists\"])\nself.assertEqual(status[\"name\"], \"test_job\")\n</code></pre>"},{"location":"development/testing/#test-case-best-practices","title":"Test Case Best Practices","text":"<p>When writing test cases, follow these best practices:</p> <ol> <li>Test One Thing: Each test method should test one specific piece of functionality</li> <li>Descriptive Names: Use descriptive method names that explain what's being tested</li> <li>Arrange, Act, Assert: Structure your tests with setup, execution, and verification</li> <li>Minimize Dependencies: Avoid dependencies between test cases</li> <li>Clean Up: Always clean up resources in <code>tearDown</code> methods</li> </ol> <p>Example of a well-structured test:</p> <pre><code>def test_job_timeout_detection(self):\n\"\"\"Test that the worker detects timed out jobs.\"\"\"\n# Arrange - Set up the test conditions\njob = Job(\nname=\"long_job\",\nfunction=lambda: time.sleep(2),\ntimeout=1\n)\njob_id = self.queue.submit(job)\n# Act - Execute the functionality being tested\nworker = Worker(self.db_path)\nworker.process_one()  # This should time out\n# Assert - Verify the results\nstatus = self.queue.get_status(job_id)\nself.assertEqual(status[\"status\"], \"timeout\")\n</code></pre>"},{"location":"development/testing/#testing-async-code","title":"Testing Async Code","text":"<p>For testing asynchronous code or long-running jobs, you may need to use timeouts and polling:</p> <pre><code>def test_concurrent_workers(self):\n\"\"\"Test that multiple workers can process jobs concurrently.\"\"\"\n# Submit multiple jobs\njob_ids = []\nfor i in range(5):\njob = Job(\nname=f\"concurrent_job_{i}\",\nfunction=lambda i=i: {\"job_number\": i}\n)\njob_id = self.queue.submit(job)\njob_ids.append(job_id)\n# Start multiple workers in separate threads\nworkers = []\nfor i in range(3):\nworker = Worker(self.db_path, worker_id=f\"worker-{i}\")\nthread = threading.Thread(target=worker.start)\nthread.daemon = True\nthread.start()\nworkers.append((worker, thread))\n# Wait for all jobs to complete (with timeout)\ndeadline = time.time() + 10  # 10 second timeout\nwhile time.time() &lt; deadline:\nstatuses = [self.queue.get_status(job_id)[\"status\"] for job_id in job_ids]\nif all(status == \"completed\" for status in statuses):\nbreak\ntime.sleep(0.1)\n# Stop all workers\nfor worker, _ in workers:\nworker.stop()\n# Verify all jobs completed\nfor job_id in job_ids:\nstatus = self.queue.get_status(job_id)\nself.assertEqual(status[\"status\"], \"completed\")\n</code></pre>"},{"location":"development/testing/#mocking-dependencies","title":"Mocking Dependencies","text":"<p>For testing components in isolation, use <code>unittest.mock</code> to mock dependencies:</p> <pre><code>from unittest.mock import MagicMock, patch\ndef test_worker_process_one_with_mock():\n\"\"\"Test worker.process_one with mocked job function.\"\"\"\njob = Job(\nname=\"mocked_job\",\nfunction=lambda: None  # This will be mocked\n)\njob_id = self.queue.submit(job)\n# Mock the _import_function method\nmock_function = MagicMock(return_value={\"mocked\": True})\nwith patch.object(Worker, '_import_function', return_value=mock_function):\nworker = Worker(self.db_path)\nworker.process_one()\n# Verify the mock was called\nmock_function.assert_called_once()\n# Verify the job is marked as completed\nstatus = self.queue.get_status(job_id)\nself.assertEqual(status[\"status\"], \"completed\")\n</code></pre>"},{"location":"development/testing/#test-data","title":"Test Data","text":""},{"location":"development/testing/#creating-test-data","title":"Creating Test Data","text":"<p>For tests that require specific data:</p> <ol> <li>Small Data: Include directly in the test</li> <li>Medium Data: Create in <code>setUp</code> method</li> <li>Large Data: Use fixtures in separate files</li> </ol> <p>Example with medium data in <code>setUp</code>:</p> <pre><code>def setUp(self):\n\"\"\"Set up test data.\"\"\"\nself.db_fd, self.db_path = tempfile.mkstemp()\nself.queue = JobQueue(self.db_path)\n# Create test jobs\nself.test_jobs = []\nfor i in range(10):\njob = Job(\nname=f\"test_job_{i}\",\nfunction=lambda i=i: {\"job_number\": i},\npriority=i\n)\njob_id = self.queue.submit(job)\nself.test_jobs.append(job_id)\n</code></pre>"},{"location":"development/testing/#testing-with-fixtures","title":"Testing with Fixtures","text":"<p>For larger test data, use fixtures:</p> <pre><code>import json\nimport os\ndef load_test_data(filename):\n\"\"\"Load test data from a fixture file.\"\"\"\nfixture_path = os.path.join(os.path.dirname(__file__), 'fixtures', filename)\nwith open(fixture_path, 'r') as f:\nreturn json.load(f)\nclass TestLargeDataset(unittest.TestCase):\ndef setUp(self):\nself.test_data = load_test_data('large_dataset.json')\n</code></pre>"},{"location":"development/testing/#testing-specific-components","title":"Testing Specific Components","text":""},{"location":"development/testing/#testing-job-class","title":"Testing Job Class","text":"<pre><code>class TestJob(unittest.TestCase):\n\"\"\"Tests for the Job class.\"\"\"\ndef test_job_initialization(self):\n\"\"\"Test that a job can be initialized with the correct parameters.\"\"\"\njob = Job(\nname=\"test_job\",\nfunction=lambda x: x * 2,\nparams={\"x\": 42},\npriority=5,\ndependencies=[\"job1\", \"job2\"],\nmax_attempts=2,\ntimeout=120,\ndescription=\"A test job\"\n)\nself.assertEqual(job.name, \"test_job\")\nself.assertEqual(job.params, {\"x\": 42})\nself.assertEqual(job.priority, 5)\nself.assertEqual(job.dependencies, [\"job1\", \"job2\"])\nself.assertEqual(job.max_attempts, 2)\nself.assertEqual(job.timeout, 120)\nself.assertEqual(job.description, \"A test job\")\nself.assertTrue(job.id)  # ID should be generated\n</code></pre>"},{"location":"development/testing/#testing-jobqueue-class","title":"Testing JobQueue Class","text":"<pre><code>class TestJobQueue(unittest.TestCase):\n\"\"\"Tests for the JobQueue class.\"\"\"\ndef setUp(self):\n\"\"\"Set up a temporary database for testing.\"\"\"\nself.db_fd, self.db_path = tempfile.mkstemp()\nself.queue = JobQueue(self.db_path)\ndef tearDown(self):\n\"\"\"Clean up the temporary database.\"\"\"\nos.close(self.db_fd)\nos.unlink(self.db_path)\ndef test_submit_job(self):\n\"\"\"Test that a job can be submitted to the queue.\"\"\"\n# Test code...\ndef test_get_status(self):\n\"\"\"Test that job status can be retrieved.\"\"\"\n# Test code...\ndef test_list_jobs(self):\n\"\"\"Test that jobs can be listed from the queue.\"\"\"\n# Test code...\ndef test_cancel_job(self):\n\"\"\"Test that a pending job can be cancelled.\"\"\"\n# Test code...\ndef test_requeue_job(self):\n\"\"\"Test that a failed job can be requeued.\"\"\"\n# Test code...\ndef test_clear_completed(self):\n\"\"\"Test that completed jobs can be cleared.\"\"\"\n# Test code...\n</code></pre>"},{"location":"development/testing/#testing-worker-class","title":"Testing Worker Class","text":"<pre><code>class TestWorker(unittest.TestCase):\n\"\"\"Tests for the Worker class.\"\"\"\ndef setUp(self):\n\"\"\"Set up a temporary database for testing.\"\"\"\nself.db_fd, self.db_path = tempfile.mkstemp()\nself.queue = JobQueue(self.db_path)\ndef tearDown(self):\n\"\"\"Clean up the temporary database.\"\"\"\nos.close(self.db_fd)\nos.unlink(self.db_path)\ndef test_process_one_job(self):\n\"\"\"Test that a worker can process a job.\"\"\"\n# Test code...\ndef test_process_failing_job(self):\n\"\"\"Test that a worker handles failing jobs correctly.\"\"\"\n# Test code...\ndef test_timeout_detection(self):\n\"\"\"Test that the worker detects timed out jobs.\"\"\"\n# Test code...\ndef test_worker_stop(self):\n\"\"\"Test that a worker can be stopped.\"\"\"\n# Test code...\n</code></pre>"},{"location":"development/testing/#testing-workflow-class","title":"Testing Workflow Class","text":"<pre><code>class TestWorkflow(unittest.TestCase):\n\"\"\"Tests for the Workflow class.\"\"\"\ndef setUp(self):\n\"\"\"Set up a temporary database for testing.\"\"\"\nself.db_fd, self.db_path = tempfile.mkstemp()\nself.queue = JobQueue(self.db_path)\ndef tearDown(self):\n\"\"\"Clean up the temporary database.\"\"\"\nos.close(self.db_fd)\nos.unlink(self.db_path)\ndef test_workflow_dependencies(self):\n\"\"\"Test that workflow dependencies are set correctly.\"\"\"\n# Test code...\ndef test_workflow_submission(self):\n\"\"\"Test that a workflow can be submitted.\"\"\"\n# Test code...\ndef test_complex_workflow(self):\n\"\"\"Test a complex workflow with multiple dependencies.\"\"\"\n# Test code...\n</code></pre>"},{"location":"development/testing/#testing-the-cli","title":"Testing the CLI","text":"<p>To test the command-line interface:</p> <pre><code>import sys\nimport io\nfrom contextlib import redirect_stdout\nfrom gigq.cli import main\nclass TestCLI(unittest.TestCase):\n\"\"\"Tests for the command-line interface.\"\"\"\ndef setUp(self):\n\"\"\"Set up a temporary database for testing.\"\"\"\nself.db_fd, self.db_path = tempfile.mkstemp()\ndef tearDown(self):\n\"\"\"Clean up the temporary database.\"\"\"\nos.close(self.db_fd)\nos.unlink(self.db_path)\ndef test_submit_command(self):\n\"\"\"Test the 'submit' command.\"\"\"\n# Prepare test arguments\nsys.argv = [\n'gigq',\n'--db', self.db_path,\n'submit', 'builtins.print',\n'--name', 'test_job',\n'--param', 'message=Hello'\n]\n# Capture stdout\nf = io.StringIO()\nwith redirect_stdout(f):\nexit_code = main()\n# Check the output\noutput = f.getvalue()\nself.assertEqual(exit_code, 0)\nself.assertIn(\"Job submitted:\", output)\ndef test_list_command(self):\n\"\"\"Test the 'list' command.\"\"\"\n# First submit a job\nsys.argv = [\n'gigq',\n'--db', self.db_path,\n'submit', 'builtins.print',\n'--name', 'test_job'\n]\nmain()\n# Then list jobs\nsys.argv = [\n'gigq',\n'--db', self.db_path,\n'list'\n]\n# Capture stdout\nf = io.StringIO()\nwith redirect_stdout(f):\nexit_code = main()\n# Check the output\noutput = f.getvalue()\nself.assertEqual(exit_code, 0)\nself.assertIn(\"test_job\", output)\n</code></pre>"},{"location":"development/testing/#integration-testing","title":"Integration Testing","text":"<p>Integration tests verify that different components work together:</p> <pre><code>class TestIntegration(unittest.TestCase):\n\"\"\"Integration tests for GigQ.\"\"\"\ndef setUp(self):\n\"\"\"Set up a temporary database for testing.\"\"\"\nself.db_fd, self.db_path = tempfile.mkstemp()\ndef tearDown(self):\n\"\"\"Clean up the temporary database.\"\"\"\nos.close(self.db_fd)\nos.unlink(self.db_path)\ndef test_end_to_end_workflow(self):\n\"\"\"Test an end-to-end workflow from submission to completion.\"\"\"\n# Create a queue\nqueue = JobQueue(self.db_path)\n# Create a workflow\nworkflow = Workflow(\"test_workflow\")\n# Define test functions\ndef step1():\nreturn {\"step1_complete\": True}\ndef step2(step1_result):\nreturn {\"step2_complete\": True, \"step1_result\": step1_result}\n# Create jobs\njob1 = Job(name=\"step1\", function=step1)\njob2 = Job(name=\"step2\", function=step2, params={\"step1_result\": {\"step1_complete\": True}})\n# Add jobs to workflow\nworkflow.add_job(job1)\nworkflow.add_job(job2, depends_on=[job1])\n# Submit workflow\njob_ids = workflow.submit_all(queue)\n# Create a worker and process jobs\nworker = Worker(self.db_path)\n# Process first job\nself.assertTrue(worker.process_one())\n# Check first job status\nstatus1 = queue.get_status(job_ids[0])\nself.assertEqual(status1[\"status\"], \"completed\")\n# Process second job\nself.assertTrue(worker.process_one())\n# Check second job status\nstatus2 = queue.get_status(job_ids[1])\nself.assertEqual(status2[\"status\"], \"completed\")\nself.assertEqual(status2[\"result\"][\"step2_complete\"], True)\n</code></pre>"},{"location":"development/testing/#performance-testing","title":"Performance Testing","text":"<p>Test performance characteristics:</p> <pre><code>class TestPerformance(unittest.TestCase):\n\"\"\"Performance tests for GigQ.\"\"\"\ndef setUp(self):\n\"\"\"Set up a temporary database for testing.\"\"\"\nself.db_fd, self.db_path = tempfile.mkstemp()\nself.queue = JobQueue(self.db_path)\ndef tearDown(self):\n\"\"\"Clean up the temporary database.\"\"\"\nos.close(self.db_fd)\nos.unlink(self.db_path)\ndef test_job_submission_performance(self):\n\"\"\"Test the performance of job submission.\"\"\"\nimport time\nnum_jobs = 1000\nstart_time = time.time()\n# Submit many jobs\nfor i in range(num_jobs):\njob = Job(\nname=f\"perf_job_{i}\",\nfunction=lambda: None\n)\nself.queue.submit(job)\nelapsed = time.time() - start_time\njobs_per_second = num_jobs / elapsed\nprint(f\"Submitted {num_jobs} jobs in {elapsed:.2f} seconds ({jobs_per_second:.2f} jobs/sec)\")\n# Assert that performance is reasonable\nself.assertGreater(jobs_per_second, 50)  # At least 50 jobs per second\n</code></pre>"},{"location":"development/testing/#continuous-integration","title":"Continuous Integration","text":"<p>GigQ uses GitHub Actions for continuous integration. The CI pipeline runs:</p> <ol> <li>Linting: Checks code style with flake8</li> <li>Type Checking: Verifies type hints with mypy</li> <li>Unit Tests: Runs the test suite</li> <li>Coverage: Ensures code coverage meets thresholds</li> </ol> <p>You can run the same checks locally:</p> <pre><code># Run linting\nflake8 gigq tests\n\n# Run type checking\nmypy gigq\n\n# Run tests with coverage\ncoverage run -m unittest discover tests\ncoverage report -m\n</code></pre>"},{"location":"development/testing/#troubleshooting-tests","title":"Troubleshooting Tests","text":""},{"location":"development/testing/#common-issues","title":"Common Issues","text":"<ol> <li> <p>Database Locking: If tests fail with database locking errors, ensure proper cleanup in <code>tearDown</code> methods.</p> </li> <li> <p>Race Conditions: If tests involving multiple workers are flaky, add appropriate synchronization or timeouts.</p> </li> <li> <p>Resource Leaks: If tests leave behind temporary files, check that all file handles are properly closed.</p> </li> </ol>"},{"location":"development/testing/#debugging-tests","title":"Debugging Tests","text":"<p>For debugging tests:</p> <pre><code># Run with increased verbosity\npython -m unittest discover tests -v\n\n# Add print statements for debugging\ndef test_problematic_function(self):\n    print(\"Starting test\")\nresult = problematic_function()\nprint(f\"Result: {result}\")\nself.assertTrue(result)\n</code></pre>"},{"location":"development/testing/#next-steps","title":"Next Steps","text":"<p>Now that you understand how to test GigQ, you might want to explore:</p> <ul> <li>Contributing Guide - Learn how to contribute to GigQ</li> <li>Project Roadmap - See what's planned for future development</li> </ul>"},{"location":"examples/data-processing/","title":"Data Processing Pipeline Example","text":"<p>This example demonstrates how to use GigQ to build a data processing pipeline that downloads, processes, and analyzes CSV data.</p>"},{"location":"examples/data-processing/#overview","title":"Overview","text":"<p>This data processing pipeline:</p> <ol> <li>Downloads a CSV file from a remote source</li> <li>Cleans and transforms the data</li> <li>Performs analysis and generates reports</li> <li>Stores results in a database</li> <li>Sends a notification when complete</li> </ol>"},{"location":"examples/data-processing/#complete-example","title":"Complete Example","text":"<p>Here's the full code example:</p> <pre><code>\"\"\"\nExample of a data processing pipeline using GigQ.\n\"\"\"\nimport os\nimport pandas as pd\nimport sqlite3\nimport requests\nimport smtplib\nimport logging\nfrom email.message import EmailMessage\nfrom datetime import datetime\nfrom gigq import Job, JobQueue, Worker, Workflow\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger('data_processing_pipeline')\n# Database to store results\nRESULTS_DB = \"data_processing_results.db\"\ndef initialize_db():\n\"\"\"Initialize the results database schema.\"\"\"\nconn = sqlite3.connect(RESULTS_DB)\ncursor = conn.cursor()\n# Create tables for data analysis\ncursor.execute('''\n    CREATE TABLE IF NOT EXISTS processed_data (\n        id INTEGER PRIMARY KEY AUTOINCREMENT,\n        date TEXT,\n        category TEXT,\n        value REAL,\n        source_file TEXT,\n        processed_at TEXT\n    )\n    ''')\ncursor.execute('''\n    CREATE TABLE IF NOT EXISTS analysis_results (\n        id INTEGER PRIMARY KEY AUTOINCREMENT,\n        date TEXT,\n        category TEXT,\n        avg_value REAL,\n        min_value REAL,\n        max_value REAL,\n        count INTEGER,\n        analysis_date TEXT\n    )\n    ''')\nconn.commit()\nconn.close()\ndef download_data(url, output_file):\n\"\"\"\n    Download data from a URL and save to a file.\n    Args:\n        url: URL to download data from\n        output_file: Path to save the downloaded file\n    Returns:\n        Dictionary with download information\n    \"\"\"\nlogger.info(f\"Downloading data from {url} to {output_file}\")\n# Ensure output directory exists\nos.makedirs(os.path.dirname(output_file), exist_ok=True)\n# Download the file\nresponse = requests.get(url, timeout=30)\nresponse.raise_for_status()\nwith open(output_file, 'wb') as f:\nf.write(response.content)\nfile_size = os.path.getsize(output_file)\nlogger.info(f\"Downloaded {file_size} bytes to {output_file}\")\nreturn {\n\"url\": url,\n\"output_file\": output_file,\n\"file_size\": file_size,\n\"download_time\": datetime.now().isoformat()\n}\ndef clean_data(input_file, output_file):\n\"\"\"\n    Clean and transform the data.\n    Args:\n        input_file: Path to the input CSV file\n        output_file: Path to save the cleaned CSV file\n    Returns:\n        Dictionary with cleaning information\n    \"\"\"\nlogger.info(f\"Cleaning data from {input_file}\")\n# Ensure output directory exists\nos.makedirs(os.path.dirname(output_file), exist_ok=True)\n# Read the data\ndf = pd.read_csv(input_file)\ninitial_rows = len(df)\n# Clean the data\n# 1. Remove rows with missing values\ndf = df.dropna()\n# 2. Convert date column to standard format\nif 'date' in df.columns:\ndf['date'] = pd.to_datetime(df['date']).dt.strftime('%Y-%m-%d')\n# 3. Normalize category names\nif 'category' in df.columns:\ndf['category'] = df['category'].str.lower().str.strip()\n# 4. Remove outliers (simple z-score approach)\nif 'value' in df.columns:\nz_scores = (df['value'] - df['value'].mean()) / df['value'].std()\ndf = df[abs(z_scores) &lt; 3]\n# Save the cleaned data\ndf.to_csv(output_file, index=False)\nfinal_rows = len(df)\nrows_removed = initial_rows - final_rows\nlogger.info(f\"Cleaned data: {rows_removed} rows removed, {final_rows} rows remaining\")\nreturn {\n\"input_file\": input_file,\n\"output_file\": output_file,\n\"initial_rows\": initial_rows,\n\"final_rows\": final_rows,\n\"rows_removed\": rows_removed,\n\"columns\": list(df.columns)\n}\ndef analyze_data(clean_file):\n\"\"\"\n    Analyze the cleaned data.\n    Args:\n        clean_file: Path to the cleaned CSV file\n    Returns:\n        Dictionary with analysis results\n    \"\"\"\nlogger.info(f\"Analyzing data from {clean_file}\")\n# Read the data\ndf = pd.read_csv(clean_file)\n# Check required columns\nrequired_cols = ['date', 'category', 'value']\nif not all(col in df.columns for col in required_cols):\nmissing = [col for col in required_cols if col not in df.columns]\nraise ValueError(f\"Missing required columns: {missing}\")\n# Perform analysis\nanalysis_date = datetime.now().isoformat()\nresults = []\n# Analyze by category\ncategory_analysis = df.groupby('category')['value'].agg(['mean', 'min', 'max', 'count'])\ncategory_analysis = category_analysis.reset_index()\n# Store results in database\nconn = sqlite3.connect(RESULTS_DB)\ncursor = conn.cursor()\n# First, store all processed data\nfor _, row in df.iterrows():\ncursor.execute(\n'''\n            INSERT INTO processed_data (date, category, value, source_file, processed_at)\n            VALUES (?, ?, ?, ?, ?)\n            ''',\n(\nrow['date'],\nrow['category'],\nrow['value'],\nclean_file,\nanalysis_date\n)\n)\n# Then store analysis results\nfor _, row in category_analysis.iterrows():\ncursor.execute(\n'''\n            INSERT INTO analysis_results\n            (category, avg_value, min_value, max_value, count, analysis_date)\n            VALUES (?, ?, ?, ?, ?, ?)\n            ''',\n(\nrow['category'],\nrow['mean'],\nrow['min'],\nrow['max'],\nrow['count'],\nanalysis_date\n)\n)\n# Add to results list\nresults.append({\n\"category\": row['category'],\n\"avg_value\": row['mean'],\n\"min_value\": row['min'],\n\"max_value\": row['max'],\n\"count\": row['count']\n})\nconn.commit()\nconn.close()\nlogger.info(f\"Analysis complete: {len(results)} categories analyzed\")\nreturn {\n\"file\": clean_file,\n\"categories\": len(results),\n\"total_records\": len(df),\n\"results\": results,\n\"analysis_date\": analysis_date\n}\ndef generate_report(analysis_results, output_file):\n\"\"\"\n    Generate a report based on the analysis results.\n    Args:\n        analysis_results: Results from the analyze_data function\n        output_file: Path to save the report\n    Returns:\n        Dictionary with report information\n    \"\"\"\nlogger.info(f\"Generating report to {output_file}\")\n# Ensure output directory exists\nos.makedirs(os.path.dirname(output_file), exist_ok=True)\n# Create a simple HTML report\nhtml = f\"\"\"\n    &lt;html&gt;\n    &lt;head&gt;\n        &lt;title&gt;Data Analysis Report&lt;/title&gt;\n        &lt;style&gt;\n            body {{ font-family: Arial, sans-serif; margin: 20px; }}\n            table {{ border-collapse: collapse; width: 100%; }}\n            th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}\n            th {{ background-color: #f2f2f2; }}\n            tr:nth-child(even) {{ background-color: #f9f9f9; }}\n        &lt;/style&gt;\n    &lt;/head&gt;\n    &lt;body&gt;\n        &lt;h1&gt;Data Analysis Report&lt;/h1&gt;\n        &lt;p&gt;Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}&lt;/p&gt;\n        &lt;p&gt;Source file: {analysis_results['file']}&lt;/p&gt;\n        &lt;p&gt;Total records: {analysis_results['total_records']}&lt;/p&gt;\n        &lt;h2&gt;Results by Category&lt;/h2&gt;\n        &lt;table&gt;\n            &lt;tr&gt;\n                &lt;th&gt;Category&lt;/th&gt;\n                &lt;th&gt;Average Value&lt;/th&gt;\n                &lt;th&gt;Minimum Value&lt;/th&gt;\n                &lt;th&gt;Maximum Value&lt;/th&gt;\n                &lt;th&gt;Count&lt;/th&gt;\n            &lt;/tr&gt;\n    \"\"\"\nfor result in analysis_results['results']:\nhtml += f\"\"\"\n            &lt;tr&gt;\n                &lt;td&gt;{result['category']}&lt;/td&gt;\n                &lt;td&gt;{result['avg_value']:.2f}&lt;/td&gt;\n                &lt;td&gt;{result['min_value']:.2f}&lt;/td&gt;\n                &lt;td&gt;{result['max_value']:.2f}&lt;/td&gt;\n                &lt;td&gt;{result['count']}&lt;/td&gt;\n            &lt;/tr&gt;\n        \"\"\"\nhtml += \"\"\"\n        &lt;/table&gt;\n    &lt;/body&gt;\n    &lt;/html&gt;\n    \"\"\"\nwith open(output_file, 'w') as f:\nf.write(html)\nlogger.info(f\"Report generated to {output_file}\")\nreturn {\n\"output_file\": output_file,\n\"categories\": analysis_results['categories'],\n\"total_records\": analysis_results['total_records'],\n\"generated_at\": datetime.now().isoformat()\n}\ndef send_notification(report_info, recipients):\n\"\"\"\n    Send a notification email about the completed report.\n    Args:\n        report_info: Information about the generated report\n        recipients: List of email addresses to notify\n    Returns:\n        Dictionary with notification information\n    \"\"\"\nlogger.info(f\"Sending notification to {recipients}\")\n# Create the email\nmsg = EmailMessage()\nmsg['Subject'] = 'Data Analysis Report Completed'\nmsg['From'] = 'noreply@example.com'\nmsg['To'] = ', '.join(recipients)\n# Customize based on whether the email will actually be sent\nif os.environ.get('SMTP_SERVER'):\n# Email content\nmsg.set_content(f\"\"\"\n        Hello,\n        A new data analysis report has been generated.\n        Report details:\n        - Generated at: {report_info['generated_at']}\n        - Categories analyzed: {report_info['categories']}\n        - Total records: {report_info['total_records']}\n        - Report file: {report_info['output_file']}\n        Please review the report at your earliest convenience.\n        Regards,\n        Data Processing Pipeline\n        \"\"\")\n# Actually send the email\ntry:\nsmtp_server = os.environ.get('SMTP_SERVER')\nsmtp_port = int(os.environ.get('SMTP_PORT', 587))\nsmtp_user = os.environ.get('SMTP_USER')\nsmtp_pass = os.environ.get('SMTP_PASS')\nwith smtplib.SMTP(smtp_server, smtp_port) as server:\nserver.starttls()\nif smtp_user and smtp_pass:\nserver.login(smtp_user, smtp_pass)\nserver.send_message(msg)\nlogger.info(f\"Notification sent to {recipients}\")\nsent = True\nexcept Exception as e:\nlogger.error(f\"Failed to send notification: {e}\")\nsent = False\nelse:\n# Log the email content for demo purposes\nlogger.info(f\"Notification would be sent to {recipients} (SMTP not configured)\")\nsent = False\nreturn {\n\"recipients\": recipients,\n\"subject\": msg['Subject'],\n\"sent\": sent,\n\"sent_at\": datetime.now().isoformat() if sent else None\n}\ndef build_data_processing_workflow(data_url, base_output_dir):\n\"\"\"\n    Build a workflow for processing data.\n    Args:\n        data_url: URL to download data from\n        base_output_dir: Base directory for output files\n    Returns:\n        A Workflow object\n    \"\"\"\n# Ensure the results database is initialized\ninitialize_db()\n# Create timestamp for this run\ntimestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n# Set up file paths\nraw_file = f\"{base_output_dir}/raw/data_{timestamp}.csv\"\nclean_file = f\"{base_output_dir}/clean/data_{timestamp}.csv\"\nreport_file = f\"{base_output_dir}/reports/report_{timestamp}.html\"\n# Create a workflow\nworkflow = Workflow(f\"data_processing_{timestamp}\")\n# Define jobs\ndownload_job = Job(\nname=f\"download_{timestamp}\",\nfunction=download_data,\nparams={'url': data_url, 'output_file': raw_file},\nmax_attempts=3,\ntimeout=300,  # 5 minutes\ndescription=f\"Download data from {data_url}\"\n)\nclean_job = Job(\nname=f\"clean_{timestamp}\",\nfunction=clean_data,\nparams={'input_file': raw_file, 'output_file': clean_file},\nmax_attempts=2,\ntimeout=600,  # 10 minutes\ndescription=f\"Clean data from {raw_file}\"\n)\nanalyze_job = Job(\nname=f\"analyze_{timestamp}\",\nfunction=analyze_data,\nparams={'clean_file': clean_file},\nmax_attempts=2,\ntimeout=600,  # 10 minutes\ndescription=f\"Analyze data from {clean_file}\"\n)\nreport_job = Job(\nname=f\"report_{timestamp}\",\nfunction=generate_report,\nparams={'analysis_results': None, 'output_file': report_file},\nmax_attempts=2,\ntimeout=300,  # 5 minutes\ndescription=f\"Generate report to {report_file}\"\n)\nnotify_job = Job(\nname=f\"notify_{timestamp}\",\nfunction=send_notification,\nparams={\n'report_info': None,\n'recipients': ['user@example.com']\n},\nmax_attempts=3,\ntimeout=180,  # 3 minutes\ndescription=\"Send notification email\"\n)\n# Add jobs to workflow with dependencies\nworkflow.add_job(download_job)\nworkflow.add_job(clean_job, depends_on=[download_job])\nworkflow.add_job(analyze_job, depends_on=[clean_job])\nworkflow.add_job(report_job, depends_on=[analyze_job])\nworkflow.add_job(notify_job, depends_on=[report_job])\nreturn workflow\nif __name__ == \"__main__\":\nimport sys\nif len(sys.argv) &lt; 2:\nprint(\"Usage: python data_processing_pipeline.py URL [output_dir]\")\nsys.exit(1)\ndata_url = sys.argv[1]\nbase_output_dir = sys.argv[2] if len(sys.argv) &gt; 2 else \"data\"\n# Create the job queue\nqueue = JobQueue(\"data_processing_jobs.db\")\n# Build and submit the workflow\nworkflow = build_data_processing_workflow(data_url, base_output_dir)\njob_ids = workflow.submit_all(queue)\nprint(f\"Submitted workflow with {len(job_ids)} jobs\")\n# Start a worker if requested\nif \"--worker\" in sys.argv:\nworker = Worker(\"data_processing_jobs.db\")\nprint(\"Starting worker...\")\nworker.start()\n</code></pre>"},{"location":"examples/data-processing/#running-the-example","title":"Running the Example","text":"<p>To run this example:</p> <pre><code># Install required dependencies\npip install gigq pandas requests\n\n# Run the example specifying a data URL and output directory\npython data_processing_pipeline.py https://example.com/data.csv data/\n\n# Run and also start a worker\npython data_processing_pipeline.py https://example.com/data.csv data/ --worker\n\n# Or start workers separately\ngigq --db data_processing_jobs.db worker\n</code></pre>"},{"location":"examples/data-processing/#workflow-visualization","title":"Workflow Visualization","text":"<p>Here's a visual representation of the workflow:</p> <pre><code>graph TD\n    A[Download Data] --&gt; B[Clean Data]\n    B --&gt; C[Analyze Data]\n    C --&gt; D[Generate Report]\n    D --&gt; E[Send Notification]</code></pre>"},{"location":"examples/data-processing/#key-features-demonstrated","title":"Key Features Demonstrated","text":"<p>This example demonstrates several important features of GigQ:</p> <ol> <li>Sequential Dependencies: Jobs run in a specific order (download \u2192 clean \u2192 analyze \u2192 report \u2192 notify)</li> <li>Error Handling: Each job has configured retry attempts</li> <li>Timeouts: Each job has an appropriate timeout</li> <li>Persistence: All job results are stored in a SQLite database</li> <li>Parameterization: Jobs pass results to subsequent jobs</li> <li>Logging: Comprehensive logging throughout the pipeline</li> </ol>"},{"location":"examples/data-processing/#extensions-and-improvements","title":"Extensions and Improvements","text":"<p>This example can be extended in several ways:</p> <ol> <li>Parallel Processing: Add parallel processing for multiple data files:</li> </ol> <pre><code># Create multiple download jobs\ndownload_jobs = []\nfor url in data_urls:\njob = Job(name=f\"download_{url}\", function=download_data, params={\"url\": url, ...})\nworkflow.add_job(job)\ndownload_jobs.append(job)\n# Create a job that depends on all downloads\ncombine_job = Job(name=\"combine\", function=combine_data, ...)\nworkflow.add_job(combine_job, depends_on=download_jobs)\n</code></pre> <ol> <li>Data Validation: Add validation steps before processing:</li> </ol> <pre><code>def validate_data(file_path):\ndf = pd.read_csv(file_path)\n# Perform validation checks\nif validation_failed:\nraise ValueError(\"Validation failed\")\nreturn {\"valid\": True, \"file\": file_path}\nvalidate_job = Job(name=\"validate\", function=validate_data, ...)\nworkflow.add_job(validate_job, depends_on=[download_job])\nworkflow.add_job(clean_job, depends_on=[validate_job])\n</code></pre> <ol> <li>Multiple Output Formats: Generate multiple report formats:</li> </ol> <pre><code>html_report_job = Job(name=\"html_report\", function=generate_html_report, ...)\npdf_report_job = Job(name=\"pdf_report\", function=generate_pdf_report, ...)\ncsv_report_job = Job(name=\"csv_report\", function=generate_csv_report, ...)\nworkflow.add_job(html_report_job, depends_on=[analyze_job])\nworkflow.add_job(pdf_report_job, depends_on=[analyze_job])\nworkflow.add_job(csv_report_job, depends_on=[analyze_job])\n# Notify after all reports are generated\nworkflow.add_job(notify_job, depends_on=[html_report_job, pdf_report_job, csv_report_job])\n</code></pre>"},{"location":"examples/data-processing/#next-steps","title":"Next Steps","text":"<p>Now that you've seen a data processing pipeline example, you might want to explore:</p> <ul> <li>GitHub Archive Example - Another example using GigQ for GitHub data processing</li> <li>Scheduled Tasks Example - How to schedule recurring jobs with GigQ</li> <li>Error Handling - More detailed information on handling errors in GigQ</li> </ul>"},{"location":"examples/github-archive/","title":"GitHub Archive Processing Example","text":"<p>This example demonstrates how to use GigQ to process GitHub Archive data. GitHub Archive provides hourly archives of GitHub events, which we'll download, process, and analyze.</p>"},{"location":"examples/github-archive/#overview","title":"Overview","text":"<p>The workflow will:</p> <ol> <li>Download hourly GitHub Archive files for a specific date</li> <li>Process each file to extract repository statistics</li> <li>Generate a daily report summarizing the activity</li> </ol>"},{"location":"examples/github-archive/#full-code-example","title":"Full Code Example","text":"<p>Below is the complete example, which is also available in <code>examples/github_archive.py</code> in the GigQ repository:</p> <pre><code>\"\"\"\nExample using GigQ to process GitHub Archive data\n\"\"\"\nimport gzip\nimport json\nimport logging\nimport os\nimport sqlite3\nimport tempfile\nfrom datetime import datetime, timedelta\nfrom urllib.request import urlretrieve\nfrom gigq import Job, JobQueue, Worker, Workflow\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger('github_archive_processor')\n# SQLite database to store processed data\nRESULTS_DB = \"github_results.db\"\ndef initialize_results_db():\n\"\"\"Initialize the results database schema.\"\"\"\nconn = sqlite3.connect(RESULTS_DB)\ncursor = conn.cursor()\n# Create tables for GitHub events analysis\ncursor.execute('''\n    CREATE TABLE IF NOT EXISTS repository_stats (\n        date TEXT,\n        hour INTEGER,\n        repo_name TEXT,\n        event_count INTEGER,\n        star_count INTEGER,\n        fork_count INTEGER,\n        watch_count INTEGER,\n        PRIMARY KEY (date, hour, repo_name)\n    )\n    ''')\ncursor.execute('''\n    CREATE TABLE IF NOT EXISTS language_stats (\n        date TEXT,\n        hour INTEGER,\n        language TEXT,\n        event_count INTEGER,\n        repository_count INTEGER,\n        PRIMARY KEY (date, hour, language)\n    )\n    ''')\nconn.commit()\nconn.close()\ndef download_archive(date, hour):\n\"\"\"\n    Download a GitHub Archive file for the specified date and hour.\n    Args:\n        date: Date in YYYY-MM-DD format\n        hour: Hour (0-23)\n    Returns:\n        Path to the downloaded file\n    \"\"\"\n# Format the URL\ndate_str = date.replace(\"-\", \"\")\nurl = f\"https://data.gharchive.org/{date_str}-{hour}.json.gz\"\n# Create a temporary file\ntemp_file = tempfile.mktemp(suffix=\".json.gz\")\nlogger.info(f\"Downloading archive: {url}\")\nurlretrieve(url, temp_file)\nlogger.info(f\"Downloaded to {temp_file}\")\nreturn temp_file\ndef process_archive(date, hour):\n\"\"\"\n    Process a GitHub Archive file for the specified date and hour.\n    Args:\n        date: Date in YYYY-MM-DD format\n        hour: Hour (0-23)\n    Returns:\n        Statistics about the processed file\n    \"\"\"\ntry:\n# Download the archive\narchive_path = download_archive(date, hour)\n# Initialize counters\nrepo_stats = {}  # repo_name -&gt; {events, stars, forks, watches}\nlanguage_stats = {}  # language -&gt; {events, repos}\ntotal_events = 0\nevent_types = {}\n# Process the archive\nlogger.info(f\"Processing archive for {date} hour {hour}\")\nwith gzip.open(archive_path, 'rt', encoding='utf-8') as f:\nfor line in f:\n# Parse the event\nevent = json.loads(line)\ntotal_events += 1\n# Count event types\nevent_type = event.get('type', 'Unknown')\nevent_types[event_type] = event_types.get(event_type, 0) + 1\n# Process repository info\nrepo = event.get('repo', {})\nrepo_name = repo.get('name')\nif repo_name:\nif repo_name not in repo_stats:\nrepo_stats[repo_name] = {'events': 0, 'stars': 0, 'forks': 0, 'watches': 0}\nrepo_stats[repo_name]['events'] += 1\n# Process specific event types\nif event_type == 'WatchEvent':\nrepo_stats[repo_name]['watches'] += 1\nelif event_type == 'ForkEvent':\nrepo_stats[repo_name]['forks'] += 1\nelif event_type == 'StarEvent':\nrepo_stats[repo_name]['stars'] += 1\n# Process language info (from payload for PushEvents)\nif event_type == 'PushEvent':\npayload = event.get('payload', {})\ncommits = payload.get('commits', [])\nfor commit in commits:\n# In a real implementation, you might need to use GitHub API\n# to get language info for each repo\npass\n# Store results in database\nconn = sqlite3.connect(RESULTS_DB)\ncursor = conn.cursor()\n# Store repository stats\nfor repo_name, stats in repo_stats.items():\ncursor.execute(\n'''\n                INSERT OR REPLACE INTO repository_stats\n                (date, hour, repo_name, event_count, star_count, fork_count, watch_count)\n                VALUES (?, ?, ?, ?, ?, ?, ?)\n                ''',\n(\ndate, hour, repo_name, stats['events'],\nstats['stars'], stats['forks'], stats['watches']\n)\n)\n# Store language stats (in a real implementation this would be populated)\nfor language, stats in language_stats.items():\ncursor.execute(\n'''\n                INSERT OR REPLACE INTO language_stats\n                (date, hour, language, event_count, repository_count)\n                VALUES (?, ?, ?, ?, ?)\n                ''',\n(date, hour, language, stats.get('events', 0), stats.get('repos', 0))\n)\nconn.commit()\nconn.close()\n# Clean up\nos.unlink(archive_path)\n# Return summary\nreturn {\n'date': date,\n'hour': hour,\n'total_events': total_events,\n'unique_repositories': len(repo_stats),\n'event_types': event_types\n}\nexcept Exception as e:\nlogger.error(f\"Error processing archive for {date} hour {hour}: {e}\")\nraise\ndef generate_daily_report(date):\n\"\"\"\n    Generate a summary report for a full day of GitHub activity.\n    Args:\n        date: Date in YYYY-MM-DD format\n    Returns:\n        Report statistics\n    \"\"\"\nconn = sqlite3.connect(RESULTS_DB)\ncursor = conn.cursor()\n# Get total events by hour\ncursor.execute(\n'''\n        SELECT hour, SUM(event_count) as total\n        FROM repository_stats\n        WHERE date = ?\n        GROUP BY hour\n        ORDER BY hour\n        ''',\n(date,)\n)\nhourly_events = {row[0]: row[1] for row in cursor.fetchall()}\n# Get top repositories by events\ncursor.execute(\n'''\n        SELECT repo_name, SUM(event_count) as total\n        FROM repository_stats\n        WHERE date = ?\n        GROUP BY repo_name\n        ORDER BY total DESC\n        LIMIT 10\n        ''',\n(date,)\n)\ntop_repos = [(row[0], row[1]) for row in cursor.fetchall()]\n# Get top repositories by stars\ncursor.execute(\n'''\n        SELECT repo_name, SUM(star_count) as total\n        FROM repository_stats\n        WHERE date = ?\n        GROUP BY repo_name\n        ORDER BY total DESC\n        LIMIT 10\n        ''',\n(date,)\n)\ntop_starred = [(row[0], row[1]) for row in cursor.fetchall()]\nconn.close()\n# Generate report\nreport = {\n'date': date,\n'total_events': sum(hourly_events.values()),\n'hourly_events': hourly_events,\n'top_repositories': top_repos,\n'top_starred': top_starred\n}\nlogger.info(f\"Generated report for {date}\")\nlogger.info(f\"Total events: {report['total_events']}\")\nreturn report\ndef build_github_archive_workflow(date):\n\"\"\"\n    Build a workflow for processing GitHub Archive data for a specific date.\n    Args:\n        date: Date in YYYY-MM-DD format\n    Returns:\n        A Workflow object\n    \"\"\"\n# Ensure the results database is initialized\ninitialize_results_db()\n# Create a workflow\nworkflow = Workflow(f\"github_archive_{date}\")\n# Add jobs for each hour of the day\nhour_jobs = []\nfor hour in range(24):\njob = Job(\nname=f\"process_{date}_{hour}\",\nfunction=process_archive,\nparams={'date': date, 'hour': hour},\nmax_attempts=3,\ntimeout=600,  # 10 minutes\ndescription=f\"Process GitHub Archive for {date} hour {hour}\"\n)\nworkflow.add_job(job)\nhour_jobs.append(job)\n# Add a final job to generate a daily report, dependent on all hourly jobs\nreport_job = Job(\nname=f\"report_{date}\",\nfunction=generate_daily_report,\nparams={'date': date},\nmax_attempts=2,\ntimeout=300,  # 5 minutes\ndescription=f\"Generate daily report for {date}\"\n)\nworkflow.add_job(report_job, depends_on=hour_jobs)\nreturn workflow\nif __name__ == \"__main__\":\n# Example usage\nimport sys\nif len(sys.argv) &lt; 2:\nprint(\"Usage: python github_archive_example.py YYYY-MM-DD\")\nsys.exit(1)\ndate = sys.argv[1]\n# Create the job queue\nqueue = JobQueue(\"github_jobs.db\")\n# Build and submit the workflow\nworkflow = build_github_archive_workflow(date)\njob_ids = workflow.submit_all(queue)\nprint(f\"Submitted workflow with {len(job_ids)} jobs\")\n# Start a worker if requested\nif \"--worker\" in sys.argv:\nworker = Worker(\"github_jobs.db\")\nprint(\"Starting worker...\")\nworker.start()\n</code></pre>"},{"location":"examples/github-archive/#step-by-step-explanation","title":"Step-by-Step Explanation","text":"<p>Let's break down how this example uses GigQ to process GitHub Archive data.</p>"},{"location":"examples/github-archive/#1-database-setup","title":"1. Database Setup","text":"<p>First, we initialize a SQLite database to store the processed results:</p> <pre><code>def initialize_results_db():\nconn = sqlite3.connect(RESULTS_DB)\ncursor = conn.cursor()\n# Create tables for GitHub events analysis\ncursor.execute('''\n    CREATE TABLE IF NOT EXISTS repository_stats (\n        date TEXT,\n        hour INTEGER,\n        repo_name TEXT,\n        event_count INTEGER,\n        star_count INTEGER,\n        fork_count INTEGER,\n        watch_count INTEGER,\n        PRIMARY KEY (date, hour, repo_name)\n    )\n    ''')\n# ... (another table for language stats)\nconn.commit()\nconn.close()\n</code></pre>"},{"location":"examples/github-archive/#2-job-functions","title":"2. Job Functions","text":"<p>We define three main functions that will be executed as jobs:</p> <ol> <li><code>download_archive(date, hour)</code> - Downloads a GitHub Archive file for a specific date and hour</li> <li><code>process_archive(date, hour)</code> - Processes the archive and stores results in the database</li> <li><code>generate_daily_report(date)</code> - Generates a summary report from the processed data</li> </ol>"},{"location":"examples/github-archive/#3-building-the-workflow","title":"3. Building the Workflow","text":"<p>The <code>build_github_archive_workflow</code> function creates a workflow with 25 jobs:</p> <pre><code>def build_github_archive_workflow(date):\n# Ensure the results database is initialized\ninitialize_results_db()\n# Create a workflow\nworkflow = Workflow(f\"github_archive_{date}\")\n# Add jobs for each hour of the day (24 jobs)\nhour_jobs = []\nfor hour in range(24):\njob = Job(\nname=f\"process_{date}_{hour}\",\nfunction=process_archive,\nparams={'date': date, 'hour': hour},\nmax_attempts=3,\ntimeout=600,  # 10 minutes\ndescription=f\"Process GitHub Archive for {date} hour {hour}\"\n)\nworkflow.add_job(job)\nhour_jobs.append(job)\n# Add a final job to generate a daily report (1 job)\nreport_job = Job(\nname=f\"report_{date}\",\nfunction=generate_daily_report,\nparams={'date': date},\nmax_attempts=2,\ntimeout=300,  # 5 minutes\ndescription=f\"Generate daily report for {date}\"\n)\nworkflow.add_job(report_job, depends_on=hour_jobs)\nreturn workflow\n</code></pre> <p>This creates a workflow with:</p> <ul> <li>24 independent jobs (one for each hour) that can run in parallel</li> <li>1 final job that depends on all 24 hourly jobs to complete</li> </ul>"},{"location":"examples/github-archive/#4-submitting-and-running-the-workflow","title":"4. Submitting and Running the Workflow","text":"<p>Finally, we create a job queue, submit the workflow, and optionally start a worker:</p> <pre><code>if __name__ == \"__main__\":\n# ... (parse command line arguments)\n# Create the job queue\nqueue = JobQueue(\"github_jobs.db\")\n# Build and submit the workflow\nworkflow = build_github_archive_workflow(date)\njob_ids = workflow.submit_all(queue)\nprint(f\"Submitted workflow with {len(job_ids)} jobs\")\n# Start a worker if requested\nif \"--worker\" in sys.argv:\nworker = Worker(\"github_jobs.db\")\nprint(\"Starting worker...\")\nworker.start()\n</code></pre>"},{"location":"examples/github-archive/#workflow-visualization","title":"Workflow Visualization","text":"<p>Here's a visual representation of the workflow:</p> <pre><code>graph TD\n    subgraph \"Hourly Processing Jobs (run in parallel)\"\n    A0[Process Hour 0] --&gt; R\n    A1[Process Hour 1] --&gt; R\n    A2[Process Hour 2] --&gt; R\n    A3[Process Hour 3] --&gt; R\n    A4[Process Hour 4] --&gt; R\n    A5[Process Hour 5] --&gt; R\n    A6[Process Hour 6] --&gt; R\n    A7[Process Hour 7] --&gt; R\n    A8[Process Hour 8] --&gt; R\n    A9[Process Hour 9] --&gt; R\n    A10[Process Hour 10] --&gt; R\n    A11[Process Hour 11] --&gt; R\n    A12[Process Hour 12] --&gt; R\n    A13[Process Hour 13] --&gt; R\n    A14[Process Hour 14] --&gt; R\n    A15[Process Hour 15] --&gt; R\n    A16[Process Hour 16] --&gt; R\n    A17[Process Hour 17] --&gt; R\n    A18[Process Hour 18] --&gt; R\n    A19[Process Hour 19] --&gt; R\n    A20[Process Hour 20] --&gt; R\n    A21[Process Hour 21] --&gt; R\n    A22[Process Hour 22] --&gt; R\n    A23[Process Hour 23] --&gt; R\n    end\n    R[Generate Daily Report]</code></pre>"},{"location":"examples/github-archive/#running-the-example","title":"Running the Example","text":"<p>To run this example:</p> <pre><code># Install GigQ if you haven't already\npip install gigq\n\n# Run the example for a specific date\npython github_archive.py 2023-01-01\n\n# Run the example and also start a worker\npython github_archive.py 2023-01-01 --worker\n\n# Or start workers separately for parallel processing\ngigq --db github_jobs.db worker\n</code></pre>"},{"location":"examples/github-archive/#monitoring-progress","title":"Monitoring Progress","text":"<p>You can monitor the progress of the workflow using the GigQ CLI:</p> <pre><code># List all jobs\ngigq --db github_jobs.db list\n\n# List pending jobs\ngigq --db github_jobs.db list --status pending\n\n# List completed jobs\ngigq --db github_jobs.db list --status completed\n\n# Check the status of the report job\ngigq --db github_jobs.db status JOB_ID --show-result\n</code></pre>"},{"location":"examples/github-archive/#extensions-and-improvements","title":"Extensions and Improvements","text":"<p>This example demonstrates the basic functionality of GigQ for processing GitHub Archive data. Some possible extensions include:</p> <ol> <li>Multiple Workers: Run multiple workers in parallel to process the hourly data faster:</li> </ol> <pre><code># Run 4 workers in separate terminals or processes\ngigq --db github_jobs.db worker\n</code></pre> <ol> <li>Error Handling: Improve error handling with custom retry logic:</li> </ol> <pre><code>def process_archive_with_retry(date, hour, retry_delay=60):\ntry:\nreturn process_archive(date, hour)\nexcept Exception as e:\nif \"Rate limit exceeded\" in str(e):\nlogger.warning(f\"Rate limit exceeded, sleeping for {retry_delay} seconds\")\ntime.sleep(retry_delay)\nreturn process_archive(date, hour)\nraise\n</code></pre> <ol> <li>Date Range Processing: Extend the example to process a range of dates:    <pre><code>def process_date_range(start_date, end_date):\ncurrent_date = datetime.strptime(start_date, \"%Y-%m-%d\")\nend = datetime.strptime(end_date, \"%Y-%m-%d\")\nwhile current_date &lt;= end:\ndate_str = current_date.strftime(\"%Y-%m-%d\")\nworkflow = build_github_archive_workflow(date_str)\nworkflow.submit_all(queue)\ncurrent_date += timedelta(days=1)\n</code></pre></li> </ol>"},{"location":"examples/github-archive/#next-steps","title":"Next Steps","text":"<p>Now that you've seen a practical example of using GigQ, you might want to explore:</p> <ul> <li>Workflows - Learn more about creating complex workflows</li> <li>Workers - Understand how workers process jobs</li> <li>Error Handling - Learn about GigQ's error handling capabilities</li> </ul>"},{"location":"examples/scheduled-tasks/","title":"Scheduled Background Tasks Example","text":"<p>This example demonstrates how to use GigQ with a scheduler to run recurring background tasks. While GigQ doesn't have built-in scheduling, it works well with external schedulers.</p>"},{"location":"examples/scheduled-tasks/#overview","title":"Overview","text":"<p>This example shows how to:</p> <ol> <li>Create a scheduler daemon that submits jobs to GigQ at specific times</li> <li>Run different types of scheduled tasks (hourly, daily, weekly)</li> <li>Handle dependencies between scheduled tasks</li> <li>Manage the lifecycle of recurring jobs</li> </ol>"},{"location":"examples/scheduled-tasks/#complete-example","title":"Complete Example","text":"<p>Here's the full code example of a scheduler for GigQ:</p> <pre><code>\"\"\"\nExample of using GigQ with a scheduler for recurring tasks.\n\"\"\"\nimport os\nimport time\nimport signal\nimport logging\nimport schedule\nimport datetime\nimport sqlite3\nfrom gigq import Job, JobQueue, Worker, Workflow\n# Configure logging\nlogging.basicConfig(\nlevel=logging.INFO,\nformat='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n)\nlogger = logging.getLogger('gigq_scheduler')\n# Global variables\nrunning = True\nqueue = JobQueue(\"scheduled_jobs.db\")\n# Example job functions\ndef hourly_data_sync(source, destination):\n\"\"\"Sync data from source to destination.\"\"\"\nlogger.info(f\"Syncing data from {source} to {destination}\")\n# Simulate work\ntime.sleep(5)\nreturn {\n\"source\": source,\n\"destination\": destination,\n\"records_synced\": 100,\n\"timestamp\": datetime.datetime.now().isoformat()\n}\ndef daily_report_generation(data_dir, output_file):\n\"\"\"Generate a daily report.\"\"\"\nlogger.info(f\"Generating daily report from {data_dir} to {output_file}\")\n# Simulate work\ntime.sleep(10)\n# Create output directory if needed\nos.makedirs(os.path.dirname(output_file), exist_ok=True)\n# Write a simple report\nwith open(output_file, 'w') as f:\nf.write(f\"Daily Report - Generated at {datetime.datetime.now().isoformat()}\\n\")\nf.write(f\"Data source: {data_dir}\\n\")\nf.write(\"Summary:\\n\")\nf.write(\"- 100 records processed\\n\")\nf.write(\"- 5 errors detected\\n\")\nf.write(\"- 95% success rate\\n\")\nreturn {\n\"data_dir\": data_dir,\n\"output_file\": output_file,\n\"records_processed\": 100,\n\"timestamp\": datetime.datetime.now().isoformat()\n}\ndef weekly_cleanup(data_dir, days_to_keep=7):\n\"\"\"Clean up old files and data.\"\"\"\nlogger.info(f\"Cleaning up files older than {days_to_keep} days in {data_dir}\")\n# Simulate work\ntime.sleep(8)\n# Get current time for comparison\nnow = datetime.datetime.now()\ncutoff = now - datetime.timedelta(days=days_to_keep)\ncutoff_str = cutoff.isoformat()\nreturn {\n\"data_dir\": data_dir,\n\"days_kept\": days_to_keep,\n\"files_removed\": 25,\n\"space_freed_mb\": 150,\n\"cutoff_date\": cutoff_str,\n\"timestamp\": now.isoformat()\n}\ndef error_notification(error_data, recipients):\n\"\"\"Send error notifications.\"\"\"\nlogger.info(f\"Sending error notification to {recipients}\")\n# Simulate sending an email\ntime.sleep(2)\nreturn {\n\"recipients\": recipients,\n\"errors_reported\": len(error_data),\n\"timestamp\": datetime.datetime.now().isoformat()\n}\n# Scheduler functions\ndef schedule_hourly_sync():\n\"\"\"Schedule the hourly data sync job.\"\"\"\nlogger.info(\"Scheduling hourly data sync\")\n# Create the job\njob = Job(\nname=f\"hourly_sync_{datetime.datetime.now().strftime('%Y%m%d_%H%M')}\",\nfunction=hourly_data_sync,\nparams={\n\"source\": \"database_a\",\n\"destination\": \"database_b\"\n},\nmax_attempts=3,\ntimeout=300,  # 5 minutes\ndescription=\"Hourly data synchronization\"\n)\n# Submit the job\njob_id = queue.submit(job)\nlogger.info(f\"Submitted hourly sync job: {job_id}\")\nreturn job_id\ndef schedule_daily_report():\n\"\"\"Schedule the daily report generation job.\"\"\"\nlogger.info(\"Scheduling daily report generation\")\ntoday = datetime.datetime.now().strftime(\"%Y-%m-%d\")\noutput_file = f\"reports/daily/report_{today}.txt\"\n# Create the job\njob = Job(\nname=f\"daily_report_{today}\",\nfunction=daily_report_generation,\nparams={\n\"data_dir\": \"data/daily\",\n\"output_file\": output_file\n},\nmax_attempts=2,\ntimeout=600,  # 10 minutes\ndescription=f\"Daily report generation for {today}\"\n)\n# Submit the job\njob_id = queue.submit(job)\nlogger.info(f\"Submitted daily report job: {job_id}\")\nreturn job_id\ndef schedule_weekly_cleanup():\n\"\"\"Schedule the weekly cleanup job.\"\"\"\nlogger.info(\"Scheduling weekly cleanup\")\n# Create the job\njob = Job(\nname=f\"weekly_cleanup_{datetime.datetime.now().strftime('%Y%m%d')}\",\nfunction=weekly_cleanup,\nparams={\n\"data_dir\": \"data/\",\n\"days_to_keep\": 30\n},\nmax_attempts=2,\ntimeout=1800,  # 30 minutes\ndescription=\"Weekly data cleanup\"\n)\n# Submit the job\njob_id = queue.submit(job)\nlogger.info(f\"Submitted weekly cleanup job: {job_id}\")\nreturn job_id\ndef schedule_dependent_tasks():\n\"\"\"Schedule a series of dependent tasks.\"\"\"\nlogger.info(\"Scheduling dependent tasks\")\n# Create a workflow\nworkflow = Workflow(\"daily_processing\")\n# Define the jobs\nsync_job = Job(\nname=f\"data_sync_{datetime.datetime.now().strftime('%Y%m%d')}\",\nfunction=hourly_data_sync,\nparams={\n\"source\": \"main_database\",\n\"destination\": \"reporting_database\"\n},\nmax_attempts=3,\ntimeout=300\n)\nreport_job = Job(\nname=f\"main_report_{datetime.datetime.now().strftime('%Y%m%d')}\",\nfunction=daily_report_generation,\nparams={\n\"data_dir\": \"data/main\",\n\"output_file\": f\"reports/main/report_{datetime.datetime.now().strftime('%Y-%m-%d')}.txt\"\n},\nmax_attempts=2,\ntimeout=600\n)\n# Define error handling job - this is a contingency job that only runs if needed\nerror_job = Job(\nname=f\"error_notification_{datetime.datetime.now().strftime('%Y%m%d')}\",\nfunction=error_notification,\nparams={\n\"error_data\": [{\"severity\": \"high\", \"message\": \"Report generation failed\"}],\n\"recipients\": [\"admin@example.com\"]\n},\nmax_attempts=5,\ntimeout=180\n)\n# Add jobs to the workflow with dependencies\nworkflow.add_job(sync_job)\nworkflow.add_job(report_job, depends_on=[sync_job])\n# Submit the workflow\njob_ids = workflow.submit_all(queue)\nlogger.info(f\"Submitted workflow with {len(job_ids)} jobs\")\nreturn job_ids\ndef monitor_job_status():\n\"\"\"Monitor the status of scheduled jobs.\"\"\"\n# Get all jobs from the last 24 hours\nyesterday = (datetime.datetime.now() - datetime.timedelta(days=1)).isoformat()\nconn = sqlite3.connect(queue.db_path)\ncursor = conn.execute(\n\"SELECT id, name, status, created_at FROM jobs WHERE created_at &gt; ?\",\n(yesterday,)\n)\njobs = cursor.fetchall()\nconn.close()\n# Count by status\nstatus_counts = {}\nfor job in jobs:\nstatus = job[2]\nif status not in status_counts:\nstatus_counts[status] = 0\nstatus_counts[status] += 1\nlogger.info(f\"Job status in the last 24 hours: {status_counts}\")\n# Check for failed jobs\nfailed_jobs = [job for job in jobs if job[2] == 'failed']\nif failed_jobs:\nlogger.warning(f\"Found {len(failed_jobs)} failed jobs\")\nfor job in failed_jobs:\nlogger.warning(f\"Failed job: {job[1]} (ID: {job[0]}) created at {job[3]}\")\n# Get failure details\nstatus = queue.get_status(job[0])\nif status.get('error'):\nlogger.warning(f\"Error: {status['error']}\")\n# Maybe requeue important jobs that have failed\nif \"critical\" in job[1].lower():\nif queue.requeue_job(job[0]):\nlogger.info(f\"Requeued critical job: {job[0]}\")\n# Main scheduler function\ndef run_scheduler():\n\"\"\"Run the scheduler.\"\"\"\nlogger.info(\"Starting GigQ scheduler\")\n# Schedule hourly tasks\nschedule.every().hour.at(\":00\").do(schedule_hourly_sync)\n# Schedule daily tasks\nschedule.every().day.at(\"01:00\").do(schedule_daily_report)\n# Schedule weekly tasks\nschedule.every().monday.at(\"02:00\").do(schedule_weekly_cleanup)\n# Schedule dependent tasks\nschedule.every().day.at(\"04:00\").do(schedule_dependent_tasks)\n# Monitor job status periodically\nschedule.every(30).minutes.do(monitor_job_status)\n# Signal handler for graceful shutdown\ndef handle_signal(sig, frame):\nglobal running\nlogger.info(f\"Received signal {sig}, shutting down scheduler...\")\nrunning = False\nsignal.signal(signal.SIGINT, handle_signal)\nsignal.signal(signal.SIGTERM, handle_signal)\n# Run the scheduler loop\nwhile running:\nschedule.run_pending()\ntime.sleep(1)\nlogger.info(\"Scheduler stopped\")\nif __name__ == \"__main__\":\nimport argparse\nparser = argparse.ArgumentParser(description=\"GigQ Scheduler\")\nparser.add_argument(\"--run-now\", action=\"store_true\", help=\"Run scheduled jobs immediately\")\nparser.add_argument(\"--worker\", action=\"store_true\", help=\"Start a worker after scheduling jobs\")\nargs = parser.parse_args()\n# Run jobs immediately if requested\nif args.run_now:\nlogger.info(\"Running scheduled jobs immediately\")\nschedule_hourly_sync()\nschedule_daily_report()\nschedule_weekly_cleanup()\nschedule_dependent_tasks()\n# Start the scheduler\nif args.worker:\n# Start both a scheduler and a worker\nimport threading\n# Start the scheduler in a separate thread\nscheduler_thread = threading.Thread(target=run_scheduler)\nscheduler_thread.daemon = True\nscheduler_thread.start()\n# Start a worker in the main thread\nlogger.info(\"Starting worker\")\nworker = Worker(\"scheduled_jobs.db\")\nworker.start()\nelse:\n# Just run the scheduler\nrun_scheduler()\n</code></pre>"},{"location":"examples/scheduled-tasks/#running-the-example","title":"Running the Example","text":"<p>To run this example:</p> <pre><code># Install required dependencies\npip install gigq schedule\n\n# Run just the scheduler\npython scheduled_tasks.py\n\n# Run the scheduler and immediately schedule jobs\npython scheduled_tasks.py --run-now\n\n# Run both the scheduler and a worker\npython scheduled_tasks.py --worker\n\n# Run separate workers (in separate terminals)\ngigq --db scheduled_jobs.db worker\n</code></pre>"},{"location":"examples/scheduled-tasks/#scheduling-patterns-demonstrated","title":"Scheduling Patterns Demonstrated","text":"<p>This example demonstrates several common scheduling patterns:</p> <ol> <li>Simple Recurring Jobs: Basic tasks that run at regular intervals (hourly sync)</li> <li>Time-of-Day Jobs: Tasks that run at specific times (daily report at 1:00 AM)</li> <li>Day-of-Week Jobs: Tasks that run on specific days (weekly cleanup on Mondays)</li> <li>Dependent Job Chains: Series of tasks where each depends on the previous one</li> <li>Monitoring Jobs: Tasks that monitor and manage other jobs</li> </ol>"},{"location":"examples/scheduled-tasks/#scheduler-visualization","title":"Scheduler Visualization","text":"<p>Here's a visual representation of when tasks run:</p> <pre><code>gantt\n    title Scheduled Tasks\n    dateFormat  HH:mm\n    axisFormat %H:%M\n\n    section Hourly\n    Data Sync           :crit, sync, 00:00, 1h\n    Data Sync           :crit, sync, 01:00, 1h\n    Data Sync           :crit, sync, 02:00, 1h\n    Data Sync           :crit, sync, 03:00, 1h\n\n    section Daily\n    Daily Report        :report, 01:00, 30m\n    Dependent Tasks     :depends, 04:00, 1h\n\n    section Weekly\n    Weekly Cleanup      :cleanup, 02:00, 1h\n\n    section Monitoring\n    Monitor Jobs        :monitor, 00:00, 15m\n    Monitor Jobs        :monitor, 00:30, 15m\n    Monitor Jobs        :monitor, 01:00, 15m\n    Monitor Jobs        :monitor, 01:30, 15m</code></pre>"},{"location":"examples/scheduled-tasks/#key-features-demonstrated","title":"Key Features Demonstrated","text":"<p>This example demonstrates several important features:</p> <ol> <li>Task Scheduling: Using the <code>schedule</code> library to trigger jobs at specific times</li> <li>Job Independence: Each scheduled task runs independently</li> <li>Dependent Jobs: Tasks that depend on other tasks (using workflows)</li> <li>Job Monitoring: Checking job status and handling failures</li> <li>Parameterized Jobs: Passing different parameters to jobs based on runtime conditions</li> <li>Graceful Shutdown: Handling signals for clean application termination</li> </ol>"},{"location":"examples/scheduled-tasks/#configuration-options","title":"Configuration Options","text":"<p>The scheduler in this example can be configured in various ways:</p> <ol> <li>Schedule Times: Modify the schedule times in <code>run_scheduler</code> to suit your needs</li> <li>Job Parameters: Change job parameters in the scheduling functions</li> <li>Retry Logic: Adjust <code>max_attempts</code> and <code>timeout</code> values for each job</li> <li>Monitoring Frequency: Change how often job monitoring runs</li> </ol>"},{"location":"examples/scheduled-tasks/#deployment-considerations","title":"Deployment Considerations","text":"<p>When deploying this scheduler in production:</p> <ol> <li>Process Monitoring: Use a process manager like Supervisor, systemd, or PM2 to ensure the scheduler stays running</li> <li>Logging: Configure proper logging to files with rotation</li> <li>Separate Workers: Run workers as separate processes to handle the jobs</li> <li>Error Notifications: Implement proper error notifications via email, Slack, etc.</li> <li>Database Backups: Regularly backup the SQLite database</li> </ol> <p>Example systemd service file for the scheduler:</p> <pre><code>[Unit]\nDescription=GigQ Scheduler\nAfter=network.target\n[Service]\nUser=appuser\nWorkingDirectory=/path/to/app\nExecStart=/path/to/python /path/to/app/scheduled_tasks.py\nRestart=always\nRestartSec=10\n[Install]\nWantedBy=multi-user.target\n</code></pre>"},{"location":"examples/scheduled-tasks/#alternative-scheduling-approaches","title":"Alternative Scheduling Approaches","text":"<p>Besides using the <code>schedule</code> library, you can also schedule GigQ jobs with:</p> <ol> <li>Cron Jobs: Use system cron to run scripts that submit jobs to GigQ</li> </ol> <pre><code># /etc/crontab entry\n0 1 * * * appuser /path/to/python /path/to/submit_daily_report.py\n</code></pre> <ol> <li>APScheduler: Use the more powerful APScheduler library instead of <code>schedule</code></li> </ol> <pre><code>from apscheduler.schedulers.blocking import BlockingScheduler\nscheduler = BlockingScheduler()\nscheduler.add_job(schedule_daily_report, 'cron', hour=1)\nscheduler.start()\n</code></pre> <ol> <li>Celery Beat: If you're already using Celery, you can use Celery Beat to schedule GigQ job submissions</li> </ol>"},{"location":"examples/scheduled-tasks/#next-steps","title":"Next Steps","text":"<p>Now that you've seen how to schedule recurring tasks with GigQ, you might want to explore:</p> <ul> <li>Data Processing Example - Learn how to build data processing pipelines</li> <li>GitHub Archive Example - See another example application</li> <li>Workflows - Learn more about creating complex workflows</li> <li>Job Queue Management - Advanced job queue management techniques</li> </ul>"},{"location":"getting-started/concepts/","title":"Core Concepts","text":"<p>This page introduces the key concepts and components of GigQ.</p>"},{"location":"getting-started/concepts/#overview","title":"Overview","text":"<p>GigQ is centered around a few simple concepts:</p> <ol> <li>Jobs - Units of work to be executed</li> <li>Job Queue - Manages the storage and retrieval of jobs</li> <li>Workers - Execute jobs from the queue</li> <li>Workflows - Define dependencies between jobs</li> </ol> <p>Let's explore each of these components in detail.</p>"},{"location":"getting-started/concepts/#jobs","title":"Jobs","text":"<p>A Job represents a unit of work to be executed. It encapsulates:</p> <ul> <li>A function to execute</li> <li>Parameters to pass to the function</li> <li>Execution settings (e.g., timeout, retry policy)</li> <li>Metadata (e.g., name, description)</li> </ul>"},{"location":"getting-started/concepts/#job-states","title":"Job States","text":"<p>A job can be in one of several states:</p> <pre><code>stateDiagram-v2\n    [*] --&gt; PENDING: Job Created\n    PENDING --&gt; RUNNING: Worker Claims Job\n    RUNNING --&gt; COMPLETED: Successful Execution\n    RUNNING --&gt; FAILED: Error (max attempts exceeded)\n    RUNNING --&gt; PENDING: Error (retry)\n    RUNNING --&gt; TIMEOUT: Execution Time Exceeded\n    PENDING --&gt; CANCELLED: User Cancellation\n    COMPLETED --&gt; [*]\n    FAILED --&gt; [*]\n    CANCELLED --&gt; [*]\n    TIMEOUT --&gt; [*]</code></pre> <ul> <li>PENDING - The job is waiting to be executed</li> <li>RUNNING - The job is currently being executed by a worker</li> <li>COMPLETED - The job has successfully completed</li> <li>FAILED - The job has failed after exhausting all retry attempts</li> <li>CANCELLED - The job was cancelled by the user</li> <li>TIMEOUT - The job execution exceeded the timeout</li> </ul>"},{"location":"getting-started/concepts/#job-attributes","title":"Job Attributes","text":"<p>A job has the following attributes:</p> Attribute Description <code>id</code> Unique identifier (auto-generated UUID) <code>name</code> Human-readable name <code>function</code> The function to execute <code>params</code> Dictionary of parameters to pass to the function <code>priority</code> Execution priority (higher values execute first) <code>dependencies</code> List of job IDs that must complete before this job runs <code>max_attempts</code> Maximum number of retry attempts <code>timeout</code> Maximum execution time in seconds <code>description</code> Optional description"},{"location":"getting-started/concepts/#job-queue","title":"Job Queue","text":"<p>The JobQueue manages the storage and retrieval of jobs. It:</p> <ul> <li>Stores jobs in a SQLite database</li> <li>Ensures atomic operations for job state transitions</li> <li>Manages job prioritization and dependency resolution</li> <li>Provides interfaces for job submission, cancellation, and status queries</li> </ul> <p>The job queue is backed by a SQLite database, making it:</p> <ul> <li>Simple to set up (no external dependencies)</li> <li>Reliable (SQLite's ACID guarantees)</li> <li>Portable (single file database)</li> <li>Suitable for most local/small-scale job processing needs</li> </ul>"},{"location":"getting-started/concepts/#workers","title":"Workers","text":"<p>A Worker processes jobs from the queue. It:</p> <ul> <li>Claims jobs from the queue</li> <li>Executes the job functions</li> <li>Updates job states based on execution results</li> <li>Handles retries for failed jobs</li> <li>Detects and recovers from timeouts</li> </ul> <p>Workers can run:</p> <ul> <li>In the same process as the job submitter</li> <li>In separate processes</li> <li>On different machines (as long as they can access the same database file)</li> </ul> <p>Multiple workers can process jobs concurrently, with SQLite's locking mechanisms ensuring that each job is processed exactly once.</p>"},{"location":"getting-started/concepts/#workflows","title":"Workflows","text":"<p>A Workflow defines a series of related jobs with dependencies. It:</p> <ul> <li>Groups jobs together under a common name</li> <li>Defines execution order through job dependencies</li> <li>Simplifies the management of complex multi-step processes</li> </ul> <p>Workflows are particularly useful for:</p> <ul> <li>ETL pipelines</li> <li>Data processing tasks with multiple stages</li> <li>Any process that requires multiple dependent steps</li> </ul>"},{"location":"getting-started/concepts/#workflow-example","title":"Workflow Example","text":"<p>Here's how a simple workflow might look:</p> <pre><code>graph TD\n    A[Download Data] --&gt; B[Process Data]\n    B --&gt; C[Generate Report]\n    B --&gt; D[Send Notifications]\n    C --&gt; E[Archive Results]\n    D --&gt; E</code></pre> <p>In GigQ, this would be defined as:</p> <pre><code>workflow = Workflow(\"data_pipeline\")\ndownload_job = Job(name=\"download\", function=download_data)\nprocess_job = Job(name=\"process\", function=process_data)\nreport_job = Job(name=\"report\", function=generate_report)\nnotify_job = Job(name=\"notify\", function=send_notifications)\narchive_job = Job(name=\"archive\", function=archive_results)\nworkflow.add_job(download_job)\nworkflow.add_job(process_job, depends_on=[download_job])\nworkflow.add_job(report_job, depends_on=[process_job])\nworkflow.add_job(notify_job, depends_on=[process_job])\nworkflow.add_job(archive_job, depends_on=[report_job, notify_job])\nworkflow.submit_all(queue)\n</code></pre>"},{"location":"getting-started/concepts/#sqlite-storage","title":"SQLite Storage","text":"<p>GigQ uses SQLite as its storage backend, with two main tables:</p> <ol> <li>jobs - Stores job definitions and current state</li> <li>job_executions - Tracks individual execution attempts</li> </ol> <p>The SQLite backend provides:</p> <ul> <li>Simplicity - No need to set up external services</li> <li>Reliability - ACID transactions ensure consistency</li> <li>Portability - Single file database, easy to backup and manage</li> <li>Concurrency - Built-in locking mechanisms for safe multi-worker operation</li> </ul>"},{"location":"getting-started/concepts/#next-steps","title":"Next Steps","text":"<p>Now that you understand the core concepts, learn more about:</p> <ul> <li>Defining Jobs - How to create and configure jobs</li> <li>Job Queue - How to manage jobs in the queue</li> <li>Workers - How to process jobs</li> <li>Workflows - How to create and manage workflows</li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>This guide covers various ways to install GigQ and set up your environment.</p>"},{"location":"getting-started/installation/#prerequisites","title":"Prerequisites","text":"<p>GigQ requires:</p> <ul> <li>Python 3.7 or newer</li> <li>SQLite 3.8.3 or newer (usually included with Python)</li> </ul>"},{"location":"getting-started/installation/#standard-installation","title":"Standard Installation","text":"<p>The simplest way to install GigQ is via pip:</p> <pre><code>pip install gigq\n</code></pre> <p>This will install the latest stable version of GigQ from PyPI.</p>"},{"location":"getting-started/installation/#installation-from-source","title":"Installation from Source","text":"<p>To install the latest development version from GitHub:</p> <pre><code>git clone https://github.com/gigq/gigq.git\ncd gigq\npip install -e .\n</code></pre> <p>The <code>-e</code> flag installs the package in \"editable\" mode, which is useful for development.</p>"},{"location":"getting-started/installation/#installation-in-a-virtual-environment","title":"Installation in a Virtual Environment","text":"<p>It's a good practice to install GigQ in a virtual environment to avoid conflicts with other packages. Here's how to do it with <code>venv</code>:</p> <pre><code># Create a virtual environment\npython -m venv gigq-env\n\n# Activate the virtual environment\n# On Windows:\ngigq-env\\Scripts\\activate\n# On macOS/Linux:\nsource gigq-env/bin/activate\n\n# Install GigQ\npip install gigq\n</code></pre>"},{"location":"getting-started/installation/#docker-installation","title":"Docker Installation","text":"<p>You can also run GigQ in a Docker container. Create a <code>Dockerfile</code>:</p> <pre><code>FROM python:3.9-slim\nWORKDIR /app\n# Install GigQ\nRUN pip install gigq\n\n# Copy your application code\nCOPY . .\n\n# Command to run when the container starts\nCMD [\"python\", \"your_script.py\"]\n</code></pre> <p>Then build and run the container:</p> <pre><code>docker build -t gigq-app .\ndocker run gigq-app\n</code></pre>"},{"location":"getting-started/installation/#verifying-the-installation","title":"Verifying the Installation","text":"<p>After installation, you can verify that GigQ is installed correctly by running:</p> <pre><code>python -c \"import gigq; print(gigq.__version__)\"\n</code></pre> <p>You should see the version number of GigQ printed.</p>"},{"location":"getting-started/installation/#installing-optional-dependencies","title":"Installing Optional Dependencies","text":"<p>GigQ has minimal dependencies by design, but you might want to install additional packages for specific use cases:</p> <pre><code># For development and testing\npip install gigq[dev]\n# For visualization capabilities\npip install gigq[viz]\n</code></pre>"},{"location":"getting-started/installation/#system-specific-notes","title":"System-specific Notes","text":""},{"location":"getting-started/installation/#windows","title":"Windows","text":"<p>On Windows, ensure that the SQLite database file is accessible to all processes that need it, especially if you're running workers in separate processes or services.</p>"},{"location":"getting-started/installation/#macos","title":"macOS","text":"<p>No special configuration is needed for macOS.</p>"},{"location":"getting-started/installation/#linux","title":"Linux","text":"<p>On Linux, you might need to install the <code>sqlite3</code> package if it's not already installed:</p> <pre><code># Debian/Ubuntu\nsudo apt-get install sqlite3\n\n# RHEL/CentOS/Fedora\nsudo yum install sqlite\n</code></pre>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter issues during installation:</p> <ol> <li>SQLite version issues: Ensure your SQLite version is 3.8.3 or newer:</li> </ol> <pre><code>python -c \"import sqlite3; print(sqlite3.sqlite_version)\"\n</code></pre> <ol> <li>Permission errors: If you get permission errors, try installing with:</li> </ol> <pre><code>pip install --user gigq\n</code></pre> <ol> <li> <p>Dependency conflicts: Use a virtual environment as described above.</p> </li> <li> <p>Import errors after installation: Make sure your Python path is set correctly:    <pre><code>python -c \"import sys; print(sys.path)\"\n</code></pre></p> </li> </ol>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<p>Now that you have GigQ installed, check out the Quick Start Guide to begin using it.</p>"},{"location":"getting-started/quick-start/","title":"Quick Start","text":"<p>This guide will help you quickly get started with GigQ by walking through the core features.</p>"},{"location":"getting-started/quick-start/#basic-job-processing","title":"Basic Job Processing","text":"<p>Let's start with a simple example of defining, submitting, and processing a job.</p>"},{"location":"getting-started/quick-start/#1-define-a-job-function","title":"1. Define a Job Function","text":"<p>First, define a function that will be executed as a job:</p> <pre><code>def process_file(filename, threshold=0.5):\n\"\"\"Process a file with the given threshold.\"\"\"\nwith open(filename, 'r') as f:\ncontent = f.read()\n# Do some processing\nword_count = len(content.split())\n# Return some results\nreturn {\n\"filename\": filename,\n\"word_count\": word_count,\n\"threshold_applied\": threshold,\n\"processed\": True\n}\n</code></pre>"},{"location":"getting-started/quick-start/#2-create-and-submit-a-job","title":"2. Create and Submit a Job","text":"<p>Now, let's create a job and submit it to the queue:</p> <pre><code>from gigq import Job, JobQueue\n# Create a job queue (or connect to an existing one)\nqueue = JobQueue(\"my_jobs.db\")\n# Create a job\njob = Job(\nname=\"process_csv_file\",\nfunction=process_file,\nparams={\"filename\": \"data.csv\", \"threshold\": 0.7},\nmax_attempts=3,    # Retry up to 3 times on failure\ntimeout=300        # Timeout after 5 minutes\n)\n# Submit the job\njob_id = queue.submit(job)\nprint(f\"Submitted job with ID: {job_id}\")\n</code></pre>"},{"location":"getting-started/quick-start/#3-start-a-worker-to-process-jobs","title":"3. Start a Worker to Process Jobs","text":"<p>You can start a worker to process jobs in the queue:</p> <pre><code>from gigq import Worker\n# Create a worker\nworker = Worker(\"my_jobs.db\")\n# Start the worker (this will block until stopped)\nworker.start()\n</code></pre> <p>Alternatively, you can process a single job:</p> <pre><code># Process just one job\nworker.process_one()\n</code></pre>"},{"location":"getting-started/quick-start/#4-check-job-status","title":"4. Check Job Status","text":"<p>You can check the status of a job:</p> <pre><code># Get the status of a job\nstatus = queue.get_status(job_id)\nprint(f\"Job status: {status['status']}\")\n# If the job is completed, you can access the result\nif status[\"status\"] == \"completed\":\nprint(f\"Result: {status['result']}\")\n</code></pre>"},{"location":"getting-started/quick-start/#using-the-command-line-interface","title":"Using the Command Line Interface","text":"<p>GigQ provides a command-line interface for working with jobs.</p>"},{"location":"getting-started/quick-start/#submit-a-job","title":"Submit a Job","text":"<pre><code>gigq --db my_jobs.db submit my_module.process_file --name \"Process CSV\" --param \"filename=data.csv\" --param \"threshold=0.7\"\n</code></pre>"},{"location":"getting-started/quick-start/#start-a-worker","title":"Start a Worker","text":"<pre><code># Start a worker\ngigq --db my_jobs.db worker\n\n# Process just one job\ngigq --db my_jobs.db worker --once\n</code></pre>"},{"location":"getting-started/quick-start/#check-job-status","title":"Check Job Status","text":"<pre><code>gigq --db my_jobs.db status your-job-id --show-result\n</code></pre>"},{"location":"getting-started/quick-start/#list-jobs","title":"List Jobs","text":"<pre><code>gigq --db my_jobs.db list\ngigq --db my_jobs.db list --status pending\n</code></pre>"},{"location":"getting-started/quick-start/#cancel-a-job","title":"Cancel a Job","text":"<pre><code>gigq --db my_jobs.db cancel your-job-id\n</code></pre>"},{"location":"getting-started/quick-start/#creating-workflows","title":"Creating Workflows","text":"<p>GigQ allows you to create workflows with dependent jobs:</p> <pre><code>from gigq import Workflow\n# Create a workflow\nworkflow = Workflow(\"data_processing\")\n# Define jobs\ndownload_job = Job(\nname=\"download\",\nfunction=download_data,\nparams={\"url\": \"https://example.com/data.csv\"}\n)\nprocess_job = Job(\nname=\"process\",\nfunction=process_file,\nparams={\"filename\": \"data.csv\"}\n)\nanalyze_job = Job(\nname=\"analyze\",\nfunction=analyze_data,\nparams={\"processed_file\": \"processed.csv\"}\n)\n# Add jobs to workflow with dependencies\nworkflow.add_job(download_job)\nworkflow.add_job(process_job, depends_on=[download_job])\nworkflow.add_job(analyze_job, depends_on=[process_job])\n# Submit all jobs in the workflow\njob_ids = workflow.submit_all(queue)\n</code></pre>"},{"location":"getting-started/quick-start/#next-steps","title":"Next Steps","text":"<p>Now that you understand the basics of GigQ, you can:</p> <ul> <li>Learn more about job definition and parameters</li> <li>Explore advanced workflow capabilities</li> <li>See a complete example application</li> <li>Understand how GigQ handles errors</li> </ul>"},{"location":"user-guide/cli/","title":"Command Line Interface","text":"<p>GigQ provides a powerful command-line interface (CLI) that allows you to manage jobs and workers directly from your terminal. This guide covers all available commands and their options.</p>"},{"location":"user-guide/cli/#overview","title":"Overview","text":"<p>The CLI is automatically installed when you install GigQ. You can access it using the <code>gigq</code> command:</p> <pre><code>gigq --help\n</code></pre> <p>This will display a list of available commands and general options.</p>"},{"location":"user-guide/cli/#global-options","title":"Global Options","text":"<p>These options can be used with any command:</p> Option Description <code>--db PATH</code> Path to the SQLite database file (default: <code>gigq.db</code>) <code>--help</code> Show help message and exit"},{"location":"user-guide/cli/#commands","title":"Commands","text":""},{"location":"user-guide/cli/#submit-a-job","title":"Submit a Job","text":"<p>Submit a new job to the queue:</p> <pre><code>gigq submit MODULE.FUNCTION [OPTIONS]\n</code></pre> <p>Where <code>MODULE.FUNCTION</code> is the fully qualified name of the function to execute (e.g., <code>my_package.my_module.my_function</code>).</p>"},{"location":"user-guide/cli/#options","title":"Options","text":"Option Description <code>--name NAME</code> Name for the job (required) <code>--param KEY=VALUE</code>, <code>-p KEY=VALUE</code> Parameters to pass to the function (can be specified multiple times) <code>--priority INT</code> Job priority (higher runs first, default: 0) <code>--max-attempts INT</code> Maximum execution attempts (default: 3) <code>--timeout INT</code> Timeout in seconds (default: 300) <code>--description TEXT</code> Job description"},{"location":"user-guide/cli/#example","title":"Example","text":"<pre><code>gigq submit my_module.process_data --name \"Process CSV\" \\\n--param \"filename=data.csv\" --param \"threshold=0.7\" \\\n--priority 10 --max-attempts 5 --timeout 600 \\\n--description \"Process the daily data CSV file\"\n</code></pre>"},{"location":"user-guide/cli/#check-job-status","title":"Check Job Status","text":"<p>Check the status of a specific job:</p> <pre><code>gigq status JOB_ID [OPTIONS]\n</code></pre>"},{"location":"user-guide/cli/#options_1","title":"Options","text":"Option Description <code>--show-params</code> Show job parameters <code>--show-result</code> Show job result <code>--show-executions</code> Show execution history"},{"location":"user-guide/cli/#example_1","title":"Example","text":"<pre><code>gigq status 1a2b3c4d-5e6f-7g8h-9i0j-1k2l3m4n5o6p --show-result\n</code></pre> <p>Example output:</p> <pre><code>Job: Process CSV (1a2b3c4d-5e6f-7g8h-9i0j-1k2l3m4n5o6p)\nStatus: completed\nCreated: 2023-03-01 12:34:56\nUpdated: 2023-03-01 12:35:12\nStarted: 2023-03-01 12:35:01\nCompleted: 2023-03-01 12:35:12\nAttempts: 1 / 3\nWorker: worker-1234\n\nResult:\n  processed: True\n  record_count: 1250\n  errors: 0\n</code></pre>"},{"location":"user-guide/cli/#list-jobs","title":"List Jobs","text":"<p>List jobs in the queue:</p> <pre><code>gigq list [OPTIONS]\n</code></pre>"},{"location":"user-guide/cli/#options_2","title":"Options","text":"Option Description <code>--status STATUS</code> Filter by status (pending, running, completed, failed, cancelled, timeout) <code>--limit INT</code> Maximum number of jobs to list (default: 100)"},{"location":"user-guide/cli/#example_2","title":"Example","text":"<pre><code>gigq list --status pending --limit 20\n</code></pre> <p>Example output:</p> <pre><code>ID                                   Name                 Status    Priority  Created              Updated\n------------------------------------ -------------------- --------- --------- -------------------- --------------------\n1a2b3c4d-5e6f-7g8h-9i0j-1k2l3m4n5o6p Process CSV          pending   10        2023-03-01 12:34:56  2023-03-01 12:34:56\nabcd1234-5e6f-7g8h-9i0j-1k2l3m4n5o6p Import Users         pending   5         2023-03-01 12:30:12  2023-03-01 12:30:12\n</code></pre>"},{"location":"user-guide/cli/#cancel-a-job","title":"Cancel a Job","text":"<p>Cancel a pending job:</p> <pre><code>gigq cancel JOB_ID\n</code></pre>"},{"location":"user-guide/cli/#example_3","title":"Example","text":"<pre><code>gigq cancel 1a2b3c4d-5e6f-7g8h-9i0j-1k2l3m4n5o6p\n</code></pre>"},{"location":"user-guide/cli/#requeue-a-failed-job","title":"Requeue a Failed Job","text":"<p>Requeue a failed job:</p> <pre><code>gigq requeue JOB_ID\n</code></pre>"},{"location":"user-guide/cli/#example_4","title":"Example","text":"<pre><code>gigq requeue 1a2b3c4d-5e6f-7g8h-9i0j-1k2l3m4n5o6p\n</code></pre>"},{"location":"user-guide/cli/#clear-completed-jobs","title":"Clear Completed Jobs","text":"<p>Clear completed jobs from the queue:</p> <pre><code>gigq clear [OPTIONS]\n</code></pre>"},{"location":"user-guide/cli/#options_3","title":"Options","text":"Option Description <code>--before DAYS</code> Clear jobs completed more than N days ago"},{"location":"user-guide/cli/#example_5","title":"Example","text":"<pre><code>gigq clear --before 7  # Clear jobs completed more than 7 days ago\n</code></pre>"},{"location":"user-guide/cli/#start-a-worker","title":"Start a Worker","text":"<p>Start a worker process:</p> <pre><code>gigq worker [OPTIONS]\n</code></pre>"},{"location":"user-guide/cli/#options_4","title":"Options","text":"Option Description <code>--worker-id ID</code> Worker ID (generated if not provided) <code>--polling-interval INT</code> Polling interval in seconds (default: 5) <code>--once</code> Process one job and exit"},{"location":"user-guide/cli/#example_6","title":"Example","text":"<pre><code>gigq worker --polling-interval 2\n</code></pre> <p>To run a worker that processes just one job and exits:</p> <pre><code>gigq worker --once\n</code></pre>"},{"location":"user-guide/cli/#using-with-shell-scripts","title":"Using with Shell Scripts","text":"<p>You can easily integrate GigQ CLI commands into shell scripts:</p> <pre><code>#!/bin/bash\n# Submit a job\nJOB_ID=$(gigq submit my_module.process_data --name \"Process Data\" --param \"filename=data.csv\" | grep -oP 'Job submitted: \\K.*')\necho \"Submitted job: $JOB_ID\"\n# Wait for job to complete\nwhile true; do\nSTATUS=$(gigq status $JOB_ID | grep -oP 'Status: \\K.*')\necho \"Job status: $STATUS\"\nif [[ \"$STATUS\" == \"completed\" ]]; then\necho \"Job completed successfully!\"\nbreak\nelif [[ \"$STATUS\" == \"failed\" || \"$STATUS\" == \"timeout\" || \"$STATUS\" == \"cancelled\" ]]; then\necho \"Job failed!\"\nexit 1\nfi\nsleep 5\ndone\n# Get the result\ngigq status $JOB_ID --show-result\n</code></pre>"},{"location":"user-guide/cli/#environment-variables","title":"Environment Variables","text":"<p>GigQ CLI respects the following environment variables:</p> Variable Description <code>GIGQ_DB</code> Default database path (overrides the default <code>gigq.db</code> but is overridden by <code>--db</code>) <code>GIGQ_WORKER_ID</code> Default worker ID for the worker command <code>GIGQ_POLLING_INTERVAL</code> Default polling interval for workers <p>Example:</p> <pre><code>export GIGQ_DB=/path/to/my/jobs.db\nexport GIGQ_POLLING_INTERVAL=2\n# Now you don't need to specify --db in every command\ngigq list\n</code></pre>"},{"location":"user-guide/cli/#command-reference-summary","title":"Command Reference Summary","text":"Command Description <code>submit</code> Submit a job to the queue <code>status</code> Check job status <code>list</code> List jobs <code>cancel</code> Cancel a pending job <code>requeue</code> Requeue a failed job <code>clear</code> Clear completed jobs <code>worker</code> Start a worker process"},{"location":"user-guide/cli/#next-steps","title":"Next Steps","text":"<p>Now that you know how to use the CLI, you might want to explore:</p> <ul> <li>Job Queue Management - Learn more about managing the job queue programmatically</li> <li>Workers - Learn more about workers and how they process jobs</li> <li>Error Handling - Learn how GigQ handles errors and how to customize error handling</li> </ul>"},{"location":"user-guide/defining-jobs/","title":"Defining Jobs","text":"<p>Jobs are the fundamental unit of work in GigQ. This guide explains how to define jobs, configure their parameters, and understand their lifecycle.</p>"},{"location":"user-guide/defining-jobs/#basic-job-definition","title":"Basic Job Definition","text":"<p>To create a job, you'll use the <code>Job</code> class:</p> <pre><code>from gigq import Job\n# Create a simple job\njob = Job(\nname=\"process_data\",\nfunction=process_data_function,\nparams={\"filename\": \"data.csv\", \"threshold\": 0.7}\n)\n</code></pre> <p>This creates a job that will execute the <code>process_data_function</code> with the specified parameters.</p>"},{"location":"user-guide/defining-jobs/#job-parameters","title":"Job Parameters","text":"<p>The <code>Job</code> class accepts the following parameters:</p> Parameter Type Required Description <code>name</code> str Yes A human-readable name for the job <code>function</code> callable Yes The function to execute <code>params</code> dict No Parameters to pass to the function <code>priority</code> int No Execution priority (higher values run first, default: 0) <code>dependencies</code> list No List of job IDs that must complete before this job runs <code>max_attempts</code> int No Maximum number of execution attempts (default: 3) <code>timeout</code> int No Maximum runtime in seconds (default: 300) <code>description</code> str No Optional description of the job"},{"location":"user-guide/defining-jobs/#job-function-requirements","title":"Job Function Requirements","text":"<p>The function referenced by a job must:</p> <ol> <li>Accept the parameters defined in the <code>params</code> dictionary</li> <li>Be importable from its module (for serialization purposes)</li> <li>Return a JSON-serializable result (or None)</li> </ol> <p>Example job function:</p> <pre><code>def process_data(filename, threshold=0.5):\n\"\"\"\n    Process data from a file.\n    Args:\n        filename: Path to the file to process\n        threshold: Processing threshold (default: 0.5)\n    Returns:\n        dict: Processing results\n    \"\"\"\n# Process the data...\nreturn {\n\"processed\": True,\n\"records\": 100,\n\"errors\": 0,\n\"threshold_used\": threshold\n}\n</code></pre>"},{"location":"user-guide/defining-jobs/#job-identification","title":"Job Identification","text":"<p>Each job is assigned a unique ID (UUID) when created. This ID is used to:</p> <ul> <li>Track the job in the queue</li> <li>Reference the job as a dependency for other jobs</li> <li>Query the job's status</li> </ul> <pre><code># Get the job ID\njob_id = job.id\n# Use the ID when submitting the job\nqueue = JobQueue(\"jobs.db\")\nsubmitted_id = queue.submit(job)  # This will be the same as job.id\n# Reference the job as a dependency\ndependent_job = Job(\nname=\"dependent_job\",\nfunction=another_function,\ndependencies=[job.id]\n)\n</code></pre>"},{"location":"user-guide/defining-jobs/#job-priorities","title":"Job Priorities","text":"<p>Jobs with higher priority values are executed before jobs with lower priority values:</p> <pre><code># High priority job (will execute first)\nhigh_priority_job = Job(\nname=\"urgent_task\",\nfunction=urgent_function,\npriority=100\n)\n# Normal priority job\nnormal_job = Job(\nname=\"regular_task\",\nfunction=regular_function,\npriority=0  # Default\n)\n# Low priority job (will execute last)\nlow_priority_job = Job(\nname=\"background_task\",\nfunction=background_function,\npriority=-10\n)\n</code></pre> <p>When multiple workers are processing jobs, they will preferentially select higher-priority jobs first.</p>"},{"location":"user-guide/defining-jobs/#job-timeouts","title":"Job Timeouts","text":"<p>You can specify a timeout for jobs to prevent them from running indefinitely:</p> <pre><code># Job with a 10-minute timeout\njob_with_timeout = Job(\nname=\"long_running_task\",\nfunction=long_running_function,\ntimeout=600  # 10 minutes in seconds\n)\n</code></pre> <p>If a job exceeds its timeout:</p> <ul> <li>The job is marked as timed out</li> <li>The worker stops processing the job</li> <li>If <code>attempts &lt; max_attempts</code>, the job is requeued for another attempt</li> </ul>"},{"location":"user-guide/defining-jobs/#job-retries","title":"Job Retries","text":"<p>Jobs can be configured to retry automatically if they fail:</p> <pre><code># Job that will retry up to 5 times\njob_with_retries = Job(\nname=\"retry_task\",\nfunction=potentially_failing_function,\nmax_attempts=5\n)\n</code></pre> <p>When a job fails (raises an exception):</p> <ol> <li>If <code>attempts &lt; max_attempts</code>, the job is requeued with <code>status=\"pending\"</code></li> <li>If <code>attempts &gt;= max_attempts</code>, the job is marked as failed</li> </ol>"},{"location":"user-guide/defining-jobs/#job-dependencies","title":"Job Dependencies","text":"<p>Jobs can depend on other jobs, ensuring they only run after those dependencies have completed successfully:</p> <pre><code># Create a job\nfirst_job = Job(name=\"first_step\", function=step_one)\n# Submit to get an ID\nqueue = JobQueue(\"jobs.db\")\nfirst_job_id = queue.submit(first_job)\n# Create a dependent job\nsecond_job = Job(\nname=\"second_step\",\nfunction=step_two,\ndependencies=[first_job_id]\n)\n# Submit the dependent job\nqueue.submit(second_job)\n</code></pre> <p>For more complex dependency structures, see the Workflows documentation.</p>"},{"location":"user-guide/defining-jobs/#job-results-and-error-handling","title":"Job Results and Error Handling","text":"<p>When a job completes:</p> <ul> <li>If successful, its result is stored in the database</li> <li>If it fails, the error message is stored</li> </ul> <p>Job functions should handle their own exceptions when possible, but unhandled exceptions will be caught by the worker:</p> <pre><code>def robust_job_function(input_data):\ntry:\n# Process data...\nreturn {\"status\": \"success\", \"result\": processed_data}\nexcept ValueError as e:\n# Handle expected errors\nreturn {\"status\": \"error\", \"message\": str(e)}\n# Unhandled exceptions will be caught by GigQ\n</code></pre>"},{"location":"user-guide/defining-jobs/#best-practices-for-job-design","title":"Best Practices for Job Design","text":"<ol> <li> <p>Keep jobs atomic: Each job should do one specific thing.</p> </li> <li> <p>Make jobs idempotent: Jobs should be safe to run multiple times with the same input.</p> </li> <li> <p>Limit job interdependencies: Minimize dependencies between jobs to improve parallelism.</p> </li> <li> <p>Use meaningful names: Give jobs clear names that describe their purpose.</p> </li> <li> <p>Set appropriate timeouts: Based on the expected runtime of the job.</p> </li> <li> <p>Validate inputs in job functions: Detect invalid inputs early.</p> </li> <li> <p>Return structured results: Results should provide useful information about what was accomplished.</p> </li> <li> <p>Record progress for long-running jobs: For long tasks, consider updating external state to track progress.</p> </li> </ol>"},{"location":"user-guide/defining-jobs/#example-data-processing-job","title":"Example: Data Processing Job","text":"<p>Here's a complete example of defining a data processing job:</p> <pre><code>import pandas as pd\nfrom gigq import Job, JobQueue\ndef process_csv(input_file, output_file, drop_nulls=False, columns=None):\n\"\"\"\n    Process a CSV file and save the results.\n    Args:\n        input_file: Path to input CSV\n        output_file: Path to save processed CSV\n        drop_nulls: Whether to drop rows with null values\n        columns: Specific columns to keep (None means all)\n    Returns:\n        dict: Processing statistics\n    \"\"\"\n# Read the data\ndf = pd.read_csv(input_file)\ninitial_rows = len(df)\n# Apply transformations\nif columns:\ndf = df[columns]\nif drop_nulls:\ndf = df.dropna()\n# Save the results\ndf.to_csv(output_file, index=False)\n# Return statistics\nreturn {\n\"input_file\": input_file,\n\"output_file\": output_file,\n\"initial_rows\": initial_rows,\n\"final_rows\": len(df),\n\"rows_dropped\": initial_rows - len(df),\n\"columns_kept\": list(df.columns)\n}\n# Create the job\ncsv_job = Job(\nname=\"process_monthly_data\",\nfunction=process_csv,\nparams={\n\"input_file\": \"data/monthly/raw_202301.csv\",\n\"output_file\": \"data/monthly/processed_202301.csv\",\n\"drop_nulls\": True,\n\"columns\": [\"date\", \"value\", \"category\"]\n},\nmax_attempts=2,\ntimeout=300,\ndescription=\"Process monthly data for January 2023\"\n)\n# Submit the job\nqueue = JobQueue(\"data_processing.db\")\njob_id = queue.submit(csv_job)\nprint(f\"Submitted job: {job_id}\")\n</code></pre>"},{"location":"user-guide/defining-jobs/#next-steps","title":"Next Steps","text":"<p>Now that you understand how to define jobs, learn how to:</p> <ul> <li>Manage jobs in the queue</li> <li>Process jobs with workers</li> <li>Create workflows with dependencies</li> </ul>"},{"location":"user-guide/error-handling/","title":"Error Handling","text":"<p>Robust error handling is essential for any job processing system. This guide explains how GigQ handles errors, how to implement retry logic, and best practices for error recovery.</p>"},{"location":"user-guide/error-handling/#how-gigq-handles-errors","title":"How GigQ Handles Errors","text":"<p>When a job raises an exception during execution, GigQ follows these steps:</p> <ol> <li>The exception is caught by the worker</li> <li>The error details are logged</li> <li>The job's attempt counter is incremented</li> <li>If the job has not reached its maximum attempts, it's requeued as pending</li> <li>If the job has reached its maximum attempts, it's marked as failed</li> <li>The error message and traceback are stored in the job's record</li> </ol> <p>This approach ensures that:</p> <ul> <li>Transient errors can be resolved through retries</li> <li>Permanent errors are properly recorded for diagnosis</li> <li>The system remains stable even when jobs fail</li> </ul>"},{"location":"user-guide/error-handling/#automatic-retry-mechanism","title":"Automatic Retry Mechanism","text":"<p>By default, GigQ will retry failed jobs automatically:</p> <pre><code>from gigq import Job\n# This job will be attempted up to 3 times (default)\njob = Job(\nname=\"potentially_failing_job\",\nfunction=risky_operation,\nparams={\"important\": \"data\"}\n)\n# Customize retry attempts\njob_with_more_retries = Job(\nname=\"important_job\",\nfunction=critical_operation,\nparams={\"data\": \"valuable\"},\nmax_attempts=5  # Try up to 5 times\n)\n# Disable retries\njob_without_retries = Job(\nname=\"one_shot_job\",\nfunction=simple_operation,\nparams={\"quick\": \"task\"},\nmax_attempts=1  # Only try once\n)\n</code></pre> <p>The <code>max_attempts</code> parameter determines how many times a job will be executed before it's considered permanently failed.</p>"},{"location":"user-guide/error-handling/#handling-timeouts","title":"Handling Timeouts","text":"<p>Jobs that run for too long will be interrupted and potentially retried:</p> <pre><code># Job with a 5-minute timeout\njob = Job(\nname=\"long_running_job\",\nfunction=process_large_file,\nparams={\"file\": \"huge_dataset.csv\"},\ntimeout=300,  # 5 minutes in seconds\nmax_attempts=2\n)\n</code></pre> <p>When a job exceeds its timeout:</p> <ol> <li>It's marked as timed out or requeued for retry</li> <li>The worker moves on to the next job</li> <li>A timeout error is recorded in the job's execution history</li> </ol> <p>Timeouts are detected:</p> <ul> <li>By the worker processing the job (if it's still running)</li> <li>By any worker during its startup (if it finds abandoned jobs)</li> </ul>"},{"location":"user-guide/error-handling/#implementing-error-handling-in-job-functions","title":"Implementing Error Handling in Job Functions","text":"<p>While GigQ handles exceptions at the system level, it's often better to handle expected errors within your job functions:</p> <pre><code>def robust_job_function(input_data):\ntry:\n# Attempt to process the data\nresult = process_data(input_data)\nreturn {\"status\": \"success\", \"result\": result}\nexcept ValueError as e:\n# Handle expected validation errors\nlogger.warning(f\"Validation error: {e}\")\nreturn {\"status\": \"error\", \"error_type\": \"validation\", \"message\": str(e)}\nexcept IOError as e:\n# Handle I/O errors (might be transient)\nlogger.error(f\"I/O error: {e}\")\n# Re-raise to trigger GigQ's retry mechanism\nraise\nexcept Exception as e:\n# Log unexpected errors\nlogger.exception(f\"Unexpected error: {e}\")\n# Re-raise to trigger GigQ's retry mechanism\nraise\n</code></pre> <p>This approach allows you to:</p> <ul> <li>Handle expected errors gracefully</li> <li>Return partial results or error information</li> <li>Let GigQ handle retries for transient errors</li> <li>Properly log all error information</li> </ul>"},{"location":"user-guide/error-handling/#manual-retry-and-requeue","title":"Manual Retry and Requeue","text":"<p>You can manually requeue failed jobs:</p> <pre><code>from gigq import JobQueue\nqueue = JobQueue(\"jobs.db\")\n# Requeue a failed job\nif queue.requeue_job(job_id):\nprint(f\"Job {job_id} requeued successfully\")\nelse:\nprint(f\"Failed to requeue job {job_id}\")\n</code></pre> <p>The <code>requeue_job</code> method:</p> <ul> <li>Resets the job's attempt counter to 0</li> <li>Sets the status back to \"pending\"</li> <li>Clears any error information</li> <li>Returns <code>True</code> if successful, <code>False</code> otherwise</li> </ul>"},{"location":"user-guide/error-handling/#job-dependencies-and-error-propagation","title":"Job Dependencies and Error Propagation","text":"<p>When a job fails, its dependent jobs won't run:</p> <pre><code># Create a workflow with dependencies\nworkflow = Workflow(\"data_pipeline\")\nextract_job = Job(name=\"extract\", function=extract_data)\ntransform_job = Job(name=\"transform\", function=transform_data)\nload_job = Job(name=\"load\", function=load_data)\nworkflow.add_job(extract_job)\nworkflow.add_job(transform_job, depends_on=[extract_job])\nworkflow.add_job(load_job, depends_on=[transform_job])\n# Submit the workflow\nworkflow.submit_all(queue)\n</code></pre> <p>If <code>extract_job</code> fails:</p> <ul> <li><code>transform_job</code> won't run because its dependency failed</li> <li><code>load_job</code> won't run because its dependency won't run</li> </ul> <p>This prevents cascading failures and ensures data integrity.</p>"},{"location":"user-guide/error-handling/#handling-different-types-of-errors","title":"Handling Different Types of Errors","text":"<p>Different errors may require different handling strategies:</p>"},{"location":"user-guide/error-handling/#transient-errors","title":"Transient Errors","text":"<p>Transient errors like network timeouts or temporary service unavailability can often be resolved by simply retrying:</p> <pre><code>def fetch_remote_data(url):\nmax_retries = 3\nretry_delay = 5  # seconds\nfor attempt in range(max_retries):\ntry:\nresponse = requests.get(url, timeout=10)\nresponse.raise_for_status()\nreturn response.json()\nexcept (requests.ConnectionError, requests.Timeout) as e:\nif attempt &lt; max_retries - 1:\ntime.sleep(retry_delay * (attempt + 1))  # Exponential backoff\ncontinue\nraise  # Re-raise after all retries failed\n</code></pre>"},{"location":"user-guide/error-handling/#permanent-errors","title":"Permanent Errors","text":"<p>Permanent errors like invalid input data should be handled definitively:</p> <pre><code>def process_data(data):\n# Validate input\nif not validate_input(data):\n# Return a result indicating validation failure\nreturn {\n\"success\": False,\n\"error\": \"Input validation failed\",\n\"details\": get_validation_errors(data)\n}\n# Process the valid data\nresult = perform_processing(data)\nreturn {\n\"success\": True,\n\"result\": result\n}\n</code></pre>"},{"location":"user-guide/error-handling/#catastrophic-errors","title":"Catastrophic Errors","text":"<p>For truly catastrophic errors that might affect system stability:</p> <pre><code>def risky_operation():\ntry:\n# Attempt the operation\nresult = perform_risky_operation()\nreturn result\nexcept Exception as e:\n# Log detailed error information\nlogger.critical(f\"Catastrophic error: {e}\", exc_info=True)\n# Notify administrators\nsend_alert(\"Catastrophic job failure\", str(e))\n# Raise to mark the job as failed\nraise\n</code></pre>"},{"location":"user-guide/error-handling/#logging-and-monitoring","title":"Logging and Monitoring","text":"<p>Effective error handling relies on good logging and monitoring:</p> <pre><code>import logging\n# Configure logging\nlogging.basicConfig(\nlevel=logging.INFO,\nformat='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\nfilename='gigq.log'\n)\nlogger = logging.getLogger('gigq.jobs')\ndef job_function_with_logging(param1, param2):\nlogger.info(f\"Starting job with params: {param1}, {param2}\")\ntry:\n# Perform the job\nresult = perform_operation(param1, param2)\nlogger.info(f\"Job completed successfully: {result}\")\nreturn result\nexcept Exception as e:\nlogger.exception(f\"Job failed: {e}\")\nraise\n</code></pre>"},{"location":"user-guide/error-handling/#creating-a-custom-error-handler","title":"Creating a Custom Error Handler","text":"<p>You can create a custom error handler to centralize your error handling logic:</p> <pre><code>class JobErrorHandler:\ndef __init__(self, queue, notification_service=None):\nself.queue = queue\nself.notification_service = notification_service\nself.logger = logging.getLogger('gigq.error_handler')\ndef handle_failed_jobs(self):\n\"\"\"Check for failed jobs and handle them.\"\"\"\nfailed_jobs = self.queue.list_jobs(status=\"failed\")\nfor job in failed_jobs:\n# Analyze the error\nerror_message = job.get('error', '')\n# Handle based on error type\nif 'connection refused' in error_message.lower():\n# Network issue - requeue\nself.logger.info(f\"Requeuing job {job['id']} due to connection error\")\nself.queue.requeue_job(job['id'])\nelif 'validation error' in error_message.lower():\n# Data validation issue - notify but don't retry\nself.logger.warning(f\"Job {job['id']} failed validation: {error_message}\")\nif self.notification_service:\nself.notification_service.send_alert(\nf\"Job {job['name']} failed validation\",\nf\"Error: {error_message}\\nJob ID: {job['id']}\"\n)\nelse:\n# Unknown error - notify\nself.logger.error(f\"Unhandled job failure: {job['id']} - {error_message}\")\nif self.notification_service:\nself.notification_service.send_alert(\nf\"Unhandled job failure: {job['name']}\",\nf\"Error: {error_message}\\nJob ID: {job['id']}\"\n)\n# Usage\nerror_handler = JobErrorHandler(queue, notification_service=email_service)\nerror_handler.handle_failed_jobs()\n</code></pre>"},{"location":"user-guide/error-handling/#best-practices-for-error-handling","title":"Best Practices for Error Handling","text":"<ol> <li> <p>Set appropriate <code>max_attempts</code> based on the nature of the job and expected transient failures.</p> </li> <li> <p>Use appropriate timeouts to prevent jobs from running indefinitely.</p> </li> <li> <p>Handle expected errors within job functions when possible.</p> </li> <li> <p>Implement exponential backoff for retrying operations with external dependencies.</p> </li> <li> <p>Include context in error messages to aid in debugging.</p> </li> <li> <p>Log generously but appropriately based on error severity.</p> </li> <li> <p>Design for idempotency so that jobs can be safely retried.</p> </li> <li> <p>Return structured results that include success/failure status and error details.</p> </li> <li> <p>Implement monitoring to detect patterns of failures.</p> </li> <li> <p>Consider dependencies when designing error handling strategies. Jobs with many dependents may need more aggressive retry logic.</p> </li> </ol>"},{"location":"user-guide/error-handling/#example-robust-data-import-job","title":"Example: Robust Data Import Job","text":"<p>Here's a complete example of a robust data import job with comprehensive error handling:</p> <pre><code>import logging\nimport time\nimport requests\nfrom gigq import Job, JobQueue\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger('data_import')\ndef import_data_from_api(api_url, target_file, api_key=None, max_retries=3, retry_delay=5):\n\"\"\"\n    Import data from an API and save to a file.\n    Args:\n        api_url: URL of the API endpoint\n        target_file: Path to save the imported data\n        api_key: Optional API key for authentication\n        max_retries: Maximum number of retry attempts for transient errors\n        retry_delay: Base delay between retries (in seconds)\n    Returns:\n        dict: Result information including success status and error details if any\n    \"\"\"\nlogger.info(f\"Starting data import from {api_url} to {target_file}\")\nheaders = {}\nif api_key:\nheaders['Authorization'] = f\"Bearer {api_key}\"\n# Track attempt number for internal retries\nfor attempt in range(1, max_retries + 1):\ntry:\nlogger.info(f\"API request attempt {attempt}/{max_retries}\")\n# Make the API request\nresponse = requests.get(api_url, headers=headers, timeout=30)\n# Check for HTTP errors\nif response.status_code == 429:  # Too Many Requests\nwait_time = int(response.headers.get('Retry-After', retry_delay * attempt))\nlogger.warning(f\"Rate limited. Waiting {wait_time} seconds before retry.\")\ntime.sleep(wait_time)\ncontinue\nelif response.status_code == 401 or response.status_code == 403:\nreturn {\n\"success\": False,\n\"error\": \"API authentication failed\",\n\"status_code\": response.status_code,\n\"details\": response.text\n}\nelif response.status_code &gt;= 400:\n# Server error (possibly transient)\nif attempt &lt; max_retries and response.status_code &gt;= 500:\nwait_time = retry_delay * (2 ** (attempt - 1))  # Exponential backoff\nlogger.warning(f\"Server error: {response.status_code}. Retrying in {wait_time} seconds.\")\ntime.sleep(wait_time)\ncontinue\nelse:\nreturn {\n\"success\": False,\n\"error\": f\"API request failed with status {response.status_code}\",\n\"status_code\": response.status_code,\n\"details\": response.text\n}\n# Parse the data\ntry:\ndata = response.json()\nexcept ValueError:\nreturn {\n\"success\": False,\n\"error\": \"Failed to parse API response as JSON\",\n\"details\": response.text[:1000]  # Include the start of the response\n}\n# Validate the data structure\nif not isinstance(data, list) or len(data) == 0:\nreturn {\n\"success\": False,\n\"error\": \"API returned invalid or empty data structure\",\n\"details\": str(data)[:1000]  # Include the start of the data\n}\n# Write the data to the file\ntry:\nwith open(target_file, 'w') as f:\nimport json\njson.dump(data, f, indent=2)\nexcept IOError as e:\nreturn {\n\"success\": False,\n\"error\": f\"Failed to write data to file: {str(e)}\",\n\"file\": target_file\n}\n# Success\nlogger.info(f\"Successfully imported {len(data)} records to {target_file}\")\nreturn {\n\"success\": True,\n\"records_imported\": len(data),\n\"file\": target_file\n}\nexcept requests.Timeout:\nif attempt &lt; max_retries:\nwait_time = retry_delay * (2 ** (attempt - 1))  # Exponential backoff\nlogger.warning(f\"Request timed out. Retrying in {wait_time} seconds.\")\ntime.sleep(wait_time)\nelse:\nreturn {\n\"success\": False,\n\"error\": \"API request timed out after multiple attempts\",\n\"attempts\": attempt\n}\nexcept requests.ConnectionError as e:\nif attempt &lt; max_retries:\nwait_time = retry_delay * (2 ** (attempt - 1))  # Exponential backoff\nlogger.warning(f\"Connection error: {e}. Retrying in {wait_time} seconds.\")\ntime.sleep(wait_time)\nelse:\nreturn {\n\"success\": False,\n\"error\": f\"Failed to connect to API after {max_retries} attempts\",\n\"details\": str(e)\n}\nexcept Exception as e:\nlogger.exception(f\"Unexpected error during import\")\nreturn {\n\"success\": False,\n\"error\": f\"Unexpected error: {str(e)}\",\n\"exception_type\": type(e).__name__\n}\n# Should never reach here, but just in case\nreturn {\n\"success\": False,\n\"error\": \"Maximum retry attempts exceeded\"\n}\n# Usage example\nif __name__ == \"__main__\":\n# Create a job to import data\nimport_job = Job(\nname=\"daily_data_import\",\nfunction=import_data_from_api,\nparams={\n\"api_url\": \"https://api.example.com/data\",\n\"target_file\": \"data/daily_import.json\",\n\"api_key\": \"secret_key\",\n\"max_retries\": 3,\n\"retry_delay\": 5\n},\nmax_attempts=2,  # GigQ-level retries (on top of function-level retries)\ntimeout=300,  # 5 minutes\ndescription=\"Import daily data from the Example API\"\n)\n# Submit the job\nqueue = JobQueue(\"import_jobs.db\")\njob_id = queue.submit(import_job)\nprint(f\"Submitted import job with ID: {job_id}\")\n</code></pre>"},{"location":"user-guide/error-handling/#next-steps","title":"Next Steps","text":"<p>Now that you understand how GigQ handles errors, you may want to explore:</p> <ul> <li>Job Queue Management - Learn more about managing and monitoring jobs</li> <li>Workers - Understand how workers process jobs and handle failures</li> <li>Workflows - Learn how to create complex workflows with error handling</li> <li>Advanced Concurrency - Understand how GigQ handles concurrent job processing</li> </ul>"},{"location":"user-guide/job-queue/","title":"Job Queue","text":"<p>The <code>JobQueue</code> is the central component in GigQ that manages job persistence, state transitions, and retrieval. This guide explains how to use the job queue to submit, monitor, and manage jobs.</p>"},{"location":"user-guide/job-queue/#creating-a-job-queue","title":"Creating a Job Queue","text":"<p>To create a job queue, instantiate the <code>JobQueue</code> class with a path to a SQLite database file:</p> <pre><code>from gigq import JobQueue\n# Create a new job queue (or connect to an existing one)\nqueue = JobQueue(\"jobs.db\")\n</code></pre> <p>If the database file doesn't exist, it will be created automatically. If it does exist, the queue will connect to it.</p> <p>You can also specify whether to initialize the database:</p> <pre><code># Connect without initializing (useful if you're sure the DB is already initialized)\nqueue = JobQueue(\"jobs.db\", initialize=False)\n</code></pre>"},{"location":"user-guide/job-queue/#submitting-jobs","title":"Submitting Jobs","text":"<p>Once you have a job queue, you can submit jobs to it:</p> <pre><code>from gigq import Job\n# Create a job\njob = Job(\nname=\"example_job\",\nfunction=process_data,\nparams={\"data\": \"example\"}\n)\n# Submit the job\njob_id = queue.submit(job)\nprint(f\"Submitted job with ID: {job_id}\")\n</code></pre> <p>The <code>submit</code> method returns the job ID, which you can use to track the job's status.</p>"},{"location":"user-guide/job-queue/#checking-job-status","title":"Checking Job Status","text":"<p>To check the status of a job, use the <code>get_status</code> method:</p> <pre><code># Get job status\nstatus = queue.get_status(job_id)\nprint(f\"Job status: {status['status']}\")\nprint(f\"Created at: {status['created_at']}\")\nif status['status'] == 'completed':\nprint(f\"Result: {status['result']}\")\nelif status['status'] == 'failed':\nprint(f\"Error: {status['error']}\")\n</code></pre> <p>The <code>get_status</code> method returns a dictionary with the following keys:</p> Key Description <code>exists</code> Boolean indicating if the job exists in the queue <code>id</code> The job ID <code>name</code> The job name <code>status</code> Current status (pending, running, completed, failed, cancelled, timeout) <code>created_at</code> ISO format timestamp of when the job was created <code>updated_at</code> ISO format timestamp of when the job was last updated <code>attempts</code> Number of execution attempts <code>max_attempts</code> Maximum number of attempts <code>started_at</code> When the job started (if it has started) <code>completed_at</code> When the job completed (if it has completed) <code>params</code> The parameters passed to the job <code>result</code> The job result (if completed) <code>error</code> Error message (if failed) <code>worker_id</code> ID of the worker processing the job (if running) <code>executions</code> List of execution attempts (each with id, status, started_at, completed_at)"},{"location":"user-guide/job-queue/#listing-jobs","title":"Listing Jobs","text":"<p>To list jobs in the queue, use the <code>list_jobs</code> method:</p> <pre><code>from gigq import JobStatus\n# List all jobs\nall_jobs = queue.list_jobs()\n# List only pending jobs\npending_jobs = queue.list_jobs(status=JobStatus.PENDING)\n# List with a limit\nrecent_jobs = queue.list_jobs(limit=10)\n# Print job information\nfor job in pending_jobs:\nprint(f\"{job['id']}: {job['name']} (priority: {job['priority']})\")\n</code></pre> <p>The <code>list_jobs</code> method returns a list of job dictionaries with the same keys as <code>get_status</code>.</p>"},{"location":"user-guide/job-queue/#cancelling-jobs","title":"Cancelling Jobs","text":"<p>To cancel a pending job:</p> <pre><code># Cancel a job\nresult = queue.cancel(job_id)\nif result:\nprint(f\"Job {job_id} cancelled successfully\")\nelse:\nprint(f\"Failed to cancel job {job_id} (may not be in pending state)\")\n</code></pre> <p>The <code>cancel</code> method returns <code>True</code> if the job was cancelled, or <code>False</code> if it couldn't be cancelled (e.g., because it's already running or completed).</p>"},{"location":"user-guide/job-queue/#requeuing-failed-jobs","title":"Requeuing Failed Jobs","text":"<p>If a job fails, you can requeue it:</p> <pre><code># Requeue a failed job\nresult = queue.requeue_job(job_id)\nif result:\nprint(f\"Job {job_id} requeued successfully\")\nelse:\nprint(f\"Failed to requeue job {job_id} (may not be in failed state)\")\n</code></pre> <p>The <code>requeue_job</code> method:</p> <ul> <li>Resets the job's <code>attempts</code> counter to 0</li> <li>Sets the job's status to <code>pending</code></li> <li>Clears the error message</li> <li>Returns <code>True</code> if successful, <code>False</code> otherwise</li> </ul>"},{"location":"user-guide/job-queue/#clearing-completed-jobs","title":"Clearing Completed Jobs","text":"<p>To clean up the queue by removing completed or cancelled jobs:</p> <pre><code># Clear all completed and cancelled jobs\ncount = queue.clear_completed()\nprint(f\"Cleared {count} completed jobs\")\n# Clear jobs completed before a certain date\nfrom datetime import datetime, timedelta\none_week_ago = (datetime.now() - timedelta(days=7)).isoformat()\ncount = queue.clear_completed(before_timestamp=one_week_ago)\nprint(f\"Cleared {count} jobs older than one week\")\n</code></pre>"},{"location":"user-guide/job-queue/#job-queue-persistence","title":"Job Queue Persistence","text":"<p>The job queue uses SQLite as its backend, which provides:</p> <ol> <li>Persistence - Jobs remain in the queue even if the application restarts</li> <li>Atomicity - Job state transitions are atomic (all-or-nothing)</li> <li>Concurrency - Multiple workers can access the queue safely</li> </ol> <p>When you create a <code>JobQueue</code> with the same database file, you're connecting to the same queue. This allows you to:</p> <ul> <li>Submit jobs from one process</li> <li>Process jobs from another process</li> <li>Monitor jobs from a third process</li> </ul> <p>All while maintaining consistency.</p>"},{"location":"user-guide/job-queue/#advanced-usage","title":"Advanced Usage","text":""},{"location":"user-guide/job-queue/#working-with-job-dependencies","title":"Working with Job Dependencies","text":"<p>When submitting jobs with dependencies, the queue ensures that dependent jobs only run after their dependencies are complete:</p> <pre><code># Create and submit a job\njob1 = Job(name=\"first_job\", function=first_function)\njob1_id = queue.submit(job1)\n# Create and submit a dependent job\njob2 = Job(\nname=\"second_job\",\nfunction=second_function,\ndependencies=[job1_id]\n)\njob2_id = queue.submit(job2)\n</code></pre> <p>The second job will only be picked up by workers after the first job has completed successfully.</p>"},{"location":"user-guide/job-queue/#using-custom-job-processing-order","title":"Using Custom Job Processing Order","text":"<p>By default, workers process jobs in order of:</p> <ol> <li>Priority (higher first)</li> <li>Creation time (older first)</li> </ol> <p>You can use the priority parameter to control processing order:</p> <pre><code># High priority job (processes first)\nhigh_job = Job(name=\"urgent\", function=urgent_task, priority=100)\nqueue.submit(high_job)\n# Normal priority job\nnormal_job = Job(name=\"normal\", function=normal_task, priority=0)\nqueue.submit(normal_job)\n# Low priority job (processes last)\nlow_job = Job(name=\"background\", function=background_task, priority=-100)\nqueue.submit(low_job)\n</code></pre>"},{"location":"user-guide/job-queue/#monitoring-job-progress","title":"Monitoring Job Progress","text":"<p>For long-running jobs, you might want to periodically check their status:</p> <pre><code>import time\njob_id = queue.submit(long_running_job)\nwhile True:\nstatus = queue.get_status(job_id)\nprint(f\"Job status: {status['status']}\")\nif status['status'] in ('completed', 'failed', 'cancelled', 'timeout'):\nbreak\ntime.sleep(5)  # Check every 5 seconds\n</code></pre>"},{"location":"user-guide/job-queue/#working-with-multiple-queues","title":"Working with Multiple Queues","text":"<p>You can work with multiple queues by creating multiple <code>JobQueue</code> instances:</p> <pre><code># Create different queues for different types of jobs\nhigh_priority_queue = JobQueue(\"high_priority.db\")\nbackground_queue = JobQueue(\"background.db\")\n# Submit to the appropriate queue\nhigh_priority_queue.submit(important_job)\nbackground_queue.submit(background_job)\n# Create workers for each queue\nhigh_worker = Worker(\"high_priority.db\")\nbackground_worker = Worker(\"background.db\")\n</code></pre> <p>This allows you to:</p> <ul> <li>Separate different types of jobs</li> <li>Allocate different resources to different queues</li> <li>Manage priorities across job categories</li> </ul>"},{"location":"user-guide/job-queue/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Use a consistent database path across all components that need to access the same queue.</p> </li> <li> <p>Handle database locking - SQLite can experience locking issues under heavy concurrency. If you're running many workers, consider using a more robust backend in a production environment.</p> </li> <li> <p>Regularly clean up completed jobs to keep the database size manageable, either through <code>clear_completed()</code> or by setting up a periodic task.</p> </li> <li> <p>Monitor queue size - If the queue grows continuously, it may indicate that you need more workers or that jobs are taking too long to process.</p> </li> <li> <p>Use appropriate error handling when submitting jobs or checking status to account for potential database connectivity issues.</p> </li> </ol>"},{"location":"user-guide/job-queue/#example-job-queue-dashboard","title":"Example: Job Queue Dashboard","text":"<p>Here's an example of creating a simple dashboard for monitoring job queue status:</p> <pre><code>def print_queue_stats(queue):\n\"\"\"Print statistics about the job queue.\"\"\"\n# Get job counts by status\nstatuses = [\"pending\", \"running\", \"completed\", \"failed\", \"cancelled\", \"timeout\"]\ncounts = {}\nfor status in statuses:\njobs = queue.list_jobs(status=status)\ncounts[status] = len(jobs)\ntotal = sum(counts.values())\n# Print summary\nprint(f\"=== Job Queue Statistics ===\")\nprint(f\"Total jobs: {total}\")\nfor status, count in counts.items():\npercentage = (count / total) * 100 if total &gt; 0 else 0\nprint(f\"{status.capitalize()}: {count} ({percentage:.1f}%)\")\n# Print recently completed jobs\ncompleted = queue.list_jobs(status=\"completed\", limit=5)\nif completed:\nprint(\"\\nRecently completed jobs:\")\nfor job in completed:\nprint(f\"  {job['name']} - Completed at {job['completed_at']}\")\n# Print failed jobs\nfailed = queue.list_jobs(status=\"failed\", limit=5)\nif failed:\nprint(\"\\nRecent failures:\")\nfor job in failed:\nprint(f\"  {job['name']} - {job['error']}\")\n# Usage\nqueue = JobQueue(\"jobs.db\")\nprint_queue_stats(queue)\n</code></pre>"},{"location":"user-guide/job-queue/#next-steps","title":"Next Steps","text":"<p>Now that you understand how to manage jobs in the queue, learn how to:</p> <ul> <li>Process jobs with workers</li> <li>Create workflows with dependencies</li> <li>Handle errors and retries</li> </ul>"},{"location":"user-guide/workers/","title":"Workers","text":"<p>Workers are responsible for executing jobs from the queue. This guide explains how to create, configure, and manage workers in GigQ.</p>"},{"location":"user-guide/workers/#creating-a-worker","title":"Creating a Worker","text":"<p>To create a worker, instantiate the <code>Worker</code> class with a path to the job queue database:</p> <pre><code>from gigq import Worker\n# Create a worker\nworker = Worker(\"jobs.db\")\n</code></pre> <p>By default, the worker will be assigned a unique ID. You can specify a custom ID if needed:</p> <pre><code># Create a worker with a custom ID\nworker = Worker(\"jobs.db\", worker_id=\"worker-1\")\n</code></pre>"},{"location":"user-guide/workers/#starting-a-worker","title":"Starting a Worker","text":"<p>Once you've created a worker, you can start it:</p> <pre><code># Start the worker (blocks until the worker is stopped)\nworker.start()\n</code></pre> <p>The <code>start</code> method blocks until the worker is stopped, typically by a keyboard interrupt (Ctrl+C) or a signal.</p> <p>To run the worker in the background, you can use a separate thread or process:</p> <pre><code>import threading\n# Start the worker in a background thread\nworker_thread = threading.Thread(target=worker.start)\nworker_thread.daemon = True  # Thread will exit when the main program exits\nworker_thread.start()\n# Continue with other tasks...\n</code></pre>"},{"location":"user-guide/workers/#processing-a-single-job","title":"Processing a Single Job","text":"<p>If you want to process just one job and then stop:</p> <pre><code># Process a single job\nresult = worker.process_one()\nif result:\nprint(\"Processed one job\")\nelse:\nprint(\"No jobs available to process\")\n</code></pre> <p>The <code>process_one</code> method returns <code>True</code> if a job was processed, or <code>False</code> if no jobs were available.</p>"},{"location":"user-guide/workers/#stopping-a-worker","title":"Stopping a Worker","text":"<p>To stop a running worker:</p> <pre><code># Stop the worker\nworker.stop()\n</code></pre> <p>This will cause the worker to exit its processing loop after completing its current job (if any).</p> <p>Workers also handle signals automatically. When a worker receives a <code>SIGINT</code> (Ctrl+C) or <code>SIGTERM</code> signal, it will stop gracefully.</p>"},{"location":"user-guide/workers/#worker-configuration","title":"Worker Configuration","text":"<p>The <code>Worker</code> class accepts several parameters to customize its behavior:</p> Parameter Type Default Description <code>db_path</code> str Path to the SQLite database file (required) <code>worker_id</code> str auto-generated Unique identifier for the worker <code>polling_interval</code> int 5 How often to check for new jobs (in seconds) <p>Example with custom configuration:</p> <pre><code># Create a worker with custom parameters\nworker = Worker(\n\"jobs.db\",\nworker_id=\"worker-batch-processor\",\npolling_interval=2  # Check for new jobs every 2 seconds\n)\n</code></pre>"},{"location":"user-guide/workers/#how-workers-process-jobs","title":"How Workers Process Jobs","text":"<p>When a worker runs, it follows this process:</p> <ol> <li>Check for timed out jobs and handle them</li> <li>Try to claim a job from the queue</li> <li>If a job is claimed, execute its function</li> <li>Update the job status based on the execution result</li> <li>If no job is claimed, wait for the polling interval</li> <li>Repeat until stopped</li> </ol> <p>This loop ensures that jobs are processed efficiently while minimizing database load during idle periods.</p>"},{"location":"user-guide/workers/#job-execution-process","title":"Job Execution Process","text":"<p>When a worker executes a job, it follows these steps:</p> <ol> <li>Claim the job: Mark the job as running and increment its attempt counter</li> <li>Import the function: Dynamically import the function from its module</li> <li>Execute the function: Call the function with the job's parameters</li> <li>Record the result: Store the function's return value or error message</li> <li>Update the job status: Mark the job as completed, failed, or pending (for retry)</li> </ol> <p>If the function raises an exception, the worker will:</p> <ul> <li>Log the error</li> <li>Determine if the job should be retried based on <code>max_attempts</code></li> <li>Update the job status accordingly</li> </ul>"},{"location":"user-guide/workers/#running-multiple-workers","title":"Running Multiple Workers","text":"<p>You can run multiple workers simultaneously to process jobs in parallel:</p> <pre><code># In worker_script.py\nfrom gigq import Worker\nimport sys\nworker_id = sys.argv[1] if len(sys.argv) &gt; 1 else None\nworker = Worker(\"jobs.db\", worker_id=worker_id)\nworker.start()\n</code></pre> <p>Then run multiple instances:</p> <pre><code># Run 3 workers\npython worker_script.py worker-1 &amp;\npython worker_script.py worker-2 &amp;\npython worker_script.py worker-3 &amp;\n</code></pre> <p>Each worker will claim and process jobs independently, with SQLite's locking mechanisms ensuring that each job is processed exactly once.</p>"},{"location":"user-guide/workers/#worker-concurrency","title":"Worker Concurrency","text":"<p>GigQ uses SQLite's locking mechanisms to ensure safe concurrency:</p> <ol> <li>Workers claim jobs using an exclusive transaction</li> <li>Only one worker can claim a particular job</li> <li>If multiple workers try to claim the same job, only one will succeed</li> </ol> <p>This approach provides robust concurrency without requiring complex distributed locking mechanisms.</p>"},{"location":"user-guide/workers/#handling-worker-crashes","title":"Handling Worker Crashes","text":"<p>If a worker crashes while processing a job, the job remains in the \"running\" state. GigQ handles this through timeout detection:</p> <ol> <li>When a worker starts, it checks for jobs that have been running longer than their timeout</li> <li>If it finds timed out jobs, it marks them as timed out or requeues them for retry</li> <li>This ensures that jobs don't get stuck in the running state if a worker crashes</li> </ol>"},{"location":"user-guide/workers/#worker-lifecycle","title":"Worker Lifecycle","text":"<p>A typical worker lifecycle looks like this:</p> <pre><code>stateDiagram-v2\n    [*] --&gt; Idle: Worker Started\n    Idle --&gt; CheckingTimeouts: Check for Timeouts\n    CheckingTimeouts --&gt; LookingForJobs: Look for Jobs\n    LookingForJobs --&gt; Idle: No Jobs Available\n    LookingForJobs --&gt; ProcessingJob: Job Claimed\n    ProcessingJob --&gt; Idle: Job Completed/Failed\n    Idle --&gt; [*]: Worker Stopped</code></pre>"},{"location":"user-guide/workers/#monitoring-workers","title":"Monitoring Workers","text":"<p>You can monitor worker activity through the job queue:</p> <pre><code>from gigq import JobQueue, JobStatus\nqueue = JobQueue(\"jobs.db\")\n# Get all running jobs\nrunning_jobs = queue.list_jobs(status=JobStatus.RUNNING)\n# Group by worker\nworkers = {}\nfor job in running_jobs:\nworker_id = job.get('worker_id')\nif worker_id:\nif worker_id not in workers:\nworkers[worker_id] = []\nworkers[worker_id].append(job)\n# Print worker activity\nfor worker_id, jobs in workers.items():\nprint(f\"Worker {worker_id} is processing {len(jobs)} jobs:\")\nfor job in jobs:\nprint(f\"  - {job['name']} (started at {job['started_at']})\")\n</code></pre>"},{"location":"user-guide/workers/#worker-best-practices","title":"Worker Best Practices","text":"<ol> <li> <p>Use appropriate polling intervals: Lower values increase responsiveness but also database load.</p> </li> <li> <p>Set reasonable job timeouts: Ensure timeouts are long enough for normal execution but short enough to detect hung jobs.</p> </li> <li> <p>Handle signals: Make sure your application handles SIGINT and SIGTERM properly to allow workers to shut down gracefully.</p> </li> <li> <p>Monitor worker health: Set up monitoring to restart workers if they crash or become unresponsive.</p> </li> <li> <p>Scale appropriately: Use enough workers to process your workload efficiently, but not so many that they overwhelm your system or database.</p> </li> <li> <p>Consider worker specialization: You can run different workers for different types of jobs by using separate queue databases.</p> </li> <li> <p>Log worker activity: Enable logging to track worker behavior and troubleshoot issues.</p> </li> </ol>"},{"location":"user-guide/workers/#example-background-processing-service","title":"Example: Background Processing Service","text":"<p>Here's an example of a background processing service that runs multiple workers:</p> <pre><code>\"\"\"\nBackground processing service for GigQ.\n\"\"\"\nimport argparse\nimport logging\nimport os\nimport signal\nimport sys\nimport time\nfrom multiprocessing import Process\nfrom gigq import Worker, JobQueue\n# Configure logging\nlogging.basicConfig(\nlevel=logging.INFO,\nformat='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n)\nlogger = logging.getLogger('background_service')\ndef run_worker(db_path, worker_id):\n\"\"\"Run a worker process.\"\"\"\nworker = Worker(db_path, worker_id=worker_id)\nlogger.info(f\"Starting worker {worker_id}\")\nworker.start()\nlogger.info(f\"Worker {worker_id} stopped\")\ndef main():\nparser = argparse.ArgumentParser(description=\"GigQ Background Processing Service\")\nparser.add_argument(\"--db\", default=\"jobs.db\", help=\"Path to job queue database\")\nparser.add_argument(\"--workers\", type=int, default=2, help=\"Number of worker processes\")\nparser.add_argument(\"--check-interval\", type=int, default=60,\nhelp=\"Interval (in seconds) to check for stalled jobs\")\nargs = parser.parse_args()\nlogger.info(f\"Starting background service with {args.workers} workers\")\n# Create worker processes\nprocesses = []\nfor i in range(args.workers):\nworker_id = f\"worker-{i+1}\"\np = Process(target=run_worker, args=(args.db, worker_id))\np.start()\nprocesses.append((p, worker_id))\nlogger.info(f\"Started worker process {worker_id} (PID: {p.pid})\")\n# Set up signal handlers\ndef handle_signal(sig, frame):\nlogger.info(f\"Received signal {sig}, shutting down...\")\nfor p, worker_id in processes:\nlogger.info(f\"Terminating worker {worker_id} (PID: {p.pid})\")\np.terminate()\nsys.exit(0)\nsignal.signal(signal.SIGINT, handle_signal)\nsignal.signal(signal.SIGTERM, handle_signal)\n# Monitor for stalled jobs periodically\nqueue = JobQueue(args.db)\ntry:\nwhile True:\n# Check for and restart any dead worker processes\nfor i, (p, worker_id) in enumerate(processes):\nif not p.is_alive():\nlogger.warning(f\"Worker {worker_id} (PID: {p.pid}) died, restarting...\")\nnew_p = Process(target=run_worker, args=(args.db, worker_id))\nnew_p.start()\nprocesses[i] = (new_p, worker_id)\nlogger.info(f\"Restarted worker {worker_id} (new PID: {new_p.pid})\")\n# Sleep until next check\ntime.sleep(args.check_interval)\nexcept Exception as e:\nlogger.error(f\"Error in main monitoring loop: {e}\")\nhandle_signal(signal.SIGTERM, None)\nif __name__ == \"__main__\":\nmain()\n</code></pre> <p>You can run this service from the command line:</p> <pre><code>python background_service.py --db jobs.db --workers 4\n</code></pre>"},{"location":"user-guide/workers/#next-steps","title":"Next Steps","text":"<p>Now that you understand how workers process jobs, learn more about:</p> <ul> <li>Error handling and job recovery</li> <li>Creating workflows with dependencies</li> <li>Advanced concurrency handling</li> </ul>"},{"location":"user-guide/workflows/","title":"Workflows","text":"<p>Workflows in GigQ allow you to define and manage complex sequences of jobs with dependencies. This is useful for creating multi-step processes where some jobs depend on the successful completion of others.</p>"},{"location":"user-guide/workflows/#basic-workflow-concepts","title":"Basic Workflow Concepts","text":"<p>A workflow consists of:</p> <ul> <li>A collection of related jobs</li> <li>Dependency relationships between those jobs</li> <li>A mechanism for submitting all jobs to a queue</li> </ul> <p>Jobs in a workflow are executed in an order that respects their dependencies, with independent jobs potentially running in parallel (if multiple workers are available).</p>"},{"location":"user-guide/workflows/#creating-a-workflow","title":"Creating a Workflow","text":"<p>To create a workflow:</p> <pre><code>from gigq import Workflow\n# Create a new workflow with a name\nworkflow = Workflow(\"data_processing_pipeline\")\n</code></pre>"},{"location":"user-guide/workflows/#adding-jobs-to-a-workflow","title":"Adding Jobs to a Workflow","text":"<p>You can add jobs to a workflow with or without dependencies:</p> <pre><code>from gigq import Job\n# Create some jobs\njob1 = Job(\nname=\"download_data\",\nfunction=download_data,\nparams={\"url\": \"https://example.com/data.csv\"}\n)\njob2 = Job(\nname=\"process_data\",\nfunction=process_data,\nparams={\"input_file\": \"data.csv\", \"output_file\": \"processed.csv\"}\n)\njob3 = Job(\nname=\"analyze_data\",\nfunction=analyze_data,\nparams={\"file\": \"processed.csv\"}\n)\n# Add jobs to the workflow, defining dependencies\nworkflow.add_job(job1)  # No dependencies\nworkflow.add_job(job2, depends_on=[job1])  # Depends on job1\nworkflow.add_job(job3, depends_on=[job2])  # Depends on job2\n</code></pre>"},{"location":"user-guide/workflows/#multiple-dependencies","title":"Multiple Dependencies","text":"<p>A job can depend on multiple other jobs:</p> <pre><code># Create more jobs\njob4 = Job(\nname=\"generate_report\",\nfunction=generate_report,\nparams={\"analysis_file\": \"analysis.json\", \"output_file\": \"report.pdf\"}\n)\njob5 = Job(\nname=\"send_notification\",\nfunction=send_notification,\nparams={\"report_file\": \"report.pdf\", \"recipients\": [\"user@example.com\"]}\n)\n# Job4 depends on both job2 and job3\nworkflow.add_job(job4, depends_on=[job2, job3])\n# Job5 depends on job4\nworkflow.add_job(job5, depends_on=[job4])\n</code></pre> <p>This would create a workflow like:</p> <pre><code>graph TD\n    A[Download Data] --&gt; B[Process Data]\n    B --&gt; C[Analyze Data]\n    B --&gt; D[Generate Report]\n    C --&gt; D\n    D --&gt; E[Send Notification]</code></pre>"},{"location":"user-guide/workflows/#submitting-a-workflow","title":"Submitting a Workflow","text":"<p>Once you've defined your workflow, you can submit all the jobs to a queue:</p> <pre><code>from gigq import JobQueue\n# Create a job queue\nqueue = JobQueue(\"workflow_jobs.db\")\n# Submit all jobs in the workflow\njob_ids = workflow.submit_all(queue)\nprint(f\"Submitted {len(job_ids)} jobs to the queue\")\n</code></pre> <p>This will submit all jobs to the queue with the appropriate dependencies. The workers will then process the jobs in the correct order.</p>"},{"location":"user-guide/workflows/#advanced-workflow-patterns","title":"Advanced Workflow Patterns","text":""},{"location":"user-guide/workflows/#fan-out-pattern","title":"Fan-Out Pattern","text":"<p>The fan-out pattern involves a single job spawning multiple parallel jobs:</p> <pre><code># Main job that splits the work\nsplit_job = Job(name=\"split_data\", function=split_data)\n# Worker jobs that process the splits in parallel\nworker_jobs = []\nfor i in range(5):\nworker_job = Job(\nname=f\"process_chunk_{i}\",\nfunction=process_chunk,\nparams={\"chunk_id\": i}\n)\nworker_jobs.append(worker_job)\n# Add to workflow\nworkflow.add_job(split_job)\nfor job in worker_jobs:\nworkflow.add_job(job, depends_on=[split_job])\n</code></pre>"},{"location":"user-guide/workflows/#fan-in-pattern","title":"Fan-In Pattern","text":"<p>The fan-in pattern involves multiple parallel jobs converging to a single job:</p> <pre><code># Add a job that combines results from all worker jobs\ncombine_job = Job(name=\"combine_results\", function=combine_results)\nworkflow.add_job(combine_job, depends_on=worker_jobs)\n</code></pre> <p>Together, the fan-out and fan-in patterns create a workflow like:</p> <pre><code>graph TD\n    A[Split Data] --&gt; B1[Process Chunk 0]\n    A --&gt; B2[Process Chunk 1]\n    A --&gt; B3[Process Chunk 2]\n    A --&gt; B4[Process Chunk 3]\n    A --&gt; B5[Process Chunk 4]\n    B1 --&gt; C[Combine Results]\n    B2 --&gt; C\n    B3 --&gt; C\n    B4 --&gt; C\n    B5 --&gt; C</code></pre>"},{"location":"user-guide/workflows/#dynamic-workflows","title":"Dynamic Workflows","text":"<p>You can create workflows dynamically based on runtime conditions:</p> <pre><code>def create_dynamic_workflow(data_source, date_range):\n\"\"\"Create a workflow based on runtime parameters.\"\"\"\nworkflow = Workflow(f\"process_{data_source}_{date_range[0]}_{date_range[1]}\")\n# Initial setup job\nsetup_job = Job(\nname=\"setup\",\nfunction=setup_environment,\nparams={\"data_source\": data_source}\n)\nworkflow.add_job(setup_job)\n# Create a job for each date in the range\ndate_jobs = []\ncurrent_date = date_range[0]\nwhile current_date &lt;= date_range[1]:\ndate_str = current_date.strftime('%Y-%m-%d')\njob = Job(\nname=f\"process_{date_str}\",\nfunction=process_date,\nparams={\"date\": date_str, \"source\": data_source}\n)\nworkflow.add_job(job, depends_on=[setup_job])\ndate_jobs.append(job)\ncurrent_date += timedelta(days=1)\n# Final job to generate a report\nreport_job = Job(\nname=\"generate_report\",\nfunction=generate_report,\nparams={\"date_range\": [date_range[0].strftime('%Y-%m-%d'),\ndate_range[1].strftime('%Y-%m-%d')]}\n)\nworkflow.add_job(report_job, depends_on=date_jobs)\nreturn workflow\n</code></pre>"},{"location":"user-guide/workflows/#monitoring-workflow-progress","title":"Monitoring Workflow Progress","text":"<p>To monitor the progress of a workflow:</p> <pre><code>def check_workflow_status(queue, job_ids):\n\"\"\"Check the status of all jobs in a workflow.\"\"\"\nstatuses = {\n\"pending\": 0,\n\"running\": 0,\n\"completed\": 0,\n\"failed\": 0,\n\"cancelled\": 0,\n\"timeout\": 0\n}\nfor job_id in job_ids:\nstatus = queue.get_status(job_id)\nstatuses[status[\"status\"]] += 1\ntotal = len(job_ids)\ncompleted_pct = (statuses[\"completed\"] / total) * 100 if total &gt; 0 else 0\nprint(f\"Workflow Progress: {completed_pct:.1f}% complete\")\nprint(f\"Pending: {statuses['pending']}\")\nprint(f\"Running: {statuses['running']}\")\nprint(f\"Completed: {statuses['completed']}\")\nprint(f\"Failed: {statuses['failed']}\")\nprint(f\"Cancelled: {statuses['cancelled']}\")\nprint(f\"Timeout: {statuses['timeout']}\")\nreturn statuses\n</code></pre>"},{"location":"user-guide/workflows/#best-practices","title":"Best Practices","text":"<p>When designing workflows, consider the following best practices:</p> <ol> <li> <p>Keep jobs atomic: Each job should perform a single, well-defined task.</p> </li> <li> <p>Set appropriate timeouts: Ensure each job has a timeout appropriate for its expected runtime.</p> </li> <li> <p>Handle errors gracefully: Jobs should handle exceptions internally when possible and return meaningful error information.</p> </li> <li> <p>Design for restartability: If a workflow is interrupted, it should be able to resume from where it left off.</p> </li> <li> <p>Use meaningful job names: Clear, descriptive names make it easier to monitor and debug workflows.</p> </li> <li> <p>Balance parallelism: Consider the available resources when designing workflows with parallel execution.</p> </li> <li> <p>Pass minimal data between jobs: Use file paths or database references rather than large data objects when passing information between jobs.</p> </li> </ol>"},{"location":"user-guide/workflows/#example-etl-workflow","title":"Example: ETL Workflow","text":"<p>Here's a complete example of an ETL (Extract, Transform, Load) workflow:</p> <pre><code>from datetime import datetime\nfrom gigq import Job, JobQueue, Workflow\n# Define job functions\ndef extract_data(source_url, output_file):\n# Download data from source_url and save to output_file\nprint(f\"Extracting data from {source_url} to {output_file}\")\n# ... extraction logic ...\nreturn {\"rows_extracted\": 1000, \"file\": output_file}\ndef transform_data(input_file, output_file):\n# Transform data from input_file and save to output_file\nprint(f\"Transforming data from {input_file} to {output_file}\")\n# ... transformation logic ...\nreturn {\"rows_transformed\": 950, \"file\": output_file}\ndef load_data(input_file, database_connection):\n# Load data from input_file into database\nprint(f\"Loading data from {input_file} to {database_connection}\")\n# ... loading logic ...\nreturn {\"rows_loaded\": 950}\ndef notify_completion(job_results, recipients):\n# Send notification of completion\nprint(f\"Sending notification to {recipients}\")\n# ... notification logic ...\nreturn {\"notifications_sent\": len(recipients)}\n# Create a workflow\nworkflow = Workflow(\"daily_etl_job\")\n# Create date-stamped filenames\ntimestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\nraw_file = f\"data/raw_{timestamp}.csv\"\ntransformed_file = f\"data/transformed_{timestamp}.csv\"\n# Create jobs\nextract_job = Job(\nname=\"extract\",\nfunction=extract_data,\nparams={\n\"source_url\": \"https://example.com/api/data\",\n\"output_file\": raw_file\n},\ntimeout=300,  # 5 minutes\nmax_attempts=3\n)\ntransform_job = Job(\nname=\"transform\",\nfunction=transform_data,\nparams={\n\"input_file\": raw_file,\n\"output_file\": transformed_file\n},\ntimeout=600,  # 10 minutes\nmax_attempts=2\n)\nload_job = Job(\nname=\"load\",\nfunction=load_data,\nparams={\n\"input_file\": transformed_file,\n\"database_connection\": \"postgresql://user:pass@localhost/db\"\n},\ntimeout=900,  # 15 minutes\nmax_attempts=3\n)\nnotify_job = Job(\nname=\"notify\",\nfunction=notify_completion,\nparams={\n\"recipients\": [\"data-team@example.com\"]\n},\ntimeout=60,  # 1 minute\nmax_attempts=5\n)\n# Add jobs to workflow with dependencies\nworkflow.add_job(extract_job)\nworkflow.add_job(transform_job, depends_on=[extract_job])\nworkflow.add_job(load_job, depends_on=[transform_job])\nworkflow.add_job(notify_job, depends_on=[load_job])\n# Submit the workflow\nqueue = JobQueue(\"etl_jobs.db\")\njob_ids = workflow.submit_all(queue)\nprint(f\"Submitted ETL workflow with {len(job_ids)} jobs\")\n</code></pre>"},{"location":"user-guide/workflows/#next-steps","title":"Next Steps","text":"<p>Now that you understand how to create and manage workflows, you may want to explore:</p> <ul> <li>Error Handling - Learn more about how GigQ handles errors in jobs and workflows</li> <li>CLI Usage - Using the command line interface to manage workflows</li> <li>Examples - See complete examples of workflows in action</li> </ul>"}]}